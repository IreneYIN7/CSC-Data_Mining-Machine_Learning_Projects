{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IreneYIN7/CSC-Data_Mining-Machine_Learning_Projects/blob/master/Irene_CSC_321_Assignment_2_SLR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMC3opJ6o1iu"
      },
      "source": [
        "# CSC-321: Data Mining and Machine Learning\n",
        "\n",
        "# Irene Yin\n",
        "\n",
        "## Assignment 2: Simple Linear Regression\n",
        "\n",
        "### Part 1: Implementing SLR\n",
        "\n",
        "Refer to the class slides for an overview of how simple linear regression works. Then implement the following algorithms. I will NOT be grading you on the style elements of your code (although I may give feedback) however you should try to use good programming style, especially with respect to variable names, spacing, printing of results and sensible code choices.\n",
        "\n",
        "The dataset I've given you here is REALLY small. Why? Because I want you to be able to check your math. You should be able to see what the variance, covariance and coefficients should be using a calculator. DO NOT just rely on the computer generated number, and assume it's correct. \n",
        "\n",
        "The data here contains 5 instances, and 2 features. The first element in each sublist is the input feature, X, and the second element is the target value, y.\n",
        "\n",
        "Be careful when coding that you do not overly rely on the fact that there is ONLY one column of input data for X here.\n",
        "\n",
        "(a) First, read the following example page for matplotlib, and create a plot of the points in the data set. Make the data points blue triangles. Make the axis in both directions range from 0 to 6.\n",
        "\n",
        "https://matplotlib.org/stable/tutorials/introductory/pyplot.html#sphx-glr-tutorials-introductory-pyplot-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgZfYzkwo1iv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "fa32dedc-2a5c-48e1-e6a8-0997bdbae649"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Use this dataset for all examples\n",
        "dataset = [[1,1],[2,3],[4,3],[3,2],[5,5]]\n",
        "array = np.array(dataset)\n",
        "Xvalue = array[:,0]\n",
        "Yvalue = array[:,1]\n",
        "plt.plot(Xvalue,Yvalue,'b^')\n",
        "plt.axis([0,6,0,6])\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlapUIEQo1ix"
      },
      "source": [
        "(b) Create a function that returns the average (the mean) for any list of values.\n",
        "\n",
        "(c) Create a function that computes the total variance of a list of values. NOTE - NOT the average variance as discussed in class, the total. That is, for each value of X, subtract the mean and square the result. Sum the resulting values, and return. The function takes two arguments, a list of values and the mean of that list of values.\n",
        "\n",
        "(d) Create a function that computes the covariance between two lists of numbers. Given two lists (X and y), and two means for those lists, for each value in X, subtract the mean of X, and then multiply by the corresponding value of (y minus the mean of y). Sum the resulting values and return. \n",
        "\n",
        "(e) Using your functions above, write a function that given two lists, one of X values and one of y values, computes the two coefficients, b1 and b0. Remember, b1 is computed by dividing the covariance of X and y by the variance of X. b0 is computed by taking the mean of y, and subtracting b1 multiplied by the mean of X from it. Return the two coefficients, b0 and b1, in that order, in a single list. \n",
        "\n",
        "(f) Test your coefficients function, with the dataset given. That means slice the data above into a list of X values, and a list of y values. Pass those lists to the functions described above to generate the coefficients. Print both of the resulting coefficients nicely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY-4NqIZo1iy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09950827-7d0f-406f-ff84-bad07f0c974d"
      },
      "source": [
        "import numpy as np\n",
        "# Write your functions b thru f here\n",
        "#b)\n",
        "def getMean(inputList):\n",
        "  # return: the average (the mean) for any list of values(numbers).\n",
        "  \n",
        "  return np.mean(inputList)\n",
        "\n",
        "#c)\n",
        "def getVariance(inputList):\n",
        "  # return: the total variance of a list of values.\n",
        "  meanValue = getMean(inputList)\n",
        "  variance = 0\n",
        "  for num in inputList:\n",
        "    variance += (num-meanValue)**2\n",
        "  return variance\n",
        "\n",
        "#d)\n",
        "def getCovariance(inputListX, inputListY, meanX, meanY):\n",
        "  # return: covariance between two lists of numbers.\n",
        "  covariance = 0\n",
        "  for i in range(len(inputListX)):\n",
        "    covariance += (inputListX[i]-meanX)*(inputListY[i] - meanY)\n",
        "  return covariance\n",
        "\n",
        "#e)\n",
        "def getCoefficient(inputListX, inputListY):\n",
        "  # return: coefficient b0 and b1 in a single list.\n",
        "  meanOfX = getMean(inputListX)\n",
        "  meanOfY = getMean(inputListY)\n",
        "  b1 = getCovariance(inputListX,inputListY,meanOfX,meanOfY)/getVariance(inputListX)\n",
        "  b0 = getMean(inputListY)-b1*meanOfX\n",
        "  return [b0,b1]\n",
        "\n",
        "#f)\n",
        "coefficient = getCoefficient(Xvalue, Yvalue)\n",
        "print('b0 for dataset', coefficient[0])\n",
        "print('b1 for dataset', coefficient[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 4 3 5]\n",
            "[1 3 3 2 5]\n",
            "b0 for dataset 0.39999999999999947\n",
            "b1 for dataset 0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRc3O_BGs6MU"
      },
      "source": [
        "We're going to use a convention here - capital X is the input feature(s), lower case y is the thing to predict. \n",
        "\n",
        "We're going to create four lists. Those will be called:\n",
        "1. X_train\n",
        "2. y_train\n",
        "\n",
        "1 and 2 comprise the data we'll use to train the model; \n",
        "\n",
        "3. X_test\n",
        "4. y_test\n",
        "\n",
        "X_test is the data we'll apply the model to, to predict the y values. \n",
        "\n",
        "y_test are the REAL y values for the X_test data and we can match those against the predicted y values to measure performance.\n",
        "\n",
        "In this assignment we're going to use the exactly the same data here for both training and testing. That is, there will be 5 instances for training, and then we'll predict y values for those same 5 instances. Does this seem like a good idea? We'll talk about it in class later. \n",
        "\n",
        "(g) Now you have sufficient functionality to write a function to make predictions using simple linear regression. Create a function that takes THREE lists as arguments: two for training (X_train, y_train) and one for testing (X_test).\n",
        "\n",
        "Here's the simple linear regression algorithm. Make this into a function.\n",
        "\n",
        "Using the functions you wrote earlier, we're going to create our coefficients from the training data. We're going to create a new list, to hold our predictions. Then for each instance in the X_test data, we're going to read the input value, and apply the formula \n",
        "\n",
        "> **y = b0 + b1 * x(i)**\n",
        "\n",
        "For each entry in the X_test data, we're going to have a predicted y value, which we'll append to our list of predictions. At the end, we're going to return our list of predictions.\n",
        "\n",
        "(h) Call your simple linear regression function, with the dataset above split appropriately into X_train, y_train, X_test and y_test. In this example, X_train and X_test will be the same, and y_train and y_test will be the same. Print the predicted output and the real output, nicely.\n",
        "\n",
        "(i) Recreate the plot from (a) above, plotting the points the same as before, by plotting your X_test values against your y_test values.\n",
        "But this time **ALSO** plot a line (X_test against predicted y) in red, that shows how well the slr algorithm is modeling the output. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJV637qvo1i1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "36f39649-8ffe-4b00-ce4e-dd147c4f27a0"
      },
      "source": [
        "\n",
        "# Create your four datasets here\n",
        "dataset = [[1,1],[2,3],[4,3],[3,2],[5,5]]\n",
        "X_train = array[:,0]\n",
        "y_train = array[:,1]\n",
        "X_test = array[:,0]\n",
        "y_test = array[:,1]\n",
        "\n",
        "# Write the function for simple linear regression here\n",
        "def slr(Xtrain,ytrain,Xtest):\n",
        "  # return: a list that hold the predictions. \n",
        "  coef = getCoefficient(Xtrain,ytrain)\n",
        "  b0 = coef[0]\n",
        "  b1 = coef[1]\n",
        "  prediction = []\n",
        "  for num in Xtest:\n",
        "    prediction.append(b0+b1*num)\n",
        "  return prediction\n",
        "\n",
        "\n",
        "# Call the function below\n",
        "y_predict = slr(X_train,y_train,X_test)\n",
        "print(\"The prediction y value is: \", y_predict)\n",
        "print(\"The real output is: \", y_test)\n",
        "\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(X_test,y_test,'b^')\n",
        "plt.plot(X_test,y_predict,'r')\n",
        "plt.axis([0,6,0,6])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The prediction y value is:  [1.1999999999999995, 1.9999999999999996, 3.5999999999999996, 2.8, 4.3999999999999995]\n",
            "The real output is:  [1 3 3 2 5]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckIJVy4po1i4"
      },
      "source": [
        "### Part 2: Implementing zeroR\n",
        "\n",
        "In class we learned about the zeroR algorithm. For regression, that means that we will always predict the mean value of the y_train variable - the target. \n",
        "\n",
        "(a) Write the function zeroRR (for regression) below. This function will take two arguments, y_train and X_test data. We **could** also pass it X_train, just to be consistent with our slr function above (which is what scikit learn does), but we know that zeroR NEVER looks at the X_train data, so we wont bother.\n",
        "\n",
        "Your zeroRR function will compute the mean from the y_train values. For prediction, it will assign this mean to every X_test instance, and append this prediction to a list. Return the list of predictions. \n",
        "\n",
        "(b) Test your zeroRR function, with the datasets as defined above. Print the input and the predicted output. Nicely.\n",
        "\n",
        "(c) Recreate the plot from (i) in Part 1, above. Plot the points, the predicted output y from the simple linear regression algorithm, and now add the predicted output y from the zeroRR algorithm as another line, this time in green."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if9K1_uro1i4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "2e02887a-6d64-4e6f-9315-65b220425b83"
      },
      "source": [
        "# Write the function for zeroRR here\n",
        "def zeroRR(ytrain, Xtest):\n",
        "  # return: compute the mean from the ytrain values and return the list of predictions.\n",
        "  meanOfytrain = np.mean(ytrain)\n",
        "  prediction = []\n",
        "  for num in Xtest:\n",
        "    prediction.append(meanOfytrain)\n",
        "  return prediction\n",
        "\n",
        "# Call the function zeroRR below\n",
        "ZeroR_predict = zeroRR(y_train, X_test)\n",
        "print(\"The input data is: \", X_test)\n",
        "print(\"The predicted output for zeroR is: \", ZeroR_predict)\n",
        "\n",
        "\n",
        "# Plot the results\n",
        "plt.plot(X_test,y_test,'b^')\n",
        "plt.plot(X_test,y_predict,'r')\n",
        "plt.plot(X_test,ZeroR_predict, 'g')\n",
        "plt.axis([0,6,0,6])\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The input data is:  [1 2 4 3 5]\n",
            "The predicted output for zeroR is:  [2.8, 2.8, 2.8, 2.8, 2.8]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHHP8duJo1i6"
      },
      "source": [
        "### Part 3: Evaluating output\n",
        "\n",
        "We want to be able to evaluate the output of our algorithms. We will implement the RMSE algorithm discussed in class for evaluating performance. This function will take two lists of values - actual y values and predicted y values.\n",
        "\n",
        "(a) Write a function that calculates RMSE. Start with an error of 0.0, then go through each y value in the actual list, and calculates the prediction error by subtracting the actual from the predicted value, and squares the error. Those squared errors are summed for all instances and then the average squared error is calculated, dividng the sum of the square errors by the number of predictions. Return the square root of that average error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHASSRMVo1i6"
      },
      "source": [
        "# Write the function rmse(actual,predicted) below\n",
        "def rmse(actual, predicted):\n",
        "  # return: prediction error.\n",
        "  predictionError = 0.0\n",
        "  for i in range(len(actual)):\n",
        "    predictionError += (actual[i] - predicted[i])**2\n",
        "  predictionError_avg = predictionError/len(actual)\n",
        "  predictionError_avg = predictionError_avg**0.5\n",
        "  return predictionError_avg\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NISST-Ovo1i8"
      },
      "source": [
        "### Part 4: Putting it all together\n",
        "\n",
        "Given the dataset below:\n",
        "- split into four sets, X_train, y_train, X_test and t_test, where X_train and X_test contain the same values, and y_test and y_train also. \n",
        "\n",
        "- Pass these appropriate arguments to both your simple linear regression code, and your zeroRR code, generating list of predictions for each.\n",
        "\n",
        "- Evaluate these predictions against the real values (y_test)\n",
        "\n",
        "- Print the results nicely\n",
        "\n",
        "- FINALLY, edit the text box after this code, telling me which algorithm works better, simple linear regression or zeroRR, and why you believe that"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfz3-w8Oo1i9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c643202-18d4-447b-b0df-a7d91d821ae4"
      },
      "source": [
        "# Evaluate the two algorithms, using RMSE\n",
        "# And the dataset below\n",
        "\n",
        "dataset = [[1,1],[2,3],[4,3],[3,2],[5,5]]\n",
        "X_train = array[:,0]\n",
        "y_train = array[:,1]\n",
        "X_test = array[:,0]\n",
        "y_test = array[:,1]\n",
        "\n",
        "Predict_y = slr(X_train,y_train,X_test)\n",
        "print(Predict_y)\n",
        "ZeroR_predict = zeroRR(y_train, X_test)\n",
        "slr_rmse = rmse(y_test, Predict_y)\n",
        "ZeroR_rmse = rmse(y_test, ZeroR_predict)\n",
        "print(\"The predicted y value is: \", Predict_y)\n",
        "print(\"The RMSE for SLR is : %.3f\" % slr_rmse)\n",
        "print(\"The predicted value for ZeroR is: \", ZeroR_predict)\n",
        "print(\"The RMSE for ZeroR is: %.3f\" % ZeroR_rmse)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 4 3 5]\n",
            "[1 3 3 2 5]\n",
            "[1.1999999999999995, 1.9999999999999996, 3.5999999999999996, 2.8, 4.3999999999999995]\n",
            "The predicted y value is:  [1.1999999999999995, 1.9999999999999996, 3.5999999999999996, 2.8, 4.3999999999999995]\n",
            "The RMSE for SLR is : 0.693\n",
            "The predicted value for ZeroR is:  [2.8, 2.8, 2.8, 2.8, 2.8]\n",
            "The RMSE for ZeroR is: 1.327\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtlvsMsJCjPZ"
      },
      "source": [
        "#### Interpretation of results\n",
        "Simple Linear Regression works better since its RMSE value is 0.693 which is smaller than the RMSE value of ZeroRR which is 1.327."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_Mw8Eso_YFM"
      },
      "source": [
        "### Part 5: Introduction to scikit-learn\n",
        "\n",
        "One of the most popular open-source python machine learning libraries is scikit-learn.\n",
        "\n",
        "As we go through this class I'll introduce you to some of the functionality. Below I use both the linear regression and zeroR functionality, as well as using a built-in evaluation metric.\n",
        "\n",
        "Recreate the four lists X_train, y_train, X_test and y_test as you would have for the previous examples, using the same data for both training and testing. Then the following code should run. \n",
        "\n",
        "Note that I have to do some reshaping, to make them into 2D arrays from the 1D list that python uses. This is just a requirement of scikit-learn, and you can see how I do it below.\n",
        "\n",
        "Also note how I use variable names. You will find that it is INCREDIBLY easy to confuse yourself with the names of models and variables. Try early on to develop a consistent approach to naming. It will help.\n",
        "\n",
        "You can use the final output of this code to check the results of your code, above. The RMSE values should be the same (possibly a 0.01 difference).\n",
        "\n",
        "You can find out more in general at: https://scikit-learn.org/stable/index.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2Mmn-Xs56DP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c88ee49-66bd-4b2d-b265-4d9cc91d70d6"
      },
      "source": [
        "# Import required libraries, two classifiers and the built in MSE metric\n",
        "# See the following for details:\n",
        "#\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "dataset = [[1,2,1],[2,2,3],[4,3,3],[3,2,2],[5,4,5],[5,2,6],[4,5,4],[7,5,5]]\n",
        "\n",
        "# **************************************************************************\n",
        "# CREATE YOUR X_train,y_train, X_Test,y_test datasets here\n",
        "\n",
        "\n",
        "X_train = []\n",
        "for i in range(len(dataset)):\n",
        "  X_train.append(dataset[i][:len(dataset[i])-1])\n",
        "\n",
        "y_train = []\n",
        "for i in range(len(dataset)):\n",
        "  y_train.append(dataset[i][-1])\n",
        "\n",
        "X_test = X_train\n",
        "y_test = y_train\n",
        "\n",
        "print(X_test)\n",
        "\n",
        "# **************************************************************************\n",
        "# I make those into numpy arrays, and reshape them\n",
        "\n",
        "# Reshaping here means I'm telling numpy\n",
        "# I KNOW the shape of one of the dimensions (it's 1)\n",
        "# The -1 tells numpy to figure it out, using the length\n",
        "# of the vector (-1 basically means unknown)\n",
        "\n",
        "# Because I know that there are FIVE instances,\n",
        "# I COULD have written\n",
        "# .reshape(5,1) \n",
        "# for the X_train / X_test\n",
        "# But as I KNOW it's a SOMETHING by 1\n",
        "# if I do reshape(-1,1), then this same code will work\n",
        "# with a 100 instances just as it will work with 5 instances.\n",
        "\n",
        "X_train = np.array(X_train).reshape(-1, 2)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "X_test = np.array(X_test).reshape(-1, 2)\n",
        "y_test = np.array(y_test)\n",
        "print(y_test)\n",
        "print(X_test)\n",
        "\n",
        "\n",
        "# Instantiate an object of the linear regression class\n",
        "lr = LinearRegression()\n",
        "\n",
        "# Fit the linear regression model using the features and the class\n",
        "# In scikit-learn, all models have a fit method\n",
        "# THIS is machine learning. Tadaa.\n",
        "\n",
        "reg = lr.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "# Exploring some built in concepts of the trained model\n",
        "# Examining b0 and b1 coefficients\n",
        "\n",
        "print('INTERCEPT (b0):',reg.intercept_ )\n",
        "print('COEFFICIENT (b1):',reg.coef_ )\n",
        "print()\n",
        "\n",
        "\n",
        "# The fit() method produces a model (that I stored in the variable 'reg')\n",
        "# We can now apply that model to new data using the predict method\n",
        "# So here I apply it to the X values (the input)\n",
        "# in order to PREDICT output values (y).\n",
        "\n",
        "lr_predY = reg.predict(X_test)\n",
        "\n",
        "\n",
        "# Now I'm to repeat this, using zeroR. In scikit this is called the\n",
        "# dummy classifier\n",
        "\n",
        "zr = DummyRegressor()\n",
        "zeroR = zr.fit(X_train, y_train)\n",
        "zr_predY = zeroR.predict(X_test)\n",
        "\n",
        "\n",
        "# And I'm going to evaluate the results\n",
        "# scikit has a built in metric for MSE. To get to RMSE \n",
        "# I simply take the square root of MSE.\n",
        "\n",
        "slr_rmse = math.sqrt(mean_squared_error(y_test,lr_predY))\n",
        "zr_rmse = math.sqrt(mean_squared_error(y_test,zr_predY))\n",
        "\n",
        "print('RMSE score for linear regression: {:.2f}'.format(slr_rmse))\n",
        "print('RMSE score for zeroR: {:.2f}'.format(zr_rmse))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2], [2, 2], [4, 3], [3, 2], [5, 4], [5, 2], [4, 5], [7, 5]]\n",
            "[1 3 3 2 5 6 4 5]\n",
            "[[1 2]\n",
            " [2 2]\n",
            " [4 3]\n",
            " [3 2]\n",
            " [5 4]\n",
            " [5 2]\n",
            " [4 5]\n",
            " [7 5]]\n",
            "INTERCEPT (b0): 1.0259740259740249\n",
            "COEFFICIENT (b1): [ 0.86796537 -0.24458874]\n",
            "\n",
            "RMSE score for linear regression: 0.83\n",
            "RMSE score for zeroR: 1.58\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Irene_CSC-321 Assignment 3_MLR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IreneYIN7/CSC-Data_Mining-Machine_Learning_Projects/blob/master/Irene_CSC_321_Assignment_3_MLR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urq6CK_yJwQf"
      },
      "source": [
        "# CSC-321: Data Mining and Machine Learning\n",
        "\n",
        "# Irene Yin\n",
        "\n",
        "## Assignment 3: Multivariate Linear Regression\n",
        "\n",
        "\n",
        "The function for making predicitions using Simple Linear Regression was:\n",
        "\n",
        "**y = b0 + b1*x**\n",
        "\n",
        "For the more complicated Multivariate Linear Regression the function is:\n",
        "\n",
        "**y = b0 + b1*x1 + b2*x2 + ... + bN*xN**\n",
        "\n",
        "With there being as many coefficients (b) as there are input features, plus 1 (for the b0 coefficient, or intercept). \n",
        "\n",
        "For more complex learning, we cannot simply read our coefficients (b0 and b1 for SLR) from the data. We *could* use the least squares method, but for MORE input coefficients we have to use more complex linear algebra that involves lots of matrix math. And in the end it will be an *estimation* of the coefficients. \n",
        "\n",
        "We have to estimate them because there are many more of them, and they all interact together. Least squares is one method of estimation. We're going to look at another, Stochastic Gradient Descent (SGD).\n",
        "\n",
        "We don't know exactly how the input variables play together for a given data set so we're going to use a mechanism to pick some random values for our initial coefficients, and gradually improve the coefficients over time.\n",
        "\n",
        "The method we discussed in class is called Stochastic Gradient Descent, and is one of a variety of optimization algorithms that are used in machine learning methods to 'learn' the coefficients, or weights, on input values with respect to some output. This estimation of coefficients IS the learning part of machine learning. At least for these linear models.\n",
        "\n",
        "Gradient descent is the process of minimizing some function (we'll call it the cost, or error function) by following the slope (or gradient) of the function down to some minimum (the lowest error over our data). \n",
        "\n",
        "Intuitively, we're going to show our model one training instance at a time, make a prediction for that instance, calculate the error using that instance, and update the model in order to improve performance (get a smaller error) for the next prediction. We'll repeat this process for a number of iterations, minimizing the error each time.\n",
        "\n",
        "Each iteration, we're going to update each of the coefficients using the formula:\n",
        "\n",
        "b = b - learning_rate * error * value_of_x\n",
        "\n",
        "for each coefficient (again, corresponding to each input feature, of each instance). The error means the error calculated when we use the coefficients to make a prediction. So for coefficient b1, this translates to:\n",
        "\n",
        "b1 = b1 - learning_rate * error * value_of_x1\n",
        "\n",
        "Remember also that learning_rate is a value we must choose (I'll tell you to start with), and the total number of iterations over all the training data (epochs) is ALSO a number we must choose. \n",
        "\n",
        "We're going to break this down into the individual steps. I recommend reading this whole notebook to get a sense of where we're going.\n",
        "\n",
        "Let's start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYAzBeUhJwQg"
      },
      "source": [
        "### Part 1: Making Predictions\n",
        "\n",
        "(a) We're going to use predictions made by our model as a guide for tuning the coefficients, so we need a function that can make predictions. This *same* function can also apply the final coefficients we have learned to make predictions over new data. \n",
        "\n",
        "Write a function called that takes a **single instance** (a list of input features, from for example an X_train or X_test data set), and a list of coefficients, and calculates the predicted y value, using the formula:\n",
        "\n",
        "y = b0 + b1*x1 + b2*x2 + ... + bN*xN\n",
        "\n",
        "This should work for ANY length of instance, but we can always assume that the length of the instance list and the length of the coefficent list are different by one. Why one? For the instance, imagine that the input feature values of a single instance are:\n",
        "\n",
        "[x1,x2,x3] \n",
        "\n",
        "For the coefficients, let's assume that the list contains all the coefficients, including b0. Let's also assume that we ALWAYS store the b0 coefficient at position coefficients[0] (i.e. the first position in the coefficients list).\n",
        "\n",
        "The resulting coefficient list for the above instance would then be:\n",
        "\n",
        "[b0,b1,b2,b3]\n",
        "\n",
        "Again, your code should work for ANY length of instance, and the corresponding list of coefficients.\n",
        "\n",
        "Your function should return the predicted y value for a given instance and a given set of coefficients, using the formula above. \n",
        "\n",
        "(b) In the Simple Linear Regression assignment, you applied your model to a contrived data set. I've reproduced this data set below. Go through this data set one instance at a time and call your new predict function for each instance. You can use the coefficients [0.4, 0.8], which are almost exactly what you learned as coefficients in Assignment 2. \n",
        "\n",
        "For each instance, print out the correct value and the value predicted by your function from (a). You should see that it performs reasonably well - the predicted values should be close to but not exactly the same as the actual values.\n",
        "\n",
        "I'd take the time to print nicely here. It helps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MztlfcIMJwQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "241d9c09-d0e6-460f-c5d3-859c11c855c9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Write your predict function here\n",
        "\n",
        "def predFunc(inputlist, coefficient):\n",
        "  # input: the length of coeficent is one more than the length of inputlist. \n",
        "  #        We assume the coefficient list contains all the coefficients, including b0.\n",
        "  # return: the predicted output value.\n",
        "  predOutput = coefficient[0]\n",
        "  for i in range(len(coefficient)-1):\n",
        "    nextIndex = i+1\n",
        "    predOutput += coefficient[nextIndex]*inputlist[i]\n",
        "  return predOutput\n",
        "\n",
        "# Apply your function to the contrived dataset\n",
        "\n",
        "# Split your data into X_train and y_train\n",
        "# In this assignment, it's important that X_train be a list of lists, i.e.\n",
        "# [[1],[2],[4],[3],[5]]\n",
        "# in this case\n",
        "# Go through each instance of X_train, and get a predicted y value\n",
        "# Print out the predicted y, and the corresponding actual y from y_train\n",
        "\n",
        "dataset = [[1,1],[2,3],[4,3],[3,2],[5,5]]\n",
        "coef = [0.4,0.8]\n",
        "X_train = []\n",
        "for i in range(len(dataset)):\n",
        "  X_train.append(dataset[i][:len(dataset[i])-1])\n",
        "\n",
        "y_train = []\n",
        "for i in range(len(dataset)):\n",
        "  y_train.append(dataset[i][-1])\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "  output = predFunc(X_train[i], coef) \n",
        "  print(\"Predicted y value: %.3f\" % output, \"Real y value: \", y_train[i] )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted y value: 1.200 Real y value:  1\n",
            "Predicted y value: 2.000 Real y value:  3\n",
            "Predicted y value: 3.600 Real y value:  3\n",
            "Predicted y value: 2.800 Real y value:  2\n",
            "Predicted y value: 4.400 Real y value:  5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR-uYV08JwQl"
      },
      "source": [
        "### Part 2: Learning coefficients with Stochastic Gradient Descent\n",
        "\n",
        "We now need to estimate coefficients for ourselves by writing a stochastic gradient descent function. For a given learning rate, for a number of iterations (epochs), we're going to estimate our coefficients.  Given a set of training data, spilt into X_train and y_train, we're going to:\n",
        "\n",
        "- loop over each epoch (where we go through ALL the instances of the training data)\n",
        "- loop over each row (instance) in the training data, for an epoch\n",
        "- loop over each coefficient, for each feature in each instance, for each epoch\n",
        "\n",
        "As computer scientists you should recognize that this requires three, nested for loops, and you should have a sense (a big-O kind of sense) why this can take a long time for large data sets.\n",
        "\n",
        "Coefficients are updated based on the error the model made. The error is calculated as the difference between the predicted y value and the actual y value:\n",
        "\n",
        "error = prediction - actual\n",
        "\n",
        "There is one coefficient for EACH input attribute, and these are updated every time, for example:\n",
        "\n",
        "b1 = b1 - learning_rate * error * x1\n",
        "\n",
        "We ALSO need to update the special intercept coefficient b0:\n",
        "\n",
        "b0 = b0 - learning_rate * error\n",
        "\n",
        "(c) Implement the following algorithm for Stochastic Gradient Descent, that takes a list of X_train values, the list of corresponding y_train values, and two other parameters, learning_rate and epochs.\n",
        "\n",
        "The algorithm is as follows: \n",
        "\n",
        "- initialize a list for the output coefficients. The length of the list will be the same as the length of each instance in your X_train data, + 1 for the special b0 coefficient. We can initialize all the coefficients to 0.0 to start with (remember, it doesn't really matter where we start)\n",
        "- for each epoch\n",
        "    - initialize the total error to 0\n",
        "    - for each instance in the X_train data\n",
        "        1. calculate the prediction for that instance, given the current list of coefficients, using our function from (a)\n",
        "        2. calculate the error for that prediction, using the corresponding y_train value \n",
        "        3. square the error, and add it to the total error. Note that squaring the individual error means it will always be a positive value. ALSO NOTE: We don't use this squared error for updating the coefficients - we use the error calculated in **step 2**. This squaring is just to give us nice, readable output at the end of each epoch. \n",
        "        4. Now update the coefficients, using the formulas given above. One update for b0 (which should always be at position 0 in the coefficients list), and then loop over the remaining coefficients, performing an update for each one, b1 through bN.\n",
        "        \n",
        "    - At the end of each epoch, print out the epoch number (we can start at epoch 0), the learning rate, and the total error for this epoch (DO THIS ON ONE LINE FOR READABILITY).\n",
        "    - repeat for the next epoch\n",
        "- once we've iterated through all the epochs, return the list of coefficients\n",
        "\n",
        "(d) Apply your stochastic gradient descent function to the contrived dataset, given below. If it's working, you should see the error rate falling each epoch. You should also note that the value of the coefficients learned at the end isn't quite the same as Simple Linear Regression, because we're estimating this time. You could try learning longer (more epochs), or altering the learning rate, and see if the coefficients approach the optimal values we learned in Assignment 2.\n",
        "\n",
        "I've given you TWO datasets to work with. The first aligns with last week, so you can test your results. The second involves more features, so you can make sure you haven't accidentally made coding decisions that rely on only two features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsT2NDwiJwQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c39b3ef-b764-4f63-be3e-116cc323b131"
      },
      "source": [
        "# Write your coefficientsSGD(train,test,learning_rate,epochs) here\n",
        "def coefficientsSGD(train,test,learning_rate,epochs):\n",
        "  # return: the coefficients\n",
        "  coefficient = [0.0 for j in range(len(train[0])+1)]\n",
        "  for iteration in range(epochs):\n",
        "    Totalerror = 0\n",
        "    for i in range(len(train)):\n",
        "      prediction = predFunc(train[i],coefficient)\n",
        "      error = prediction - test[i]\n",
        "      Totalerror += error**2\n",
        "      coefficient[0] -= learning_rate * error\n",
        "      for index in range(len(coefficient)-1):\n",
        "        coefficient[index+1] -= learning_rate * error * train[i][index]\n",
        "    print(\"epoch number: \", iteration, \"learning rate is: \", learning_rate,\n",
        "          \"total error is: \", Totalerror)\n",
        "  return coefficient\n",
        "\n",
        "# Apply to the contrived data here. Try my parameters first, before you experiment\n",
        "# BEFORE you submit your assigment, return the code to using my parameters:\n",
        "# learning_rate = 0.001\n",
        "# epochs = 50\n",
        "\n",
        "dataset1 = [[1,1],[2,3],[4,3],[3,2],[5,5]]\n",
        "dataset2 = [[1,2,1],[2,2,3],[4,3,3],[3,2,2],[5,4,5],[5,2,6],[4,5,4],[7,5,5]]\n",
        "X_train1 = []\n",
        "for i in range(len(dataset1)):\n",
        "  X_train1.append(dataset1[i][:len(dataset1[i])-1])\n",
        "print(X_train1)\n",
        "print(len(X_train1[0]))\n",
        "y_train1 = []\n",
        "for i in range(len(dataset1)):\n",
        "  y_train1.append(dataset1[i][-1])\n",
        "\n",
        "X_train2 = []\n",
        "for i in range(len(dataset2)):\n",
        "  X_train2.append(dataset2[i][:len(dataset2[i])-1])\n",
        "\n",
        "y_train2 = []\n",
        "for i in range(len(dataset2)):\n",
        "  y_train2.append(dataset2[i][-1])\n",
        "\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 50\n",
        "\n",
        "coefficient1 = coefficientsSGD(X_train1, y_train1, learning_rate, epochs)\n",
        "print('The coefficients of the dataset1 is: ', coefficient1)\n",
        "\n",
        "coefficient2 = coefficientsSGD(X_train2, y_train2, learning_rate, epochs)\n",
        "print('The coefficients of the dataset2 is: ', coefficient2)\n",
        "# Slice your data into X_train and y_train\n",
        "# Call your SGD function to obtain a list of coefficients\n",
        "# PRINT your coefficients nicely\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1], [2], [4], [3], [5]]\n",
            "1\n",
            "epoch number:  0 learning rate is:  0.001 total error is:  46.23569225016471\n",
            "epoch number:  1 learning rate is:  0.001 total error is:  41.305142323835085\n",
            "epoch number:  2 learning rate is:  0.001 total error is:  36.92968879875065\n",
            "epoch number:  3 learning rate is:  0.001 total error is:  33.046843407651664\n",
            "epoch number:  4 learning rate is:  0.001 total error is:  29.601151923716085\n",
            "epoch number:  5 learning rate is:  0.001 total error is:  26.543402390484545\n",
            "epoch number:  6 learning rate is:  0.001 total error is:  23.82992247422831\n",
            "epoch number:  7 learning rate is:  0.001 total error is:  21.421955907121237\n",
            "epoch number:  8 learning rate is:  0.001 total error is:  19.285109118735654\n",
            "epoch number:  9 learning rate is:  0.001 total error is:  17.38886015544335\n",
            "epoch number:  10 learning rate is:  0.001 total error is:  15.706122876572774\n",
            "epoch number:  11 learning rate is:  0.001 total error is:  14.212860205347214\n",
            "epoch number:  12 learning rate is:  0.001 total error is:  12.88774091297372\n",
            "epoch number:  13 learning rate is:  0.001 total error is:  11.7118350357667\n",
            "epoch number:  14 learning rate is:  0.001 total error is:  10.668343576747102\n",
            "epoch number:  15 learning rate is:  0.001 total error is:  9.742358632631682\n",
            "epoch number:  16 learning rate is:  0.001 total error is:  8.920650521505976\n",
            "epoch number:  17 learning rate is:  0.001 total error is:  8.191478871959525\n",
            "epoch number:  18 learning rate is:  0.001 total error is:  7.544424976557279\n",
            "epoch number:  19 learning rate is:  0.001 total error is:  6.970243016110007\n",
            "epoch number:  20 learning rate is:  0.001 total error is:  6.4607280306236845\n",
            "epoch number:  21 learning rate is:  0.001 total error is:  6.00859875189941\n",
            "epoch number:  22 learning rate is:  0.001 total error is:  5.607393624934783\n",
            "epoch number:  23 learning rate is:  0.001 total error is:  5.251378533574368\n",
            "epoch number:  24 learning rate is:  0.001 total error is:  4.935464912958705\n",
            "epoch number:  25 learning rate is:  0.001 total error is:  4.6551370796144145\n",
            "epoch number:  26 learning rate is:  0.001 total error is:  4.40638774162885\n",
            "epoch number:  27 learning rate is:  0.001 total error is:  4.185660768141068\n",
            "epoch number:  28 learning rate is:  0.001 total error is:  3.9898004010231354\n",
            "epoch number:  29 learning rate is:  0.001 total error is:  3.8160061836022887\n",
            "epoch number:  30 learning rate is:  0.001 total error is:  3.6617929628979957\n",
            "epoch number:  31 learning rate is:  0.001 total error is:  3.5249553942839214\n",
            "epoch number:  32 learning rate is:  0.001 total error is:  3.4035364417673994\n",
            "epoch number:  33 learning rate is:  0.001 total error is:  3.295799424125944\n",
            "epoch number:  34 learning rate is:  0.001 total error is:  3.2002032077659255\n",
            "epoch number:  35 learning rate is:  0.001 total error is:  3.1153801920958983\n",
            "epoch number:  36 learning rate is:  0.001 total error is:  3.0401167730772007\n",
            "epoch number:  37 learning rate is:  0.001 total error is:  2.9733360059968246\n",
            "epoch number:  38 learning rate is:  0.001 total error is:  2.914082219907388\n",
            "epoch number:  39 learning rate is:  0.001 total error is:  2.8615073640442605\n",
            "epoch number:  40 learning rate is:  0.001 total error is:  2.8148588912588495\n",
            "epoch number:  41 learning rate is:  0.001 total error is:  2.7734690054522004\n",
            "epoch number:  42 learning rate is:  0.001 total error is:  2.736745119468375\n",
            "epoch number:  43 learning rate is:  0.001 total error is:  2.7041613871898917\n",
            "epoch number:  44 learning rate is:  0.001 total error is:  2.6752511889152393\n",
            "epoch number:  45 learning rate is:  0.001 total error is:  2.649600462709536\n",
            "epoch number:  46 learning rate is:  0.001 total error is:  2.6268417864985882\n",
            "epoch number:  47 learning rate is:  0.001 total error is:  2.606649126395945\n",
            "epoch number:  48 learning rate is:  0.001 total error is:  2.5887331762654906\n",
            "epoch number:  49 learning rate is:  0.001 total error is:  2.572837221964086\n",
            "The coefficients of the dataset1 is:  [0.22998234937311363, 0.8017220304137576]\n",
            "epoch number:  0 learning rate is:  0.001 total error is:  104.95963111640654\n",
            "epoch number:  1 learning rate is:  0.001 total error is:  68.13269356061261\n",
            "epoch number:  2 learning rate is:  0.001 total error is:  45.434235718657675\n",
            "epoch number:  3 learning rate is:  0.001 total error is:  31.435063931822953\n",
            "epoch number:  4 learning rate is:  0.001 total error is:  22.7933908685992\n",
            "epoch number:  5 learning rate is:  0.001 total error is:  17.45204214708868\n",
            "epoch number:  6 learning rate is:  0.001 total error is:  14.144458110513806\n",
            "epoch number:  7 learning rate is:  0.001 total error is:  12.090701740065143\n",
            "epoch number:  8 learning rate is:  0.001 total error is:  10.810382402856767\n",
            "epoch number:  9 learning rate is:  0.001 total error is:  10.007522832980285\n",
            "epoch number:  10 learning rate is:  0.001 total error is:  9.499697229265717\n",
            "epoch number:  11 learning rate is:  0.001 total error is:  9.174413219484975\n",
            "epoch number:  12 learning rate is:  0.001 total error is:  8.962260217038054\n",
            "epoch number:  13 learning rate is:  0.001 total error is:  8.820376819009814\n",
            "epoch number:  14 learning rate is:  0.001 total error is:  8.722269697506222\n",
            "epoch number:  15 learning rate is:  0.001 total error is:  8.651542332828527\n",
            "epoch number:  16 learning rate is:  0.001 total error is:  8.59803089160606\n",
            "epoch number:  17 learning rate is:  0.001 total error is:  8.55542235459354\n",
            "epoch number:  18 learning rate is:  0.001 total error is:  8.519785574167669\n",
            "epoch number:  19 learning rate is:  0.001 total error is:  8.488664772337057\n",
            "epoch number:  20 learning rate is:  0.001 total error is:  8.46051967397181\n",
            "epoch number:  21 learning rate is:  0.001 total error is:  8.434379371335854\n",
            "epoch number:  22 learning rate is:  0.001 total error is:  8.409628049736929\n",
            "epoch number:  23 learning rate is:  0.001 total error is:  8.38587212481754\n",
            "epoch number:  24 learning rate is:  0.001 total error is:  8.362857690851486\n",
            "epoch number:  25 learning rate is:  0.001 total error is:  8.340419097214554\n",
            "epoch number:  26 learning rate is:  0.001 total error is:  8.318446813099575\n",
            "epoch number:  27 learning rate is:  0.001 total error is:  8.296867266444341\n",
            "epoch number:  28 learning rate is:  0.001 total error is:  8.275630133992223\n",
            "epoch number:  29 learning rate is:  0.001 total error is:  8.254700281540062\n",
            "epoch number:  30 learning rate is:  0.001 total error is:  8.234052616886142\n",
            "epoch number:  31 learning rate is:  0.001 total error is:  8.213668775351417\n",
            "epoch number:  32 learning rate is:  0.001 total error is:  8.193534964590674\n",
            "epoch number:  33 learning rate is:  0.001 total error is:  8.173640547605162\n",
            "epoch number:  34 learning rate is:  0.001 total error is:  8.153977099507648\n",
            "epoch number:  35 learning rate is:  0.001 total error is:  8.134537771121646\n",
            "epoch number:  36 learning rate is:  0.001 total error is:  8.115316853410853\n",
            "epoch number:  37 learning rate is:  0.001 total error is:  8.096309474924809\n",
            "epoch number:  38 learning rate is:  0.001 total error is:  8.077511388502082\n",
            "epoch number:  39 learning rate is:  0.001 total error is:  8.05891881871108\n",
            "epoch number:  40 learning rate is:  0.001 total error is:  8.04052835122799\n",
            "epoch number:  41 learning rate is:  0.001 total error is:  8.022336851601414\n",
            "epoch number:  42 learning rate is:  0.001 total error is:  8.004341404910312\n",
            "epoch number:  43 learning rate is:  0.001 total error is:  7.98653927048411\n",
            "epoch number:  44 learning rate is:  0.001 total error is:  7.968927847622265\n",
            "epoch number:  45 learning rate is:  0.001 total error is:  7.9515046494408885\n",
            "epoch number:  46 learning rate is:  0.001 total error is:  7.934267282786598\n",
            "epoch number:  47 learning rate is:  0.001 total error is:  7.917213432720776\n",
            "epoch number:  48 learning rate is:  0.001 total error is:  7.9003408504732295\n",
            "epoch number:  49 learning rate is:  0.001 total error is:  7.883647344046471\n",
            "The coefficients of the dataset2 is:  [0.15311017505222785, 0.6342385233806282, 0.2923092415366756]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMJDSWn1JwQr"
      },
      "source": [
        "(e) Now you have sufficient functionality to write a function to learn and make predictions using multivariate linear regression. \n",
        "\n",
        "Create a function that takes:\n",
        "\n",
        "1. X_train\n",
        "2. y_train\n",
        "\n",
        "which are the training data, from which we'll estimate coefficients\n",
        "\n",
        "3. X_test\n",
        "\n",
        "our test data, which we'll use to evaluate the model\n",
        "\n",
        "4. Learning rate\n",
        "5. Epochs\n",
        "\n",
        "and our parameters for learning.\n",
        "\n",
        "Just like the last assignment, we're going to use the same dataset here for both training and testing, even though that might not be a great idea (why not?).\n",
        "\n",
        "Here's the multivariate linear regression algorithm. We're going to estimate our coefficients from the training data, using the function from (c) above. We're going to create a new list, to hold our predictions. Then for each entry in the testing data, we're going to read the input value, and make a prediction, using our function from (a). We'll append our predicted y value to the prediction list. We're going to return our list of predictions.\n",
        "\n",
        "(f) Compare the predicitons to the actual y values stored in y_test, and compute the RMSE value. Print it nicely.\n",
        "Also print the zeroR value. This means you're going to need your RMSE function and your zeroRR function from the previous assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvhXcBGsJwQs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36be67fc-6296-417a-8c26-ab7662f07460"
      },
      "source": [
        "# Write your function for multivariate linear regression here\n",
        "X_test = X_train\n",
        "def mlr(X_train, y_train, X_test, Learning_rate, Epochs):\n",
        "  coefficient = coefficientsSGD(X_train, y_train, Learning_rate, Epochs)\n",
        "  predictions = []\n",
        "  for instance in X_test:\n",
        "    predictions.append(predFunc(instance, coefficient))\n",
        "  return predictions\n",
        "\n",
        "#f)\n",
        "\n",
        "def rmse(actual, predicted):\n",
        "  # return: prediction error.\n",
        "  predictionError = 0.0\n",
        "  for i in range(len(actual)):\n",
        "    predictionError += (actual[i] - predicted[i])**2\n",
        "  predictionError_avg = predictionError/len(actual)\n",
        "  predictionError_avg = predictionError_avg**0.5\n",
        "  return predictionError_avg\n",
        "\n",
        "def zeroRR(ytrain, Xtest):\n",
        "  # return: compute the mean from the ytrain values and return the list of predictions.\n",
        "  meanOfytrain = np.mean(ytrain)\n",
        "  prediction = []\n",
        "  for num in Xtest:\n",
        "    prediction.append(meanOfytrain)\n",
        "  return prediction\n",
        "\n",
        "predictions = mlr(X_train, y_train, X_test, learning_rate, epochs)\n",
        "predictions2 = zeroRR(y_train, X_test)\n",
        "print(\"The RMSE value is: \", rmse(y_train, predictions))\n",
        "print(\"The ZeroR value is: \", rmse(y_train, predictions2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction:  0.0\n",
            "prediction:  0.003\n",
            "prediction:  0.031973\n",
            "prediction:  0.063563351\n",
            "prediction:  0.132278553384\n",
            "epoch number:  0 learning rate is:  0.001 total error is:  46.23569225016471\n",
            "prediction:  0.062783210275696\n",
            "prediction:  0.11460888582494891\n",
            "prediction:  0.24047988979213297\n",
            "prediction:  0.22063162702668088\n",
            "prediction:  0.3826217302722257\n",
            "epoch number:  1 learning rate is:  0.001 total error is:  41.305142323835085\n",
            "prediction:  0.12193316085896903\n",
            "prediction:  0.21975246248293684\n",
            "prediction:  0.4369010262119619\n",
            "prediction:  0.3685976498504865\n",
            "prediction:  0.6184522052846227\n",
            "epoch number:  2 learning rate is:  0.001 total error is:  36.92968879875065\n",
            "prediction:  0.1776604281876328\n",
            "prediction:  0.3188054518429674\n",
            "prediction:  0.6219368917998004\n",
            "prediction:  0.5079889785983791\n",
            "prediction:  0.8406111270224543\n",
            "epoch number:  3 learning rate is:  0.001 total error is:  33.046843407651664\n",
            "prediction:  0.2301633838402014\n",
            "prediction:  0.41212085715859803\n",
            "prediction:  0.7962473696163249\n",
            "prediction:  0.6393025954395527\n",
            "prediction:  1.049890891699109\n",
            "epoch number:  4 learning rate is:  0.001 total error is:  29.601151923716085\n",
            "prediction:  0.2796289019210107\n",
            "prediction:  0.5000312218637281\n",
            "prediction:  0.9604540963600732\n",
            "prediction:  0.7630066778045603\n",
            "prediction:  1.2470379687944704\n",
            "epoch number:  5 learning rate is:  0.001 total error is:  26.543402390484545\n",
            "prediction:  0.326233025445792\n",
            "prediction:  0.5828498154094602\n",
            "prediction:  1.1151426790998948\n",
            "prediction:  0.8795422678878553\n",
            "prediction:  1.432755562936332\n",
            "epoch number:  6 learning rate is:  0.001 total error is:  23.82992247422831\n",
            "prediction:  0.3701415941040032\n",
            "prediction:  0.6608717503706873\n",
            "prediction:  1.2608647835271354\n",
            "prediction:  0.989324845387132\n",
            "prediction:  1.6077061215010222\n",
            "epoch number:  7 learning rate is:  0.001 total error is:  21.421955907121237\n",
            "prediction:  0.4115108356364928\n",
            "prediction:  0.734375034805959\n",
            "prediction:  1.3981401011741836\n",
            "prediction:  1.0927458090877922\n",
            "prediction:  1.7725136968752484\n",
            "epoch number:  8 learning rate is:  0.001 total error is:  19.285109118735654\n",
            "prediction:  0.4504879229373283\n",
            "prediction:  0.8036215636233072\n",
            "prediction:  1.5274582026144043\n",
            "prediction:  1.1901738725758104\n",
            "prediction:  1.927766171802896\n",
            "epoch number:  9 learning rate is:  0.001 total error is:  17.38886015544335\n",
            "prediction:  0.48721149886639115\n",
            "prediction:  0.8688580524871994\n",
            "prediction:  1.6492802832518965\n",
            "prediction:  1.2819563790560553\n",
            "prediction:  2.0740173557522765\n",
            "epoch number:  10 learning rate is:  0.001 total error is:  15.706122876572774\n",
            "prediction:  0.5218121706441994\n",
            "prediction:  0.9303169175968964\n",
            "prediction:  1.7640408079264949\n",
            "prediction:  1.3684205399646587\n",
            "prediction:  2.2117889597793954\n",
            "epoch number:  11 learning rate is:  0.001 total error is:  14.212860205347214\n",
            "prediction:  0.5544129755919528\n",
            "prediction:  0.9882171044734716\n",
            "prediction:  1.8721490601986155\n",
            "prediction:  1.449874601792278\n",
            "prediction:  2.3415724569295238\n",
            "epoch number:  12 learning rate is:  0.001 total error is:  12.88774091297372\n",
            "prediction:  0.5851298198776091\n",
            "prediction:  1.042764868710913\n",
            "prediction:  1.973990601838633\n",
            "prediction:  1.5266089452790936\n",
            "prediction:  2.463830834811194\n",
            "epoch number:  13 learning rate is:  0.001 total error is:  11.7118350357667\n",
            "prediction:  0.6140718918325445\n",
            "prediction:  1.0941545114754365\n",
            "prediction:  2.0699286477252716\n",
            "prediction:  1.5988971209012368\n",
            "prediction:  2.579000246592227\n",
            "epoch number:  14 learning rate is:  0.001 total error is:  10.668343576747102\n",
            "prediction:  0.6413420513126684\n",
            "prediction:  1.1425690723757707\n",
            "prediction:  2.1603053610558436\n",
            "prediction:  1.6669968243411417\n",
            "prediction:  2.6874915663051886\n",
            "epoch number:  15 learning rate is:  0.001 total error is:  9.742358632631682\n",
            "prediction:  0.6670371964924409\n",
            "prediction:  1.188180982175167\n",
            "prediction:  2.2454430734870123\n",
            "prediction:  1.7311508154203206\n",
            "prediction:  2.78969185400842\n",
            "epoch number:  16 learning rate is:  0.001 total error is:  8.920650521505976\n",
            "prediction:  0.6912486093997637\n",
            "prediction:  1.231152677672674\n",
            "prediction:  2.3256454345570394\n",
            "prediction:  1.7915877837714336\n",
            "prediction:  2.8859657360273636\n",
            "epoch number:  17 learning rate is:  0.001 total error is:  8.191478871959525\n",
            "prediction:  0.7140622814239111\n",
            "prediction:  1.2716371809463247\n",
            "prediction:  2.4011984944883302\n",
            "prediction:  1.8485231643366133\n",
            "prediction:  2.976656705198058\n",
            "epoch number:  18 learning rate is:  0.001 total error is:  7.544424976557279\n",
            "prediction:  0.7355592199572479\n",
            "prediction:  1.3097786450237914\n",
            "prediction:  2.472371724231493\n",
            "prediction:  1.902159905600073\n",
            "prediction:  3.06208834574944\n",
            "epoch number:  19 learning rate is:  0.001 total error is:  6.970243016110007\n",
            "prediction:  0.7558157372642076\n",
            "prediction:  1.3457128679263517\n",
            "prediction:  2.53941897638836\n",
            "prediction:  1.9526891932944912\n",
            "prediction:  3.142565487192312\n",
            "epoch number:  20 learning rate is:  0.001 total error is:  6.4607280306236845\n",
            "prediction:  0.7749037226076263\n",
            "prediction:  1.379567776919225\n",
            "prediction:  2.6025793904405794\n",
            "prediction:  2.0002911321618764\n",
            "prediction:  3.218375291329713\n",
            "epoch number:  21 learning rate is:  0.001 total error is:  6.00859875189941\n",
            "prediction:  0.7928908986028249\n",
            "prediction:  1.4114638846950953\n",
            "prediction:  2.6620782455117915\n",
            "prediction:  2.0451353882000527\n",
            "prediction:  3.2897882762649293\n",
            "epoch number:  22 learning rate is:  0.001 total error is:  5.607393624934783\n",
            "prediction:  0.8098410627135852\n",
            "prediction:  1.4415147191175586\n",
            "prediction:  2.718127763704301\n",
            "prediction:  2.08738179368498\n",
            "prediction:  3.357059281058716\n",
            "epoch number:  23 learning rate is:  0.001 total error is:  5.251378533574368\n",
            "prediction:  0.8258143147511916\n",
            "prediction:  1.4698272280569376\n",
            "prediction:  2.770927866874922\n",
            "prediction:  2.1271809171264136\n",
            "prediction:  3.420428374475661\n",
            "epoch number:  24 learning rate is:  0.001 total error is:  4.935464912958705\n",
            "prediction:  0.8408672711877843\n",
            "prediction:  1.496502160762098\n",
            "prediction:  2.8206668895486184\n",
            "prediction:  2.164674600189321\n",
            "prediction:  3.4801217110602334\n",
            "epoch number:  25 learning rate is:  0.001 total error is:  4.6551370796144145\n",
            "prediction:  0.8550532670482606\n",
            "prediction:  1.5216344271282174\n",
            "prediction:  2.8675222505121702\n",
            "prediction:  2.199996463495715\n",
            "prediction:  3.53635233759528\n",
            "epoch number:  26 learning rate is:  0.001 total error is:  4.40638774162885\n",
            "prediction:  0.868422546100664\n",
            "prediction:  1.5453134361416467\n",
            "prediction:  2.91166108548274\n",
            "prediction:  2.2332723831105636\n",
            "prediction:  3.5893209528187344\n",
            "epoch number:  27 learning rate is:  0.001 total error is:  4.185660768141068\n",
            "prediction:  0.8810224400232693\n",
            "prediction:  1.567623414708744\n",
            "prediction:  2.9532408431074075\n",
            "prediction:  2.2646209394109076\n",
            "prediction:  3.6392166231076812\n",
            "epoch number:  28 learning rate is:  0.001 total error is:  3.9898004010231354\n",
            "prediction:  0.8928975371872698\n",
            "prediction:  1.588643708005611\n",
            "prediction:  2.992409846418992\n",
            "prediction:  2.294153839938841\n",
            "prediction:  3.6862174566818764\n",
            "epoch number:  29 learning rate is:  0.001 total error is:  3.8160061836022887\n",
            "prediction:  0.904089841656937\n",
            "prediction:  1.608449062419769\n",
            "prediction:  3.0293078217502827\n",
            "prediction:  2.3219763177462225\n",
            "prediction:  3.7304912387308935\n",
            "epoch number:  30 learning rate is:  0.001 total error is:  3.6617929628979957\n",
            "prediction:  0.9146389229742422\n",
            "prediction:  1.6271098920927314\n",
            "prediction:  3.0640663969927724\n",
            "prediction:  2.348187506651614\n",
            "prediction:  3.77219602972975\n",
            "epoch number:  31 learning rate is:  0.001 total error is:  3.5249553942839214\n",
            "prediction:  0.9245820572620667\n",
            "prediction:  1.6446925300139579\n",
            "prediction:  3.0968095709766628\n",
            "prediction:  2.3728807947475787\n",
            "prediction:  3.8114807290765795\n",
            "epoch number:  32 learning rate is:  0.001 total error is:  3.4035364417673994\n",
            "prediction:  0.9339543601491676\n",
            "prediction:  1.661259464561575\n",
            "prediction:  3.1276541556459327\n",
            "prediction:  2.396144157418953\n",
            "prediction:  3.8484856060622477\n",
            "epoch number:  33 learning rate is:  0.001 total error is:  3.295799424125944\n",
            "prediction:  0.9427889119909053\n",
            "prediction:  1.6768695623333634\n",
            "prediction:  3.156710192605243\n",
            "prediction:  2.4180604710596016\n",
            "prediction:  3.8833428000653525\n",
            "epoch number:  34 learning rate is:  0.001 total error is:  3.2002032077659255\n",
            "prediction:  0.9511168758322667\n",
            "prediction:  1.6915782780626167\n",
            "prediction:  3.184081345524082\n",
            "prediction:  2.4387078086063796\n",
            "prediction:  3.9161767917562638\n",
            "epoch number:  35 learning rate is:  0.001 total error is:  3.1153801920958983\n",
            "prediction:  0.9589676085338309\n",
            "prediction:  1.7054378523674218\n",
            "prediction:  3.209865269797432\n",
            "prediction:  2.458159717944142\n",
            "prediction:  3.947104846990504\n",
            "epoch number:  36 learning rate is:  0.001 total error is:  3.0401167730772007\n",
            "prediction:  0.9663687654569542\n",
            "prediction:  1.7184974980385326\n",
            "prediction:  3.2341539607811702\n",
            "prediction:  2.4764854841746\n",
            "prediction:  3.9762374349743874\n",
            "epoch number:  37 learning rate is:  0.001 total error is:  2.9733360059968246\n",
            "prediction:  0.9733463990814741\n",
            "prediction:  1.7308035755301245\n",
            "prediction:  3.25703408184398\n",
            "prediction:  2.4937503766842553\n",
            "prediction:  4.003678622194061\n",
            "epoch number:  38 learning rate is:  0.001 total error is:  2.914082219907388\n",
            "prediction:  0.9799250519075995\n",
            "prediction:  1.7423997582792308\n",
            "prediction:  3.2785872734056105\n",
            "prediction:  2.51001588189245\n",
            "prediction:  4.029526443512702\n",
            "epoch number:  39 learning rate is:  0.001 total error is:  2.8615073640442605\n",
            "prediction:  0.9861278439732726\n",
            "prediction:  1.7533271884433776\n",
            "prediction:  3.29889044406349\n",
            "prediction:  2.5253399225095\n",
            "prediction:  4.053873251759169\n",
            "epoch number:  40 learning rate is:  0.001 total error is:  2.8148588912588495\n",
            "prediction:  0.9919765552990856\n",
            "prediction:  1.763624623611775\n",
            "prediction:  3.3180160448458436\n",
            "prediction:  2.539777064086784\n",
            "prediction:  4.076806047054756\n",
            "epoch number:  41 learning rate is:  0.001 total error is:  2.7734690054522004\n",
            "prediction:  0.9974917035547473\n",
            "prediction:  1.7733285750132293\n",
            "prediction:  3.3360323275692934\n",
            "prediction:  2.5533787095953273\n",
            "prediction:  4.098406787052378\n",
            "epoch number:  42 learning rate is:  0.001 total error is:  2.736745119468375\n",
            "prediction:  1.002692617224056\n",
            "prediction:  1.7824734377136238\n",
            "prediction:  3.353003588222233\n",
            "prediction:  2.566193282726755\n",
            "prediction:  4.118752679194527\n",
            "epoch number:  43 learning rate is:  0.001 total error is:  2.7041613871898917\n",
            "prediction:  1.0075975045292815\n",
            "prediction:  1.7910916132672408\n",
            "prediction:  3.368990396241871\n",
            "prediction:  2.578266400570244\n",
            "prediction:  4.137916456032151\n",
            "epoch number:  44 learning rate is:  0.001 total error is:  2.6752511889152393\n",
            "prediction:  1.0122235183607382\n",
            "prediction:  1.7992136252593003\n",
            "prediction:  3.3840498105025336\n",
            "prediction:  2.5896410362812357\n",
            "prediction:  4.155966634586246\n",
            "epoch number:  45 learning rate is:  0.001 total error is:  2.649600462709536\n",
            "prediction:  1.0165868174430837\n",
            "prediction:  1.8068682281517343\n",
            "prediction:  3.3982355827854422\n",
            "prediction:  2.600357672321998\n",
            "prediction:  4.172967760677029\n",
            "epoch number:  46 learning rate is:  0.001 total error is:  2.6268417864985882\n",
            "prediction:  1.020702623956465\n",
            "prediction:  1.8140825098203357\n",
            "prediction:  3.41159834945552\n",
            "prediction:  2.6104544448204554\n",
            "prediction:  4.18898063909197\n",
            "epoch number:  47 learning rate is:  0.001 total error is:  2.606649126395945\n",
            "prediction:  1.0245852778179796\n",
            "prediction:  1.8208819881489255\n",
            "prediction:  3.4241858120287487\n",
            "prediction:  2.619967279562091\n",
            "prediction:  4.204062550413436\n",
            "epoch number:  48 learning rate is:  0.001 total error is:  2.5887331762654906\n",
            "prediction:  1.0282482878170243\n",
            "prediction:  1.8272907020249889\n",
            "prediction:  3.4360429072739613\n",
            "prediction:  2.6289300200998515\n",
            "prediction:  4.218267455279159\n",
            "epoch number:  49 learning rate is:  0.001 total error is:  2.572837221964086\n",
            "The RMSE value is:  0.7121670923381398\n",
            "The ZeroR value is:  1.32664991614216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6km1GgQJwQv"
      },
      "source": [
        "### Part 3: Introduction to scikit-learn\n",
        "\n",
        "\n",
        "As we go through this class I'll introduce you to some of the functionality of scikit learn. \n",
        "\n",
        "Below I import the Stochastic Gradient Descent regressor. This learns the coefficients using SGD, and then when predict is called uses those coefficients in the multivariate linear regression formula.\n",
        "\n",
        "Note that when I create an instance of the SGDRegressor, I give a parameter which is the MAXIMUM number of iterations before stopping. The implementation of SGD in scikit learn is a little more advanced than what you did above, in that it will AUTOMATICALLY stop when the amount of learning (the change in error) stops changing significantly enough. \n",
        "\n",
        "I've added some code to print out the intercept (b0), the list of input coefficients, AND the number of iterations before it stopped. \n",
        "\n",
        "Recreate the four lists X_train, y_train, X_test and y_test as you would have for the previous examples, using the same data for both training and testing. Then the following code should run. \n",
        "\n",
        "Note that I have to do some reshaping, to make them into 2D arrays from the 1D list that python uses. This is just a requirement of scikit-learn, and you can see how I do it below.\n",
        "\n",
        "You can find out more in general at: https://scikit-learn.org/stable/index.html\n",
        "\n",
        "This time however, I want you to add the following:\n",
        "- Add in the linear regression from sklearn from Assignment 2(* see the note below)\n",
        "- Add in the dummy regressor (zeroR)\n",
        "- Add in RMSE evaluation for all three models (SGDRegressor, Linear Regression and zeroR)\n",
        "- Print out the RMSE for all three\n",
        "- Add a text box AFTER the results commenting on which model(s) are better and why\n",
        "- As a last thing, CHANGE the max iterations in the SGD regressor to 20. You SHOULD get a warning when you run it about the model NOT converging. This means it did NOT have enough 'time' to find the best solution.\n",
        "\n",
        "Think carefully about structuring your code, and presenting the output. Make it neat and tidy. Consider this a REPORT as well as a coding assignment. \n",
        "\n",
        "(\\*) Last week you used the built in LinearRegression model to do SIMPLE linear regressrion. By default, the linear regression function uses least squares to estimate the coefficients, which means it works for multivariate linear regression too, by doing matrix math. \n",
        "\n",
        "So it's using a different estimation approach to acquire the same coefficients. You could print out THOSE coefficients and see if they are similar to the ones from SGD. There are OTHER ML models (most notably, any neural approach) where least squares DOES NOT work anymore, and we MUST use SGD (or some varient).\n",
        "\n",
        "You can read more here:\n",
        "* [SGDRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)\n",
        "* [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
        "* [zeroR](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)\n",
        "* [Mean Squared Error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvH3q0vxJwQw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9e7edd-9314-4132-cf7b-b3fa297486e9"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn import linear_model\n",
        "import math\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Use this dataset for these examples\n",
        "\n",
        "dataset = [[1,2,1],[2,2,3],[4,3,3],[3,2,2],[5,4,5],[5,2,6],[4,5,4],[7,5,5]]\n",
        "\n",
        "\n",
        "# Create and shape our data\n",
        "# **************************************************************************\n",
        "# CREATE YOUR X_train,y_train, X_Test,y_test datasets here\n",
        "# As before, you should use the same data for X_train and X_test\n",
        "# AND y_train and y_test - although you can ALWAYS test your SGD against the \n",
        "# dataset I gave you earlier\n",
        "\n",
        "\n",
        "X_train = []\n",
        "for i in range(len(dataset)):\n",
        "  X_train.append(dataset[i][:len(dataset[i])-1])\n",
        "\n",
        "y_train = []\n",
        "for i in range(len(dataset)):\n",
        "  y_train.append(dataset[i][-1])\n",
        "\n",
        "X_test = X_train\n",
        "y_test = y_train\n",
        "\n",
        "\n",
        "X_train = np.array(X_train).reshape(-1, 2)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "X_test = np.array(X_test).reshape(-1, 2)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Instantiate an object of the linear regression class\n",
        "lr = LinearRegression()\n",
        "reg = lr.fit(X_train,y_train)\n",
        "\n",
        "# add linear regression\n",
        "lr_predY = reg.predict(X_test)\n",
        "\n",
        "# add ZeroR\n",
        "zr = DummyRegressor()\n",
        "zeroR = zr.fit(X_train, y_train)\n",
        "zr_predY = zeroR.predict(X_test)\n",
        "\n",
        "# **************************************************************************\n",
        "# Create an instance of the SGD Regressor\n",
        "# Create a model, by fitting the X_train values to the y_train values\n",
        "\n",
        "sgd_clf = linear_model.SGDRegressor(max_iter=20)\n",
        "sgd = sgd_clf.fit(X_train, y_train)\n",
        "\n",
        "# Explore some built in attributes\n",
        "# Note: even though we set max_iter to 200\n",
        "# This implementation stops when the learning doesn't improve \"enough\"\n",
        "\n",
        "print('INTERCEPT:',sgd.intercept_)\n",
        "print('COEFFS:',sgd.coef_)\n",
        "print('ITERATIONS TAKEN:',sgd.n_iter_)\n",
        "\n",
        "# Test our model on the X_test data\n",
        "\n",
        "sgd_predY = sgd.predict(X_test)\n",
        "\n",
        "# Print out predicted vs. actual\n",
        "\n",
        "print('\\nPREDICTED : ACTUAL')\n",
        "print('------------------')\n",
        "for i in range(len(sgd_predY)):\n",
        "  print('    {:.2f}  :  {:.2f}  '.format(sgd_predY[i],y_test[i]))\n",
        "print()\n",
        "\n",
        "#find RMSE for all three models\n",
        "slr_rmse = math.sqrt(mean_squared_error(y_test,lr_predY))\n",
        "zr_rmse = math.sqrt(mean_squared_error(y_test,zr_predY))\n",
        "sgd_rmse = math.sqrt(mean_squared_error(y_test, sgd_predY))\n",
        "\n",
        "#print rmse value of three\n",
        "print('RMSE score for linear regression: {:.2f}'.format(slr_rmse))\n",
        "print('RMSE score for zeroR: {:.2f}'.format(zr_rmse))\n",
        "print('RMSE score for sgd_rmse: {:.2f}'.format(sgd_rmse))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INTERCEPT: [0.16735641]\n",
            "COEFFS: [0.65908135 0.26950177]\n",
            "ITERATIONS TAKEN: 20\n",
            "\n",
            "PREDICTED : ACTUAL\n",
            "------------------\n",
            "    1.37  :  1.00  \n",
            "    2.02  :  3.00  \n",
            "    3.61  :  3.00  \n",
            "    2.68  :  2.00  \n",
            "    4.54  :  5.00  \n",
            "    4.00  :  6.00  \n",
            "    4.15  :  4.00  \n",
            "    6.13  :  5.00  \n",
            "\n",
            "RMSE score for linear regression: 0.83\n",
            "RMSE score for zeroR: 1.58\n",
            "RMSE score for sgd_rmse: 0.96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1187: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSw2QDcfVm43"
      },
      "source": [
        "We have that Linear Regression is the best regarding the RMSE score. Since Linear Regression has the lowest RMSE among these three models. SGD is also okay since it's very close to Linear Regression but RMSE score is a bit higher. "
      ]
    }
  ]
}
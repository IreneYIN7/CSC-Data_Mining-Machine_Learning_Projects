{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"provenance":[{"file_id":"1iO1xJG56FSOCsLyxh7L8IePF2ZcXu0pB","timestamp":1619681944427},{"file_id":"1IbrI-cI5YppYzamkwjG70qVOnxxmwkCY","timestamp":1587498044478}],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"HpHYRu1s7Hy5"},"source":["# CSC-321: Data Mining and Machine Learning\n","# Irene Yin\n","\n","## Assignment 4: Logistic Regression\n","\n","### Part 1: Prediction\n","\n","Everything so far has been a regression task - predicting a numeric value. We've moved on to talk about classification in class, so let's implement our first basic binary classifier. This is the same idea as linear regression, but we're going to predict one of two binary classes, using logistic regression.\n","\n","The general outline for logistic regression is the same as for multivariate linear regression. We're going to need a function to make predictions, and a function to learn coefficients. \n","\n","(a) The formula for making a prediction, predY, for logistic regression is:\n","\n","predY = 1.0 / 1.0 + e^-(b0 + b1 * x1 + ... + bN * xN)\n","\n","Where b0 is the intercept or bias, bN is the coefficient for the input variable xN, and e is the base of the natural logarithms, or Euler's number. We can use the python math library which has an implementation of e called math.exp(x): https://docs.python.org/3/library/math.html\n","\n","The formula given above is an implementation of a sigmoid function (a commonly used, s-shaped function that can take any input value and produce a number between 0 and 1).\n","\n","We will assume there can be multiple input features (x1, x2 etc) not just a single value, and that each input feature will have a corresponding coefficient (b1, b2 etc).\n","\n","Write your predict function, that will take a single instance, and a list of coefficients, and return a prediction. In the list of coefficients, assume coefficient[0] corresponds to b0. This will be very similar to your predict function from last week.\n","\n","    "]},{"cell_type":"code","metadata":{"id":"f3Br6uiP7Hy6"},"source":["# Write your predict function here\n","import math\n","def pred(instance, coefficient):\n","  #return: a prediction\n","  exponent = coefficient[0]\n","  for i in range(len(coefficient)-1):\n","    exponent += coefficient[i+1]*instance[i]\n","  predY = 1.0/(1.0 + math.exp(-exponent))\n","  return predY\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gY6ryL4P7Hy-"},"source":["We can test your predict function on the contrived dataset below. It includes TWO input features (we shall call x1 and x2) and a class feature (y) for each instance. The class is either 0 or 1.\n","\n","(b) Graph the instances in this data, x1 (on the x axis) against x2 (on the y axis), using different colored points for the two classes. You may choose the colors. Use axes ranges of 0-10 on the x, and -1 to 6 on the y.\n","\n","(c) Call your predict function on each instance in the contrived data set, using the coefficients given below. Get the predicted class from your function, and print (for each instance), the expected class, the predicted value AND the predicted class. In order to get the predicted class from the value predicted, we need to do rounding. There is a round() function that can help you. If it works correctly, you should predict the correct class of each instance in the dataset."]},{"cell_type":"code","metadata":{"id":"5opNDZSa7Hy_","colab":{"base_uri":"https://localhost:8080/","height":459},"executionInfo":{"status":"ok","timestamp":1622117718226,"user_tz":240,"elapsed":32,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"fc5d14ec-e3c8-4e59-a1b7-46fa1f7346bb"},"source":["# Here's the contrived data set\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","dataset = [[2.7810836,2.550537003,0],\n","    [1.465489372,2.362125076,0],\n","    [3.396561688,4.400293529,0],\n","    [1.38807019,1.850220317,0],\n","    [3.06407232,3.005305973,0],\n","    [7.627531214,2.759262235,1],\n","    [5.332441248,2.088626775,1],\n","    [6.922596716,1.77106367,1],\n","    [8.675418651,-0.242068655,1],\n","    [7.673756466,3.508563011,1]]\n","\n","# Do the graphing here\n","dataX0 = []\n","dataX1 = []\n","dataY0 = []\n","dataY1 = []\n","for i in dataset:\n","    if i[2] == 0:\n","        dataX0.append(i[0])\n","        dataY0.append(i[1])\n","    else:\n","        dataX1.append(i[0])\n","        dataY1.append(i[1])\n","\n","plt.plot(dataX0, dataY0,'b^', label = '0 class')\n","plt.plot(dataX1, dataY1,'go', label = '1 class')\n","plt.axis([0,10,-1,6])\n","plt.show()\n","\n","\n","# Call your predict function on the data here, using the following coefficients\n","\n","\n","coef = [-0.406605464, 0.852573316, -1.104746259]\n","predValue = [pred(i,coef) for i in dataset]\n","\n","\n","for index in range(len(predValue)):\n","  print(\"expected class: \", dataset[index][2], \"predicted value: \", predValue[index], \n","        \"predicted class: \", round(predValue[index]))\n","\n","# Go through each instance of X_train, and get a predicted y value\n","# Print out the predicted y, and the corresponding actual y from y_train\n","\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["expected class:  0 predicted value:  0.2987569855650975 predicted class:  0\n","expected class:  0 predicted value:  0.14595105593031163 predicted class:  0\n","expected class:  0 predicted value:  0.08533326519733725 predicted class:  0\n","expected class:  0 predicted value:  0.21973731424800344 predicted class:  0\n","expected class:  0 predicted value:  0.24705900008926596 predicted class:  0\n","expected class:  1 predicted value:  0.9547021347460022 predicted class:  1\n","expected class:  1 predicted value:  0.8620341905282771 predicted class:  1\n","expected class:  1 predicted value:  0.9717729050420985 predicted class:  1\n","expected class:  1 predicted value:  0.9992954520878627 predicted class:  1\n","expected class:  1 predicted value:  0.9054893228110497 predicted class:  1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ISHioihS7HzC"},"source":["### Part 2: Learning coefficients\n","\n","(d) Above I gave you coefficients. Just as with MLR, we need to estimate the coefficients for a data set. To do that, we're going to use stochastic gradient descent. The algorithm is exactly the same as for multivariate linear regression except for the following two things.\n","\n","b0 is computed by:\n","\n","b0 = b0 + learning_rate * error * predictedY * (1.0 - predictedY)\n","\n","and bN is computed by:\n","\n","bN = bN + learning_rate * error * predictedY * (1.0 - predictedY) * xN\n","\n","for all coefficients b1..bN\n","\n","To **calculate the error**, we run the algorithm with default coefficients and perform prediction, then get the error by subtracting the **predictedY** from the **actual Y value**. This is the opposite way around to last week. I'm just keeping you on your toes (actually, I've reversed the gradient. You're welcome).\n","\n","Refer back to Assignment 3 for the complete algorithm\n","\n","(e) Apply your coefficient learning function to the contrived dataset given above, using the learning rate of 0.3, and 100 epochs. Print the resulting coefficients. I've shown the last 5 epochs of my code over this example, so you can check your code. YOUR OUTPUT SHOULD BE THE SAME VALUES.\n"]},{"cell_type":"code","metadata":{"id":"1ByMSNCE7HzC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622120789335,"user_tz":240,"elapsed":526,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"3b20d0a4-32ba-4b1d-9f25-6c9eff7aadb6"},"source":["# Write your function sgd_log(X_train, y_train, learning_rate, epochs) here\n","\n","def sgd_log(X_train, y_train, learning_rate, epochs):\n","  # return the coefficients.\n","  coefficient = [0.0 for j in range(len(X_train[0])+1)]\n","  for iteration in range(epochs):\n","    Totalerror = 0\n","    for i in range(len(X_train)):\n","      preditedY = pred(X_train[i],coefficient)\n","      error = y_train[i] - preditedY\n","      Totalerror += error**2\n","      coefficient[0] += learning_rate * error * preditedY*(1.0-preditedY)\n","      for index in range(len(coefficient)-1):\n","        coefficient[index+1] += learning_rate * error *preditedY*(1.0-preditedY)* X_train[i][index]\n","    print(\">epoch=\", iteration, \"lrate=\", learning_rate,\n","          \"error=%.3f\" %Totalerror)\n","  return coefficient\n","\n","\n","\n","# Call your function using the parameters given here. \n","learning_rate = 0.3\n","epochs = 100\n","X_train = []\n","y_train = []\n","for i in range(len(dataset)):\n","  X_train.append(dataset[i][:len(dataset[i])-1])\n","\n","for i in range(len(dataset)):\n","  y_train.append(dataset[i][-1])\n","\n","coefficient = sgd_log(X_train, y_train, learning_rate, epochs)\n","print(\"the coefficient is: \", coefficient)\n","\n","\n","\n","# Example output\n","#\n","#>epoch=95, lrate=0.300, error=0.023\n","#>epoch=96, lrate=0.300, error=0.023\n","#>epoch=97, lrate=0.300, error=0.023\n","#>epoch=98, lrate=0.300, error=0.023\n","#>epoch=99, lrate=0.300, error=0.022\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":[">epoch= 0 lrate= 0.3 error=2.217\n",">epoch= 1 lrate= 0.3 error=1.613\n",">epoch= 2 lrate= 0.3 error=1.113\n",">epoch= 3 lrate= 0.3 error=0.827\n",">epoch= 4 lrate= 0.3 error=0.623\n",">epoch= 5 lrate= 0.3 error=0.494\n",">epoch= 6 lrate= 0.3 error=0.412\n",">epoch= 7 lrate= 0.3 error=0.354\n",">epoch= 8 lrate= 0.3 error=0.310\n",">epoch= 9 lrate= 0.3 error=0.276\n",">epoch= 10 lrate= 0.3 error=0.248\n",">epoch= 11 lrate= 0.3 error=0.224\n",">epoch= 12 lrate= 0.3 error=0.205\n",">epoch= 13 lrate= 0.3 error=0.189\n",">epoch= 14 lrate= 0.3 error=0.174\n",">epoch= 15 lrate= 0.3 error=0.162\n",">epoch= 16 lrate= 0.3 error=0.151\n",">epoch= 17 lrate= 0.3 error=0.142\n",">epoch= 18 lrate= 0.3 error=0.134\n",">epoch= 19 lrate= 0.3 error=0.126\n",">epoch= 20 lrate= 0.3 error=0.119\n",">epoch= 21 lrate= 0.3 error=0.113\n",">epoch= 22 lrate= 0.3 error=0.108\n",">epoch= 23 lrate= 0.3 error=0.103\n",">epoch= 24 lrate= 0.3 error=0.098\n",">epoch= 25 lrate= 0.3 error=0.094\n",">epoch= 26 lrate= 0.3 error=0.090\n",">epoch= 27 lrate= 0.3 error=0.087\n",">epoch= 28 lrate= 0.3 error=0.084\n",">epoch= 29 lrate= 0.3 error=0.080\n",">epoch= 30 lrate= 0.3 error=0.078\n",">epoch= 31 lrate= 0.3 error=0.075\n",">epoch= 32 lrate= 0.3 error=0.073\n",">epoch= 33 lrate= 0.3 error=0.070\n",">epoch= 34 lrate= 0.3 error=0.068\n",">epoch= 35 lrate= 0.3 error=0.066\n",">epoch= 36 lrate= 0.3 error=0.064\n",">epoch= 37 lrate= 0.3 error=0.062\n",">epoch= 38 lrate= 0.3 error=0.060\n",">epoch= 39 lrate= 0.3 error=0.059\n",">epoch= 40 lrate= 0.3 error=0.057\n",">epoch= 41 lrate= 0.3 error=0.056\n",">epoch= 42 lrate= 0.3 error=0.054\n",">epoch= 43 lrate= 0.3 error=0.053\n",">epoch= 44 lrate= 0.3 error=0.052\n",">epoch= 45 lrate= 0.3 error=0.051\n",">epoch= 46 lrate= 0.3 error=0.050\n",">epoch= 47 lrate= 0.3 error=0.048\n",">epoch= 48 lrate= 0.3 error=0.047\n",">epoch= 49 lrate= 0.3 error=0.046\n",">epoch= 50 lrate= 0.3 error=0.045\n",">epoch= 51 lrate= 0.3 error=0.044\n",">epoch= 52 lrate= 0.3 error=0.044\n",">epoch= 53 lrate= 0.3 error=0.043\n",">epoch= 54 lrate= 0.3 error=0.042\n",">epoch= 55 lrate= 0.3 error=0.041\n",">epoch= 56 lrate= 0.3 error=0.040\n",">epoch= 57 lrate= 0.3 error=0.040\n",">epoch= 58 lrate= 0.3 error=0.039\n",">epoch= 59 lrate= 0.3 error=0.038\n",">epoch= 60 lrate= 0.3 error=0.038\n",">epoch= 61 lrate= 0.3 error=0.037\n",">epoch= 62 lrate= 0.3 error=0.036\n",">epoch= 63 lrate= 0.3 error=0.036\n",">epoch= 64 lrate= 0.3 error=0.035\n",">epoch= 65 lrate= 0.3 error=0.035\n",">epoch= 66 lrate= 0.3 error=0.034\n",">epoch= 67 lrate= 0.3 error=0.033\n",">epoch= 68 lrate= 0.3 error=0.033\n",">epoch= 69 lrate= 0.3 error=0.032\n",">epoch= 70 lrate= 0.3 error=0.032\n",">epoch= 71 lrate= 0.3 error=0.032\n",">epoch= 72 lrate= 0.3 error=0.031\n",">epoch= 73 lrate= 0.3 error=0.031\n",">epoch= 74 lrate= 0.3 error=0.030\n",">epoch= 75 lrate= 0.3 error=0.030\n",">epoch= 76 lrate= 0.3 error=0.029\n",">epoch= 77 lrate= 0.3 error=0.029\n",">epoch= 78 lrate= 0.3 error=0.029\n",">epoch= 79 lrate= 0.3 error=0.028\n",">epoch= 80 lrate= 0.3 error=0.028\n",">epoch= 81 lrate= 0.3 error=0.027\n",">epoch= 82 lrate= 0.3 error=0.027\n",">epoch= 83 lrate= 0.3 error=0.027\n",">epoch= 84 lrate= 0.3 error=0.026\n",">epoch= 85 lrate= 0.3 error=0.026\n",">epoch= 86 lrate= 0.3 error=0.026\n",">epoch= 87 lrate= 0.3 error=0.026\n",">epoch= 88 lrate= 0.3 error=0.025\n",">epoch= 89 lrate= 0.3 error=0.025\n",">epoch= 90 lrate= 0.3 error=0.025\n",">epoch= 91 lrate= 0.3 error=0.024\n",">epoch= 92 lrate= 0.3 error=0.024\n",">epoch= 93 lrate= 0.3 error=0.024\n",">epoch= 94 lrate= 0.3 error=0.024\n",">epoch= 95 lrate= 0.3 error=0.023\n",">epoch= 96 lrate= 0.3 error=0.023\n",">epoch= 97 lrate= 0.3 error=0.023\n",">epoch= 98 lrate= 0.3 error=0.023\n",">epoch= 99 lrate= 0.3 error=0.022\n","the coefficient is:  [-0.8596443546618897, 1.5223825112460005, -2.218700210565016]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"edsPCAst-shI"},"source":["### Part 3: Classification algorithms\n","\n","(f) Now you have sufficient functionality to write a function to make predictions using logistic regression. Create a function with the signature log_reg(X_train, y_train, X_test, learning_rate, epochs). \n","\n","We're going to use the same dataset here for both training and testing, even though we know that might not be a great idea.\n","\n","Here's the log_reg algorithm. We're going to estimate our coefficients from the training data, using the function from (d) above, and we'll print them out. We're going to create a new list, to hold our predictions. Then for each entry in the testing data, we're going to read the input value, and make a prediction, using our function from (a). For each entry in the test data, we're going to append our predicted y class to the prediction list. In order to predict the class correctly, we need to round our score, as above, before appending it. We're going to return our list of predictions."]},{"cell_type":"code","metadata":{"id":"FLz2rvYE_uZs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622119750829,"user_tz":240,"elapsed":1027,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"b927ba33-4e56-4f55-d99f-e143ee06ab18"},"source":["# Write your function log_reg(X_train, y_train, X_test, learning_rate, epochs) here\n","def log_reg(X_train, y_train, X_test, learning_rate, epochs):\n","  # return prediction\n","  coefficient = sgd_log(X_train, y_train, learning_rate, epochs)\n","  print(\"Coefficient is: \", coefficient)\n","  predictions = []\n","  for i in range(len(X_test)):\n","    predictY = pred(X_test[i],coefficient)\n","    predictions.append(round(predictY))\n","  return predictions\n","X_test = X_train\n","epochs1 = 10\n","print(log_reg(X_train, y_train, X_test, learning_rate, epochs))"],"execution_count":null,"outputs":[{"output_type":"stream","text":[">epoch= 0 lrate= 0.3 error=2.217\n",">epoch= 1 lrate= 0.3 error=1.613\n",">epoch= 2 lrate= 0.3 error=1.113\n",">epoch= 3 lrate= 0.3 error=0.827\n",">epoch= 4 lrate= 0.3 error=0.623\n",">epoch= 5 lrate= 0.3 error=0.494\n",">epoch= 6 lrate= 0.3 error=0.412\n",">epoch= 7 lrate= 0.3 error=0.354\n",">epoch= 8 lrate= 0.3 error=0.310\n",">epoch= 9 lrate= 0.3 error=0.276\n",">epoch= 10 lrate= 0.3 error=0.248\n",">epoch= 11 lrate= 0.3 error=0.224\n",">epoch= 12 lrate= 0.3 error=0.205\n",">epoch= 13 lrate= 0.3 error=0.189\n",">epoch= 14 lrate= 0.3 error=0.174\n",">epoch= 15 lrate= 0.3 error=0.162\n",">epoch= 16 lrate= 0.3 error=0.151\n",">epoch= 17 lrate= 0.3 error=0.142\n",">epoch= 18 lrate= 0.3 error=0.134\n",">epoch= 19 lrate= 0.3 error=0.126\n",">epoch= 20 lrate= 0.3 error=0.119\n",">epoch= 21 lrate= 0.3 error=0.113\n",">epoch= 22 lrate= 0.3 error=0.108\n",">epoch= 23 lrate= 0.3 error=0.103\n",">epoch= 24 lrate= 0.3 error=0.098\n",">epoch= 25 lrate= 0.3 error=0.094\n",">epoch= 26 lrate= 0.3 error=0.090\n",">epoch= 27 lrate= 0.3 error=0.087\n",">epoch= 28 lrate= 0.3 error=0.084\n",">epoch= 29 lrate= 0.3 error=0.080\n",">epoch= 30 lrate= 0.3 error=0.078\n",">epoch= 31 lrate= 0.3 error=0.075\n",">epoch= 32 lrate= 0.3 error=0.073\n",">epoch= 33 lrate= 0.3 error=0.070\n",">epoch= 34 lrate= 0.3 error=0.068\n",">epoch= 35 lrate= 0.3 error=0.066\n",">epoch= 36 lrate= 0.3 error=0.064\n",">epoch= 37 lrate= 0.3 error=0.062\n",">epoch= 38 lrate= 0.3 error=0.060\n",">epoch= 39 lrate= 0.3 error=0.059\n",">epoch= 40 lrate= 0.3 error=0.057\n",">epoch= 41 lrate= 0.3 error=0.056\n",">epoch= 42 lrate= 0.3 error=0.054\n",">epoch= 43 lrate= 0.3 error=0.053\n",">epoch= 44 lrate= 0.3 error=0.052\n",">epoch= 45 lrate= 0.3 error=0.051\n",">epoch= 46 lrate= 0.3 error=0.050\n",">epoch= 47 lrate= 0.3 error=0.048\n",">epoch= 48 lrate= 0.3 error=0.047\n",">epoch= 49 lrate= 0.3 error=0.046\n",">epoch= 50 lrate= 0.3 error=0.045\n",">epoch= 51 lrate= 0.3 error=0.044\n",">epoch= 52 lrate= 0.3 error=0.044\n",">epoch= 53 lrate= 0.3 error=0.043\n",">epoch= 54 lrate= 0.3 error=0.042\n",">epoch= 55 lrate= 0.3 error=0.041\n",">epoch= 56 lrate= 0.3 error=0.040\n",">epoch= 57 lrate= 0.3 error=0.040\n",">epoch= 58 lrate= 0.3 error=0.039\n",">epoch= 59 lrate= 0.3 error=0.038\n",">epoch= 60 lrate= 0.3 error=0.038\n",">epoch= 61 lrate= 0.3 error=0.037\n",">epoch= 62 lrate= 0.3 error=0.036\n",">epoch= 63 lrate= 0.3 error=0.036\n",">epoch= 64 lrate= 0.3 error=0.035\n",">epoch= 65 lrate= 0.3 error=0.035\n",">epoch= 66 lrate= 0.3 error=0.034\n",">epoch= 67 lrate= 0.3 error=0.033\n",">epoch= 68 lrate= 0.3 error=0.033\n",">epoch= 69 lrate= 0.3 error=0.032\n",">epoch= 70 lrate= 0.3 error=0.032\n",">epoch= 71 lrate= 0.3 error=0.032\n",">epoch= 72 lrate= 0.3 error=0.031\n",">epoch= 73 lrate= 0.3 error=0.031\n",">epoch= 74 lrate= 0.3 error=0.030\n",">epoch= 75 lrate= 0.3 error=0.030\n",">epoch= 76 lrate= 0.3 error=0.029\n",">epoch= 77 lrate= 0.3 error=0.029\n",">epoch= 78 lrate= 0.3 error=0.029\n",">epoch= 79 lrate= 0.3 error=0.028\n",">epoch= 80 lrate= 0.3 error=0.028\n",">epoch= 81 lrate= 0.3 error=0.027\n",">epoch= 82 lrate= 0.3 error=0.027\n",">epoch= 83 lrate= 0.3 error=0.027\n",">epoch= 84 lrate= 0.3 error=0.026\n",">epoch= 85 lrate= 0.3 error=0.026\n",">epoch= 86 lrate= 0.3 error=0.026\n",">epoch= 87 lrate= 0.3 error=0.026\n",">epoch= 88 lrate= 0.3 error=0.025\n",">epoch= 89 lrate= 0.3 error=0.025\n",">epoch= 90 lrate= 0.3 error=0.025\n",">epoch= 91 lrate= 0.3 error=0.024\n",">epoch= 92 lrate= 0.3 error=0.024\n",">epoch= 93 lrate= 0.3 error=0.024\n",">epoch= 94 lrate= 0.3 error=0.024\n",">epoch= 95 lrate= 0.3 error=0.023\n",">epoch= 96 lrate= 0.3 error=0.023\n",">epoch= 97 lrate= 0.3 error=0.023\n",">epoch= 98 lrate= 0.3 error=0.023\n",">epoch= 99 lrate= 0.3 error=0.022\n","Coefficient is:  [-0.8596443546618897, 1.5223825112460005, -2.218700210565016]\n","[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Rj010Z_w7HzF"},"source":["\n","\n","(g) Now we need a function for calculating accuracy. We need to keep count of how many predictions are correct. It will take a list of actual class values, and a list of predicted class values. If the actual value of an instance and the predicted value of an instance are the same, increment a counter. In the end, return the value of this counter divided by the length of the actual values list. Call this function accuracy.\n","\n","(h) We also need a baseline function. Create a function called zeroRC(train, test). This function should take the training data, and find the most common value of Y in the training data. It should then return a list of predictions the same length as the test data, containing ONLY this value that was most common. It works very similarly to the zeroR function you've been using for linear regression.\n","\n","(i) Apply logistic regression and zeroRC to the sample dataset. Use a learning rate of 0.3 and 100 epochs for logistic regression. Calculate accuracy for both algorithms, and print the results as a percentage accuracy. Give me a write up on the experiment below - which algorithm is better?\n","\n","(j) See how the error rate for the SGD learned coefficients goes down? Write in the text box the error rate at iteration 100, and the error rate at iteration 10. Run the code with just 10 epochs. Does the accuracy of the overall classifier change? \n"]},{"cell_type":"code","metadata":{"id":"1S5derU87HzG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622117718742,"user_tz":240,"elapsed":534,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"9911728d-a711-46ef-af22-37b1c7999cb6"},"source":["# Do all the code here\n","#g)\n","def accuracy(actual_value, predicted_value):\n","  # return how many times the function predicts correctly in ratio.\n","  counter = 0 \n","  for i in range(len(actual_value)):\n","    if actual_value[i] == predicted_value[i]:\n","      counter += 1\n","  return counter/len(actual_value)\n","#h)\n","def zeroRC(train, test):\n","  # return baseline\n","  commonElement = max(set(train), key = train.count)\n","  return [commonElement for i in range(len(test))]\n","#i)\n","X_test = X_train\n","logReg_prediction = log_reg(X_train, y_train,X_test,learning_rate,epochs)\n","zeroRC_prediction = zeroRC(y_train, X_test)\n","accuracy_logReg = accuracy(y_train, logReg_prediction) * 100\n","accuracy_zeroRC = accuracy(y_train, zeroRC_prediction) * 100\n","print(\"Logistic Regression prediction accuracy = \", accuracy_logReg)\n","print(\"ZeroR prediction accuracy = \", accuracy_zeroRC)\n","\n","#j)\n","epochs1 = 10\n","print(sgd_log(X_train,y_train, learning_rate, epochs))\n","print(sgd_log(X_train, y_train, learning_rate,epochs1))\n","\n","predic1 = log_reg(X_train,y_train,X_test,learning_rate,epochs)\n","predic2 = log_reg(X_train,y_train,X_test,learning_rate,epochs1)\n","accuracy1 = accuracy(y_train, predic1)\n","accuracy2 = accuracy(y_train, predic2)\n","print(\"The accuracy for iteration 100 = \", accuracy1*100)\n","print(\"The accuracy for iteration 10 = \", accuracy2*100)\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":[">epoch= 0 lrate= 0.3 error=2.217\n",">epoch= 1 lrate= 0.3 error=1.613\n",">epoch= 2 lrate= 0.3 error=1.113\n",">epoch= 3 lrate= 0.3 error=0.827\n",">epoch= 4 lrate= 0.3 error=0.623\n",">epoch= 5 lrate= 0.3 error=0.494\n",">epoch= 6 lrate= 0.3 error=0.412\n",">epoch= 7 lrate= 0.3 error=0.354\n",">epoch= 8 lrate= 0.3 error=0.310\n",">epoch= 9 lrate= 0.3 error=0.276\n",">epoch= 10 lrate= 0.3 error=0.248\n",">epoch= 11 lrate= 0.3 error=0.224\n",">epoch= 12 lrate= 0.3 error=0.205\n",">epoch= 13 lrate= 0.3 error=0.189\n",">epoch= 14 lrate= 0.3 error=0.174\n",">epoch= 15 lrate= 0.3 error=0.162\n",">epoch= 16 lrate= 0.3 error=0.151\n",">epoch= 17 lrate= 0.3 error=0.142\n",">epoch= 18 lrate= 0.3 error=0.134\n",">epoch= 19 lrate= 0.3 error=0.126\n",">epoch= 20 lrate= 0.3 error=0.119\n",">epoch= 21 lrate= 0.3 error=0.113\n",">epoch= 22 lrate= 0.3 error=0.108\n",">epoch= 23 lrate= 0.3 error=0.103\n",">epoch= 24 lrate= 0.3 error=0.098\n",">epoch= 25 lrate= 0.3 error=0.094\n",">epoch= 26 lrate= 0.3 error=0.090\n",">epoch= 27 lrate= 0.3 error=0.087\n",">epoch= 28 lrate= 0.3 error=0.084\n",">epoch= 29 lrate= 0.3 error=0.080\n",">epoch= 30 lrate= 0.3 error=0.078\n",">epoch= 31 lrate= 0.3 error=0.075\n",">epoch= 32 lrate= 0.3 error=0.073\n",">epoch= 33 lrate= 0.3 error=0.070\n",">epoch= 34 lrate= 0.3 error=0.068\n",">epoch= 35 lrate= 0.3 error=0.066\n",">epoch= 36 lrate= 0.3 error=0.064\n",">epoch= 37 lrate= 0.3 error=0.062\n",">epoch= 38 lrate= 0.3 error=0.060\n",">epoch= 39 lrate= 0.3 error=0.059\n",">epoch= 40 lrate= 0.3 error=0.057\n",">epoch= 41 lrate= 0.3 error=0.056\n",">epoch= 42 lrate= 0.3 error=0.054\n",">epoch= 43 lrate= 0.3 error=0.053\n",">epoch= 44 lrate= 0.3 error=0.052\n",">epoch= 45 lrate= 0.3 error=0.051\n",">epoch= 46 lrate= 0.3 error=0.050\n",">epoch= 47 lrate= 0.3 error=0.048\n",">epoch= 48 lrate= 0.3 error=0.047\n",">epoch= 49 lrate= 0.3 error=0.046\n",">epoch= 50 lrate= 0.3 error=0.045\n",">epoch= 51 lrate= 0.3 error=0.044\n",">epoch= 52 lrate= 0.3 error=0.044\n",">epoch= 53 lrate= 0.3 error=0.043\n",">epoch= 54 lrate= 0.3 error=0.042\n",">epoch= 55 lrate= 0.3 error=0.041\n",">epoch= 56 lrate= 0.3 error=0.040\n",">epoch= 57 lrate= 0.3 error=0.040\n",">epoch= 58 lrate= 0.3 error=0.039\n",">epoch= 59 lrate= 0.3 error=0.038\n",">epoch= 60 lrate= 0.3 error=0.038\n",">epoch= 61 lrate= 0.3 error=0.037\n",">epoch= 62 lrate= 0.3 error=0.036\n",">epoch= 63 lrate= 0.3 error=0.036\n",">epoch= 64 lrate= 0.3 error=0.035\n",">epoch= 65 lrate= 0.3 error=0.035\n",">epoch= 66 lrate= 0.3 error=0.034\n",">epoch= 67 lrate= 0.3 error=0.033\n",">epoch= 68 lrate= 0.3 error=0.033\n",">epoch= 69 lrate= 0.3 error=0.032\n",">epoch= 70 lrate= 0.3 error=0.032\n",">epoch= 71 lrate= 0.3 error=0.032\n",">epoch= 72 lrate= 0.3 error=0.031\n",">epoch= 73 lrate= 0.3 error=0.031\n",">epoch= 74 lrate= 0.3 error=0.030\n",">epoch= 75 lrate= 0.3 error=0.030\n",">epoch= 76 lrate= 0.3 error=0.029\n",">epoch= 77 lrate= 0.3 error=0.029\n",">epoch= 78 lrate= 0.3 error=0.029\n",">epoch= 79 lrate= 0.3 error=0.028\n",">epoch= 80 lrate= 0.3 error=0.028\n",">epoch= 81 lrate= 0.3 error=0.027\n",">epoch= 82 lrate= 0.3 error=0.027\n",">epoch= 83 lrate= 0.3 error=0.027\n",">epoch= 84 lrate= 0.3 error=0.026\n",">epoch= 85 lrate= 0.3 error=0.026\n",">epoch= 86 lrate= 0.3 error=0.026\n",">epoch= 87 lrate= 0.3 error=0.026\n",">epoch= 88 lrate= 0.3 error=0.025\n",">epoch= 89 lrate= 0.3 error=0.025\n",">epoch= 90 lrate= 0.3 error=0.025\n",">epoch= 91 lrate= 0.3 error=0.024\n",">epoch= 92 lrate= 0.3 error=0.024\n",">epoch= 93 lrate= 0.3 error=0.024\n",">epoch= 94 lrate= 0.3 error=0.024\n",">epoch= 95 lrate= 0.3 error=0.023\n",">epoch= 96 lrate= 0.3 error=0.023\n",">epoch= 97 lrate= 0.3 error=0.023\n",">epoch= 98 lrate= 0.3 error=0.023\n",">epoch= 99 lrate= 0.3 error=0.022\n","Coefficient is:  [-0.8596443546618897, 1.5223825112460005, -2.218700210565016]\n","Logistic Regression prediction accuracy =  100.0\n","ZeroR prediction accuracy =  50.0\n",">epoch= 0 lrate= 0.3 error=2.217\n",">epoch= 1 lrate= 0.3 error=1.613\n",">epoch= 2 lrate= 0.3 error=1.113\n",">epoch= 3 lrate= 0.3 error=0.827\n",">epoch= 4 lrate= 0.3 error=0.623\n",">epoch= 5 lrate= 0.3 error=0.494\n",">epoch= 6 lrate= 0.3 error=0.412\n",">epoch= 7 lrate= 0.3 error=0.354\n",">epoch= 8 lrate= 0.3 error=0.310\n",">epoch= 9 lrate= 0.3 error=0.276\n",">epoch= 10 lrate= 0.3 error=0.248\n",">epoch= 11 lrate= 0.3 error=0.224\n",">epoch= 12 lrate= 0.3 error=0.205\n",">epoch= 13 lrate= 0.3 error=0.189\n",">epoch= 14 lrate= 0.3 error=0.174\n",">epoch= 15 lrate= 0.3 error=0.162\n",">epoch= 16 lrate= 0.3 error=0.151\n",">epoch= 17 lrate= 0.3 error=0.142\n",">epoch= 18 lrate= 0.3 error=0.134\n",">epoch= 19 lrate= 0.3 error=0.126\n",">epoch= 20 lrate= 0.3 error=0.119\n",">epoch= 21 lrate= 0.3 error=0.113\n",">epoch= 22 lrate= 0.3 error=0.108\n",">epoch= 23 lrate= 0.3 error=0.103\n",">epoch= 24 lrate= 0.3 error=0.098\n",">epoch= 25 lrate= 0.3 error=0.094\n",">epoch= 26 lrate= 0.3 error=0.090\n",">epoch= 27 lrate= 0.3 error=0.087\n",">epoch= 28 lrate= 0.3 error=0.084\n",">epoch= 29 lrate= 0.3 error=0.080\n",">epoch= 30 lrate= 0.3 error=0.078\n",">epoch= 31 lrate= 0.3 error=0.075\n",">epoch= 32 lrate= 0.3 error=0.073\n",">epoch= 33 lrate= 0.3 error=0.070\n",">epoch= 34 lrate= 0.3 error=0.068\n",">epoch= 35 lrate= 0.3 error=0.066\n",">epoch= 36 lrate= 0.3 error=0.064\n",">epoch= 37 lrate= 0.3 error=0.062\n",">epoch= 38 lrate= 0.3 error=0.060\n",">epoch= 39 lrate= 0.3 error=0.059\n",">epoch= 40 lrate= 0.3 error=0.057\n",">epoch= 41 lrate= 0.3 error=0.056\n",">epoch= 42 lrate= 0.3 error=0.054\n",">epoch= 43 lrate= 0.3 error=0.053\n",">epoch= 44 lrate= 0.3 error=0.052\n",">epoch= 45 lrate= 0.3 error=0.051\n",">epoch= 46 lrate= 0.3 error=0.050\n",">epoch= 47 lrate= 0.3 error=0.048\n",">epoch= 48 lrate= 0.3 error=0.047\n",">epoch= 49 lrate= 0.3 error=0.046\n",">epoch= 50 lrate= 0.3 error=0.045\n",">epoch= 51 lrate= 0.3 error=0.044\n",">epoch= 52 lrate= 0.3 error=0.044\n",">epoch= 53 lrate= 0.3 error=0.043\n",">epoch= 54 lrate= 0.3 error=0.042\n",">epoch= 55 lrate= 0.3 error=0.041\n",">epoch= 56 lrate= 0.3 error=0.040\n",">epoch= 57 lrate= 0.3 error=0.040\n",">epoch= 58 lrate= 0.3 error=0.039\n",">epoch= 59 lrate= 0.3 error=0.038\n",">epoch= 60 lrate= 0.3 error=0.038\n",">epoch= 61 lrate= 0.3 error=0.037\n",">epoch= 62 lrate= 0.3 error=0.036\n",">epoch= 63 lrate= 0.3 error=0.036\n",">epoch= 64 lrate= 0.3 error=0.035\n",">epoch= 65 lrate= 0.3 error=0.035\n",">epoch= 66 lrate= 0.3 error=0.034\n",">epoch= 67 lrate= 0.3 error=0.033\n",">epoch= 68 lrate= 0.3 error=0.033\n",">epoch= 69 lrate= 0.3 error=0.032\n",">epoch= 70 lrate= 0.3 error=0.032\n",">epoch= 71 lrate= 0.3 error=0.032\n",">epoch= 72 lrate= 0.3 error=0.031\n",">epoch= 73 lrate= 0.3 error=0.031\n",">epoch= 74 lrate= 0.3 error=0.030\n",">epoch= 75 lrate= 0.3 error=0.030\n",">epoch= 76 lrate= 0.3 error=0.029\n",">epoch= 77 lrate= 0.3 error=0.029\n",">epoch= 78 lrate= 0.3 error=0.029\n",">epoch= 79 lrate= 0.3 error=0.028\n",">epoch= 80 lrate= 0.3 error=0.028\n",">epoch= 81 lrate= 0.3 error=0.027\n",">epoch= 82 lrate= 0.3 error=0.027\n",">epoch= 83 lrate= 0.3 error=0.027\n",">epoch= 84 lrate= 0.3 error=0.026\n",">epoch= 85 lrate= 0.3 error=0.026\n",">epoch= 86 lrate= 0.3 error=0.026\n",">epoch= 87 lrate= 0.3 error=0.026\n",">epoch= 88 lrate= 0.3 error=0.025\n",">epoch= 89 lrate= 0.3 error=0.025\n",">epoch= 90 lrate= 0.3 error=0.025\n",">epoch= 91 lrate= 0.3 error=0.024\n",">epoch= 92 lrate= 0.3 error=0.024\n",">epoch= 93 lrate= 0.3 error=0.024\n",">epoch= 94 lrate= 0.3 error=0.024\n",">epoch= 95 lrate= 0.3 error=0.023\n",">epoch= 96 lrate= 0.3 error=0.023\n",">epoch= 97 lrate= 0.3 error=0.023\n",">epoch= 98 lrate= 0.3 error=0.023\n",">epoch= 99 lrate= 0.3 error=0.022\n","[-0.8596443546618897, 1.5223825112460005, -2.218700210565016]\n",">epoch= 0 lrate= 0.3 error=2.217\n",">epoch= 1 lrate= 0.3 error=1.613\n",">epoch= 2 lrate= 0.3 error=1.113\n",">epoch= 3 lrate= 0.3 error=0.827\n",">epoch= 4 lrate= 0.3 error=0.623\n",">epoch= 5 lrate= 0.3 error=0.494\n",">epoch= 6 lrate= 0.3 error=0.412\n",">epoch= 7 lrate= 0.3 error=0.354\n",">epoch= 8 lrate= 0.3 error=0.310\n",">epoch= 9 lrate= 0.3 error=0.276\n","[-0.4066054639903037, 0.8525733163581036, -1.1047462590413233]\n",">epoch= 0 lrate= 0.3 error=2.217\n",">epoch= 1 lrate= 0.3 error=1.613\n",">epoch= 2 lrate= 0.3 error=1.113\n",">epoch= 3 lrate= 0.3 error=0.827\n",">epoch= 4 lrate= 0.3 error=0.623\n",">epoch= 5 lrate= 0.3 error=0.494\n",">epoch= 6 lrate= 0.3 error=0.412\n",">epoch= 7 lrate= 0.3 error=0.354\n",">epoch= 8 lrate= 0.3 error=0.310\n",">epoch= 9 lrate= 0.3 error=0.276\n",">epoch= 10 lrate= 0.3 error=0.248\n",">epoch= 11 lrate= 0.3 error=0.224\n",">epoch= 12 lrate= 0.3 error=0.205\n",">epoch= 13 lrate= 0.3 error=0.189\n",">epoch= 14 lrate= 0.3 error=0.174\n",">epoch= 15 lrate= 0.3 error=0.162\n",">epoch= 16 lrate= 0.3 error=0.151\n",">epoch= 17 lrate= 0.3 error=0.142\n",">epoch= 18 lrate= 0.3 error=0.134\n",">epoch= 19 lrate= 0.3 error=0.126\n",">epoch= 20 lrate= 0.3 error=0.119\n",">epoch= 21 lrate= 0.3 error=0.113\n",">epoch= 22 lrate= 0.3 error=0.108\n",">epoch= 23 lrate= 0.3 error=0.103\n",">epoch= 24 lrate= 0.3 error=0.098\n",">epoch= 25 lrate= 0.3 error=0.094\n",">epoch= 26 lrate= 0.3 error=0.090\n",">epoch= 27 lrate= 0.3 error=0.087\n",">epoch= 28 lrate= 0.3 error=0.084\n",">epoch= 29 lrate= 0.3 error=0.080\n",">epoch= 30 lrate= 0.3 error=0.078\n",">epoch= 31 lrate= 0.3 error=0.075\n",">epoch= 32 lrate= 0.3 error=0.073\n",">epoch= 33 lrate= 0.3 error=0.070\n",">epoch= 34 lrate= 0.3 error=0.068\n",">epoch= 35 lrate= 0.3 error=0.066\n",">epoch= 36 lrate= 0.3 error=0.064\n",">epoch= 37 lrate= 0.3 error=0.062\n",">epoch= 38 lrate= 0.3 error=0.060\n",">epoch= 39 lrate= 0.3 error=0.059\n",">epoch= 40 lrate= 0.3 error=0.057\n",">epoch= 41 lrate= 0.3 error=0.056\n",">epoch= 42 lrate= 0.3 error=0.054\n",">epoch= 43 lrate= 0.3 error=0.053\n",">epoch= 44 lrate= 0.3 error=0.052\n",">epoch= 45 lrate= 0.3 error=0.051\n",">epoch= 46 lrate= 0.3 error=0.050\n",">epoch= 47 lrate= 0.3 error=0.048\n",">epoch= 48 lrate= 0.3 error=0.047\n",">epoch= 49 lrate= 0.3 error=0.046\n",">epoch= 50 lrate= 0.3 error=0.045\n",">epoch= 51 lrate= 0.3 error=0.044\n",">epoch= 52 lrate= 0.3 error=0.044\n",">epoch= 53 lrate= 0.3 error=0.043\n",">epoch= 54 lrate= 0.3 error=0.042\n",">epoch= 55 lrate= 0.3 error=0.041\n",">epoch= 56 lrate= 0.3 error=0.040\n",">epoch= 57 lrate= 0.3 error=0.040\n",">epoch= 58 lrate= 0.3 error=0.039\n",">epoch= 59 lrate= 0.3 error=0.038\n",">epoch= 60 lrate= 0.3 error=0.038\n",">epoch= 61 lrate= 0.3 error=0.037\n",">epoch= 62 lrate= 0.3 error=0.036\n",">epoch= 63 lrate= 0.3 error=0.036\n",">epoch= 64 lrate= 0.3 error=0.035\n",">epoch= 65 lrate= 0.3 error=0.035\n",">epoch= 66 lrate= 0.3 error=0.034\n",">epoch= 67 lrate= 0.3 error=0.033\n",">epoch= 68 lrate= 0.3 error=0.033\n",">epoch= 69 lrate= 0.3 error=0.032\n",">epoch= 70 lrate= 0.3 error=0.032\n",">epoch= 71 lrate= 0.3 error=0.032\n",">epoch= 72 lrate= 0.3 error=0.031\n",">epoch= 73 lrate= 0.3 error=0.031\n",">epoch= 74 lrate= 0.3 error=0.030\n",">epoch= 75 lrate= 0.3 error=0.030\n",">epoch= 76 lrate= 0.3 error=0.029\n",">epoch= 77 lrate= 0.3 error=0.029\n",">epoch= 78 lrate= 0.3 error=0.029\n",">epoch= 79 lrate= 0.3 error=0.028\n",">epoch= 80 lrate= 0.3 error=0.028\n",">epoch= 81 lrate= 0.3 error=0.027\n",">epoch= 82 lrate= 0.3 error=0.027\n",">epoch= 83 lrate= 0.3 error=0.027\n",">epoch= 84 lrate= 0.3 error=0.026\n",">epoch= 85 lrate= 0.3 error=0.026\n",">epoch= 86 lrate= 0.3 error=0.026\n",">epoch= 87 lrate= 0.3 error=0.026\n",">epoch= 88 lrate= 0.3 error=0.025\n",">epoch= 89 lrate= 0.3 error=0.025\n",">epoch= 90 lrate= 0.3 error=0.025\n",">epoch= 91 lrate= 0.3 error=0.024\n",">epoch= 92 lrate= 0.3 error=0.024\n",">epoch= 93 lrate= 0.3 error=0.024\n",">epoch= 94 lrate= 0.3 error=0.024\n",">epoch= 95 lrate= 0.3 error=0.023\n",">epoch= 96 lrate= 0.3 error=0.023\n",">epoch= 97 lrate= 0.3 error=0.023\n",">epoch= 98 lrate= 0.3 error=0.023\n",">epoch= 99 lrate= 0.3 error=0.022\n","Coefficient is:  [-0.8596443546618897, 1.5223825112460005, -2.218700210565016]\n",">epoch= 0 lrate= 0.3 error=2.217\n",">epoch= 1 lrate= 0.3 error=1.613\n",">epoch= 2 lrate= 0.3 error=1.113\n",">epoch= 3 lrate= 0.3 error=0.827\n",">epoch= 4 lrate= 0.3 error=0.623\n",">epoch= 5 lrate= 0.3 error=0.494\n",">epoch= 6 lrate= 0.3 error=0.412\n",">epoch= 7 lrate= 0.3 error=0.354\n",">epoch= 8 lrate= 0.3 error=0.310\n",">epoch= 9 lrate= 0.3 error=0.276\n","Coefficient is:  [-0.4066054639903037, 0.8525733163581036, -1.1047462590413233]\n","The accuracy for iteration 100 =  100.0\n","The accuracy for iteration 10 =  100.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nS7Ykp-77HzJ"},"source":["After comparing the prediction accuracy of logistic regression and ZeroR, we have that Logistic Regression is better since zeroR only have 50% accuracy while logistic regression has 100% accuracy in this case.\n"]},{"cell_type":"markdown","metadata":{"id":"uwmUXdeACeho"},"source":["The error rate of GSD for iteration 100 is 0.022, and the error rate of GSD for iteration 10 is 0.276. However, the accuracy for this two iteration are the same which is 100%. The overall accuracy didn't change."]},{"cell_type":"markdown","metadata":{"id":"d6km1GgQJwQv"},"source":["### Part 4: Introduction to scikit-learn\n","\n","One of the most popular open-source python machine learning libraries is scikit-learn. You can find out more in general at: https://scikit-learn.org/stable/index.html\n","\n","This time, I'm only doing the bare minimum. I'll load the relevant models from scikit-learn, but it's up to you to train and test them, and report the scores appropriately. I also want you to generate a confusion matrix.\n","\n","Do the following:\n","- Train the SGD Classifier, using the fit method (note: I'm using the 'log' loss parameter, which means it will behave as logistic regression. There are other options).\n","  - Print out information about the model, including the coefficients, the intercept and the number of iterations it took to train\n","  - Compare these coefficients to the ones you learned. They WILL be different\n","  - Use the model to predict values for X_test\n","- Evaluate performance using the imported accuracy_metric\n","- Plot the confusion matrix using the imported plot_confusion_matrix\n","- Train and test the standard logistic regression model from sklearn \n","- Train and test the dummy classifier (zeroR)\n","- Print out the ACCURACY for all three\n","- Add a text box AFTER the results commenting on which model(s) are better, and comparing the coefficients your code learned, to those of the SGD Classifier you used here.\n","\n","Important links:\n","- [SGD Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier)\n","- [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n","- [Dummy Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html)\n","-[accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n","-[plot_confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"yvH3q0vxJwQw","colab":{"base_uri":"https://localhost:8080/","height":371},"executionInfo":{"status":"ok","timestamp":1622117734463,"user_tz":240,"elapsed":7,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"7d41b24d-bfc8-4c1b-898f-ad32fb9732ad"},"source":["from sklearn.dummy import DummyClassifier\n","from sklearn.linear_model import SGDClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import plot_confusion_matrix\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Use this dataset for all examples\n","\n","dataset = [[2.7810836,2.550537003,0],\n","    [1.465489372,2.362125076,0],\n","    [3.396561688,4.400293529,0],\n","    [1.38807019,1.850220317,0],\n","    [3.06407232,3.005305973,0],\n","    [7.627531214,2.759262235,1],\n","    [5.332441248,2.088626775,1],\n","    [6.922596716,1.77106367,1],\n","    [8.675418651,-0.242068655,1],\n","    [7.673756466,3.508563011,1]]\n","\n","# **************************************************************************\n","# CREATE YOUR X_train,y_train, X_Test,y_test datasets here\n","\n","#\n","X_train = []\n","y_train = []\n","for i in range(len(dataset)):\n","  X_train.append(dataset[i][:len(dataset[i])-1])\n","\n","for i in range(len(dataset)):\n","  y_train.append(dataset[i][-1])\n","\n","X_test = X_train\n","y_test = y_train\n","\n","# **************************************************************************\n","#sgd\n","sgd_clf = SGDClassifier(loss='log',max_iter=50)\n","sgd = sgd_clf.fit(X_train, y_train)\n","print('INTERCEPT: (B0)',sgd.intercept_)\n","print('COEFFS: (B1...BN)',sgd.coef_)         \n","print('ITERATIONS TAKEN:',sgd.n_iter_)\n","sgd_predY = sgd.predict(X_test)\n","sgd_score = accuracy_score(y_train, sgd_predY) \n","\n","plot_confusion_matrix(sgd, X_test, y_test) \n","plt.show()\n","\n","# Logistic Regression\n","lr_clf = LogisticRegression()\n","lr = lr_clf.fit(X_train,y_train)\n","lr_predY = lr.predict(X_test)\n","lr_score = accuracy_score(y_train, lr_predY)\n","\n","# zeroR\n","zr_clf = DummyClassifier(strategy=\"most_frequent\")\n","zeroR = zr_clf.fit(X_train, y_train)\n","zeroR_predY = zeroR.predict(X_test)\n","zeroR_score = accuracy_score(y_train, zeroR_predY)\n","\n","print(\"SGD accuracy Score = \", sgd_score, \" Logistic Regression accuracy Score = \", lr_score, \n","      \" zeroR accuracy score = \", zeroR_score)\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["INTERCEPT: (B0) [-14.91048761]\n","COEFFS: (B1...BN) [[ 35.34316324 -39.97356502]]\n","ITERATIONS TAKEN: 7\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["SGD accuracy Score =  1.0  Logistic Regression accuracy Score =  1.0  zeroR accuracy score =  0.5\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NH5FbHgsTCz3"},"source":["After comparing the score of each classifier, SGD and Logistic Regression are the relatively better because they all get score 1. But the zeroR got score 0.5, which is lower than the other two."]},{"cell_type":"markdown","metadata":{"id":"_pKmvs_wUXuF"},"source":["The coefficient my code learned and the coefficient of SGD classifier learned here are compeletly different. They are not closed at all. There is also a great difference in terms of the magnitude. They have different dimentions. The coefficient my code learned is one dimention while the SGD classifier is two dimention."]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Irene_CSC-321 Assignment 1.ipynb","provenance":[{"file_id":"1jQZGkiCnLaydYUGl6vrVaTuIlIb6_FJq","timestamp":1617542009789},{"file_id":"199CtlnMyNdmhQ3dM9dEwEY-yzn9CuZaF","timestamp":1617403367429},{"file_id":"1RzFWMrYiTLblWVBcieckpPAzhshbUK4s","timestamp":1617371229590}],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"5vF2KGxTkj7Q"},"source":["# CSC-321: Data Mining and Machine Learning\n","# Irene Yin\n","\n","\n","## Assignment 1: Oh, the data structures we shall know\n","\n","First, don't forget to make a copy (File -> Save copy in drive), otherwise you wont be able to edit. THEN don't forget to add your name, by double clicking the text cell.\n","\n","In this notebook, I'm going to take you on a whirlwind tour of some techniques in python, and some data structures, from lists and dictionaries to arrays and dataframes.\n","\n","Buckle up. You're going to have to do some reading.\n","\n","Given the nature of this class, there's NO WAY we have time to cover all the important data structures, so you'll have to do some self learning. That's ok, you're all computer scientists, so I can trust you to go about this the right way.\n","\n","I provide links to tutorials and documentation. It's good practice to read that stuff and try things out for yourself.\n","\n","Two other notes: In this class, for the most part, I'm NOT going to grade the quality of your code. This assignment is different - I mandate HOW to do some things. But in general I'm not here to grade your code, beyond \"does it get the job done.\" I want you to grasp the concepts. \n","\n","Second, I don't mind if you use resources like stack overflow. Learning to use those resources is also important. I never exactly remember the syntax for a dictionary comprehension with if statements, and so I look it up. It's ok to do the same.\n","\n","It is NOT ok to copy complete homework solutions from the iternet, OR to copy from each other. Each homework assignment, this one included, has a text box with the Union College Honor Code included. You will add your initials at the end of the statement for EVERY homework. \n","\n","If in any doubt - ASK ME."]},{"cell_type":"markdown","metadata":{"id":"DerpSG6Xkj7W"},"source":["### Part 1: Functions with multiple return values\n","\n","Read the tutorial at: https://www.tutorialspoint.com/How-do-we-return-multiple-values-in-Python\n","\n","(a) Write a function called minmax, that takes a list as a single argument, and returns two values as a tuple - the minimum value in the list, and the maximum value in a list. \n","\n","(b) Use the [random.randint(a,b)](https://www.w3schools.com/python/ref_random_randint.asp) function to create a list filled with 100 random integers in the range 0 to 1000 (inclusive).\n","\n","(c) Pass this list to your function from (a), capture the two return values into TWO distinct variables, and print them nicely! (i.e. Print two, distinct, meaningful output messages, not just the tuple returned)."]},{"cell_type":"code","metadata":{"id":"gB6m2EU9kj7X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617812798532,"user_tz":240,"elapsed":750,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"e8a82b37-6a3c-4a10-e106-329be1bc138b"},"source":["import random\n","#(a)\n","def minmax(InputList):\n","  # input: a single argument\n","  # output: 2 values as a tuple - the minimun value in the list, and the maximun value in a list.\n","  return (min(InputList), max(InputList))\n","\n","#(b)\n","InputList = []\n","for i in range(100):\n","  num = random.randint(0,1000)\n","  InputList.append(num)\n","\n","#(c)\n","minmaxTuple = minmax(InputList)\n","\n","print(\"The minimun number in this list is: \", minmaxTuple[0], '\\n'\n","      'The maximun number in this list is: ', minmaxTuple[1])\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The minimun number in this list is:  2 \n","The maximun number in this list is:  999\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s_hOz77CdCnW"},"source":["\n","\n","### Part 2: Lists and list comprehension\n","\n","Lists are a major part of all programming languages (except those where they're not. Obviously). List comprehensions are a particularly pythonic way of creating and manipulating lists. \n","\n","I want you to complete the tasks (a) through (g) below USING list comprehension. Yes, you can achieve them by NOT using comprehension - but where's the fun in that, and I'll penalize you for not using list comprehension.\n","\n","First, read the tutorial at: https://www.programiz.com/python-programming/list-comprehension\n","\n","\n","(a) You *could* have solved part1(b) above using a list comprehension. If you did, great. If you didn't, great. Either way do 1(b) (and 1(c)) AGAIN below, generating your list of random numbers using list comprehension. Print the min and the max values in the list.\n","\n","For (b) through (e) take the list called startList, below, and use list comprehension to complete the following. You must use list comprehension, and you must start with the original list (startList) each time. You must print the resulting list at the end each time.\n","\n","(b) Create a new list from every other element, starting at index 0. The range function can help you here.\n","\n","(c) Create a new list that contains True for each element in the original list that is even, False if it is odd. Note, those are BOOLEAN values, not strings\n","\n","(d) Create a new list that replaces the values in the original list with 1, 2 or 3 under the following conditions. If the value is less than 3, make the new value 1, if it's greater than or equal to 3. but below 6, make it equal to 2, otherwise make it 3\n","\n","(e) Go through the list and multiply each elements by 5, putting the result in a new list.\n","\n","The following two parts use the variable data, instead of startList. Both are east to do if you remember how SLICING / INDEXING works.\n","\n","(f) Go through the list of lists contained in the variable data defined below, and extract all elements from each list EXCEPT the last one, and put into a new list of lists. \n","\n","e.g. the output should be: [[1, 2], [4, 5], [7, 8], [10, 11]]\n","\n","(g) Go through the list of lists contained in the variable data, and create a single list containing ONLY the last element of each nested list. You must use list comprehension.\n","\n","e.g. the output should be: [3, 6, 9, 12]"]},{"cell_type":"code","metadata":{"id":"PEigG0WBkj7V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617860366323,"user_tz":240,"elapsed":941,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"68eb0572-d7c1-43d0-ff9f-888d6132ddd0"},"source":["import random\n","#a)\n","def minmax(InputList):\n","  # input: a single argument\n","  # output: 2 values as a tuple - the minimun value in the list, and the maximun value in a list.\n","  return (min(InputList), max(InputList))\n","\n","InputList = [random.randint(0,1000) for i in range(100)]\n","\n","minmaxTuple = minmax(InputList)\n","\n","print(\"The minimun number in this list is: \", minmaxTuple[0], '\\n'\n","      'The maximun number in this list is: ', minmaxTuple[1])\n","\n","# Use this list for b-e above\n","startList = [2, 7, 1, 9, 1, 4, 8, 10, 2, 3, 5]\n","# b) Create a new list from every other element, starting at index 0.\n","outputList1 = [startList[i] for i in range(len(startList)) if i%2 == 0]\n","print(\"A new list from every other element, starting at index 0 is: \", outputList1)\n","\n","# c) Create a new list that contains True for each element in the original list that is even, \n","#    False if it is odd. Note, those are BOOLEAN values, not strings\n","outputList2 = [True if num%2 == 0 else False for num in startList]\n","print(\"A new list that contains True if element in the list is even, false otherwise: \",\n","      outputList2)\n","\n","# d) Create a new list that replaces the values in the original list with 1, 2 or 3 under the following conditions. \n","#    If the value is less than 3, make the new value 1, if it's greater than or equal to 3. \n","#    but below 6, make it equal to 2, otherwise make it 3\n","outputList3 = [1 if num<3 else 2 if 3<=num<6 else 3 for num in startList]\n","print(\"A new list that matches the condition is: \", outputList3)\n","\n","# e) Go through the list and multiply each elements by 5, putting the result in a new list.\n","outputList4 = [5*num for num in startList]\n","print(\"A new list with every element that is multiplied by 5 from the original list is: \", \n","      outputList4)\n","\n","\n","\n","# Use this list for g-g above\n","data = [[1,2,3],[4,5,6],[7,8,9],[10,11,12]]\n","# f) Go through the list of lists contained in the variable data defined below, \n","# and extract all elements from each list EXCEPT the last one, and put into a new list of lists.\n","outputList5 = [data[i][:2] for i in range(len(data))]\n","print(\"A new list that matches the condition is: \", outputList5)\n","\n","# g) Go through the list of lists contained in the variable data, \n","# and create a single list containing ONLY the last element of each nested list. \n","# You must use list comprehension.\n","outputList6 = [data[i][len(data[i])- 1] for i in range(len(data))]\n","print(\"a new list containing only the last element of each nested list is: \", outputList6)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The minimun number in this list is:  1 \n","The maximun number in this list is:  998\n","A new list from every other element, starting at index 0 is:  [2, 1, 1, 8, 2, 5]\n","A new list that contains True if element in the list is even, false otherwise:  [True, False, False, False, False, True, True, True, True, False, False]\n","A new list that matches the condition is:  [1, 3, 1, 3, 1, 2, 3, 3, 1, 2, 2]\n","A new list with every element that is multiplied by 5 from the original list is:  [10, 35, 5, 45, 5, 20, 40, 50, 10, 15, 25]\n","A new list that matches the condition is:  [[1, 2], [4, 5], [7, 8], [10, 11]]\n","a new list containing only the last element of each nested list is:  [3, 6, 9, 12]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Fm8jW0UFu3Ad"},"source":["### Part 3: Dictionaries, and dictionary comprehension \n","\n","Just as with lists, you can use dictionary comprehension to create dictionaries. \n","\n","Read the tutorial at: https://www.programiz.com/python-programming/dictionary-comprehension\n","\n","\n","(a) Take the two lists below. The first is a list of words. The second is a list of counts of those words. So for example, the word animal occurs 12 times in some test. Use dictionary comprehension to create a dictionary of counts, where the key is the name, and the value is the count of that thing, Print the resulting dictionary.\n","\n","(b) Now use dictionary comprehension to create a dictionary of counts, but ONLY of words that start with a vowel (a, e, i, o or u). Print the resulting dictionary.\n"]},{"cell_type":"code","metadata":{"id":"-sNDnEndvV8b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617865077414,"user_tz":240,"elapsed":1022,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"51c5a77f-e199-4d03-f664-0d94b2c861ca"},"source":["words = ['animal','insect','dinosaur','fish','plant','shrub','tree','arachnid','iguana']\n","counts = [12,432,7,800,2000,1,14,7,6]\n","\n","# a) \n","dictionary = {words[i]:counts[i] for i in range(len(words))}\n","print(\"Resultant dictionary is: \", dictionary)\n","\n","# b)\n","dictionary2 = {words:counts for (words, counts) in dictionary.items() if words[0] == \"a\" \n","               or words[0] == \"e\" or words[0] == \"i\" or words[0] == \"o\" or words[0] == \"u\"}\n","print(\"Resultant dictionary is: \", dictionary2)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Resultant dictionary is:  {'animal': 12, 'insect': 432, 'dinosaur': 7, 'fish': 800, 'plant': 2000, 'shrub': 1, 'tree': 14, 'arachnid': 7, 'iguana': 6}\n","Resultant dictionary is:  {'animal': 12, 'insect': 432, 'arachnid': 7, 'iguana': 6}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vnR9-S4lkj7X"},"source":["### Part 4: Numpy\n","\n","Next up - n-dimensional arrays. Numpy is a library that brings n-dimensional arrays to python. Arrays are powerful, because it allows us to simplify operations over data, for instance you'll see that part2(e) can be done much more easily with arrays. However, you need to read the documentation first. Here is the key link:\n","\n","https://numpy.org/devdocs/user/quickstart.html\n","\n","You're REALLY interested in:\n","- creating arrays\n","- reshaping arrays\n","- built in attributes of arrays\n","- array operations\n","- indexing and slicing arrays\n","\n","Then work on the following:\n","\n","(a) convert the startList to a one dimensional array. Print this array, and it's shape.\n","\n","(b) MULTIPLY every value of the array from (a) by 5. The result should be another one dimensional array. \n","\n","(c) convert the startList into a two dimensional array, of 5 rows of 4 columns. Print this array, and print it's shape. Nicely.\n","\n","Remember in class I talked about Machine Learning as a process that uses some features (think of things like size and mass) to predict a class (like the type of coin)? \n","\n","Well, it turns out that the library we're going to use (scikit learn) EXPECTS this data in the form of arrays. It expects the data values for each instance to be a row in an n-dimensional array (so example (c) above could be 5 instances of data). \n","\n","It ALSO expects the data set to be SPLIT into TWO parts. One of them is all the input features, and the second is the class - the thing we're predicting. This will look like the solutions to 2(f) (the input values) and 2(g) (the values we're predicting).\n","\n","For now, we'll imagine that the last feature of each instance is the value we're trying to predict. Therefore:\n","\n","(d) use array slicing on (c) above to produce similar output to 2(f) and 2(g) above. One variable should contain 5 rows of 3 columns (all the data EXCEPT the last column from each row), and one variable that is a 5 by 1 array, containing the values from the last column. "]},{"cell_type":"code","metadata":{"id":"pDBgfBcWkj7X","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620291140141,"user_tz":240,"elapsed":1017,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"6ec50984-0e60-4d2b-850a-99a6928d944d"},"source":["import numpy as np\n","# a)\n","startList = [2,4,6,8,1,3,5,9,22,44,66,88,11,33,55,99,222,444,666,888]\n","oneDimArray = np.array(startList)\n","print('The One dimentional Array is: \\n', oneDimArray)\n","print('shape of one dimensional array: ', oneDimArray.shape)\n","\n","# b)\n","modifiedArray = oneDimArray*5\n","print(\"Modified array is: \\n\", modifiedArray)\n","\n","# c)\n","twoDimArray = np.array(startList).reshape(5,4)\n","print(\"The two dimentional array is: \\n\", twoDimArray)\n","print('shape of two dimensional array: ', twoDimArray.shape)\n","\n","# d)\n","#2(f)twoDimArray[]\n","outputValue1 = twoDimArray [:,:3]\n","print(\"All the data expect the last column from each row is: \\n\", outputValue1)\n","#2(g)\n","print(len(twoDimArray))\n","outputValue2 = twoDimArray[:,3]\n","print(outputValue2.shape)\n","print(\"Last column value is: \\n\", outputValue2)\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["The One dimentional Array is: \n"," [  2   4   6   8   1   3   5   9  22  44  66  88  11  33  55  99 222 444\n"," 666 888]\n","shape of one dimensional array:  (20,)\n","Modified array is: \n"," [  10   20   30   40    5   15   25   45  110  220  330  440   55  165\n","  275  495 1110 2220 3330 4440]\n","The two dimentional array is: \n"," [[  2   4   6   8]\n"," [  1   3   5   9]\n"," [ 22  44  66  88]\n"," [ 11  33  55  99]\n"," [222 444 666 888]]\n","shape of two dimensional array:  (5, 4)\n","All the data expect the last column from each row is: \n"," [[  2   4   6]\n"," [  1   3   5]\n"," [ 22  44  66]\n"," [ 11  33  55]\n"," [222 444 666]]\n","5\n","(5,)\n","Last column value is: \n"," [  8   9  88  99 888]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jxCqVrIBjypm"},"source":["### Part 5: Pandas\n","\n","Ok, that was a lot of build up to this: an introduction to Pandas. [Pandas](https://www.youtube.com/watch?v=ySRMDsJoIzM&ab_channel=TricksyPets) are cute bears, that everyone loves. Pandas is ALSO the most widely used library for data science in Python. I can't introduce you to ALL of pandas, but I can get you to think about what pandas does from a computer science perspective.\n","\n","If you've ever worked with the language R, then you already know about the dataframe. Pandas brings the R dataframe and all the associated methods to Python. \n","\n","You can think of the dataframe as like a spreadsheet - with rows and columns. Look back at Assignment 0, where I use Pandas at the bottom to load a data set. \n","\n","Because we're computer scientists, it also helps to think of dataframes in terms of other data structures.\n","\n","Each COLUMN in a pandas dataframe is a Series. And a Series is effectively a cross between a dictionary, and an n-dimensional array. Hey! That's why he had us do dictionaries and arrays.\n","\n","A dataframe is just a collection of series. \n","\n","There's an introduction to Pandas [you can read](https://www.learndatasci.com/tutorials/python-pandas-tutorial-complete-introduction-for-beginners/). There's more here than you need for this assignment, but I absolutely recommend getting familiar with pandas.\n","\n","Below I load the wine data I used in Assignment 0 using the csv at my github account using pandas, exactly as I did in Assignment 0. I set the attributes - the column headings - as I read in the data. For now, they aren't very meaningful. \n","\n","I then show the head - the first 5 rows of the data.\n","\n","Pandas is an incredibly powerful library, but for now, see if you can:\n","\n","(a) Print the number of rows and columns in the wine data (HINT: Shape can help you here)\n","\n","(b) Print JUST the values from the att3 column. (HINT: Datframes behave like dictionaries in many ways)\n","\n","(c) Print the mean values for each column (and only the mean values) (HINT: Dataframes have methods that work over them)\n","\n","(d) Tell me if there are any MISSING values in this data. Missing values are represented by the special value NaN (which comes from numpy, Not a Number). (HINT: Some methods can be combined, such as summing all the NaN values, to get a count on a column by column basis)\n","\n","(e) We can use pandas to prepare data for machine learning. Just as with 4(d) above, we want to create TWO variables. ONE will be all the features EXCEPT the class. The other will contain JUST the class values.\n","\n","To do this with pandas is easy. \n","\n","There is a method called drop, which allows you to drop a column. Let's say that att8 is the column we will try to predict.\n","\n","So, our input features will be everything except the column att8. The class values will be ONLY the values in the column called att8.\n","\n","Create two variables, called X for the input features, and y for the class values. Their structure (the shape of the data) should be (rows, 11) for the X data, and ( , rows) for the y data, where rows is the number of instances in the data. "]},{"cell_type":"code","metadata":{"id":"YEkvwvqumxo5","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1617867026614,"user_tz":240,"elapsed":1172,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"c82baeb4-3a76-4d08-b5cf-9559ea775ee8"},"source":["import pandas as pd\n","\n","# Assign column names\n","labels = ['att0','att1','att2','att3','att4','att5','att6','att7','att8','att9','att10','att11']\n","\n","# Load the data\n","wine_data = pd.read_csv(\"https://raw.githubusercontent.com/nixwebb/CSV_Data/master/winequality-white.csv\",names=labels)\n","\n","# Show the head of the data\n","wine_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>att0</th>\n","      <th>att1</th>\n","      <th>att2</th>\n","      <th>att3</th>\n","      <th>att4</th>\n","      <th>att5</th>\n","      <th>att6</th>\n","      <th>att7</th>\n","      <th>att8</th>\n","      <th>att9</th>\n","      <th>att10</th>\n","      <th>att11</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7.0</td>\n","      <td>0.27</td>\n","      <td>0.36</td>\n","      <td>20.7</td>\n","      <td>0.045</td>\n","      <td>45.0</td>\n","      <td>170.0</td>\n","      <td>1.0010</td>\n","      <td>3.00</td>\n","      <td>0.45</td>\n","      <td>8.8</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.3</td>\n","      <td>0.30</td>\n","      <td>0.34</td>\n","      <td>1.6</td>\n","      <td>0.049</td>\n","      <td>14.0</td>\n","      <td>132.0</td>\n","      <td>0.9940</td>\n","      <td>3.30</td>\n","      <td>0.49</td>\n","      <td>9.5</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8.1</td>\n","      <td>0.28</td>\n","      <td>0.40</td>\n","      <td>6.9</td>\n","      <td>0.050</td>\n","      <td>30.0</td>\n","      <td>97.0</td>\n","      <td>0.9951</td>\n","      <td>3.26</td>\n","      <td>0.44</td>\n","      <td>10.1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7.2</td>\n","      <td>0.23</td>\n","      <td>0.32</td>\n","      <td>8.5</td>\n","      <td>0.058</td>\n","      <td>47.0</td>\n","      <td>186.0</td>\n","      <td>0.9956</td>\n","      <td>3.19</td>\n","      <td>0.40</td>\n","      <td>9.9</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.2</td>\n","      <td>0.23</td>\n","      <td>0.32</td>\n","      <td>8.5</td>\n","      <td>0.058</td>\n","      <td>47.0</td>\n","      <td>186.0</td>\n","      <td>0.9956</td>\n","      <td>3.19</td>\n","      <td>0.40</td>\n","      <td>9.9</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   att0  att1  att2  att3   att4  att5   att6    att7  att8  att9  att10  att11\n","0   7.0  0.27  0.36  20.7  0.045  45.0  170.0  1.0010  3.00  0.45    8.8      6\n","1   6.3  0.30  0.34   1.6  0.049  14.0  132.0  0.9940  3.30  0.49    9.5      6\n","2   8.1  0.28  0.40   6.9  0.050  30.0   97.0  0.9951  3.26  0.44   10.1      6\n","3   7.2  0.23  0.32   8.5  0.058  47.0  186.0  0.9956  3.19  0.40    9.9      6\n","4   7.2  0.23  0.32   8.5  0.058  47.0  186.0  0.9956  3.19  0.40    9.9      6"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"code","metadata":{"id":"u8dqtXCCw3Qj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617883115661,"user_tz":240,"elapsed":769,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"d409fb00-7add-4452-88cd-c6a175abfa62"},"source":["# Write your solutions (a) through (e) here. \n","#[Really liked the attached panda video link !!!!]\n","#a)\n","print(wine_data.shape)\n","\n","#b)\n","att3 = wine_data[['att3']]\n","print(\"The values of the att3 column: \\n \", wine_data['att3'].tolist())\n","\n","#c)\n","print(\"The mean value of each column: \\n\", wine_data.mean(axis = 0))\n","\n","#d)\n","print(\"There is no missing values in this data: \\n\", wine_data.isnull().sum())\n","\n","#e)\n","X = wine_data.drop(columns = 'att11',axis = 1)\n","print(\"All the data expect the last column: \\n\", X)\n","print(\"The structure of X is: \\n\",X.shape)\n","y = wine_data['att11']\n","print(\"Only the value of last column: \\n\", y.tolist())\n","print(\"The structure of y is: \\n\",y.shape)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(4898, 12)\n","The values of the att3 column: \n","  [20.7, 1.6, 6.9, 8.5, 8.5, 6.9, 7.0, 20.7, 1.6, 1.5, 1.45, 4.2, 1.2, 1.5, 19.25, 1.5, 1.1, 1.2, 1.1, 7.5, 1.2, 2.9, 1.7, 1.5, 1.3, 9.0, 1.0, 8.7, 1.1, 2.0, 10.4, 1.1, 2.05, 1.5, 14.95, 1.7, 5.4, 1.2, 17.95, 17.95, 2.5, 2.9, 7.4, 1.4, 1.4, 1.4, 4.4, 4.4, 7.4, 5.0, 1.6, 1.1, 1.1, 1.0, 0.9, 1.3, 11.9, 12.4, 4.6, 1.2, 4.9, 12.4, 1.1, 4.6, 1.4, 1.2, 6.4, 1.2, 2.1, 10.1, 7.8, 4.6, 4.8, 1.0, 2.1, 10.1, 1.4, 1.3, 8.8, 5.4, 1.4, 16.0, 6.1, 8.2, 11.8, 11.8, 11.0, 13.3, 11.8, 11.8, 11.0, 7.5, 1.7, 1.8, 1.7, 12.4, 15.9, 1.2, 10.5, 15.9, 13.5, 9.6, 12.1, 18.9, 13.5, 14.4, 9.6, 16.5, 16.5, 1.3, 8.5, 18.75, 13.3, 9.2, 9.2, 1.5, 3.3, 1.7, 13.3, 13.3, 5.7, 12.2, 6.85, 5.0, 1.6, 1.1, 7.0, 7.6, 2.8, 2.7, 10.4, 7.6, 13.1, 15.8, 8.1, 15.7, 1.2, 10.5, 1.9, 1.9, 2.2, 10.5, 1.2, 4.3, 2.0, 1.1, 1.4, 5.2, 4.6, 4.1, 8.0, 10.2, 4.1, 1.2, 7.5, 14.0, 14.0, 1.5, 1.4, 1.4, 1.5, 14.5, 4.8, 14.0, 13.1, 13.0, 8.7, 2.4, 8.95, 9.85, 1.5, 1.5, 0.8, 4.55, 17.7, 7.5, 5.1, 12.9, 1.2, 5.5, 5.5, 1.2, 22.0, 14.0, 11.9, 11.9, 9.4, 4.2, 3.8, 8.5, 8.5, 22.0, 8.0, 5.1, 3.6, 3.5, 3.6, 7.8, 13.1, 7.8, 16.1, 13.1, 13.2, 13.9, 2.3, 11.6, 8.3, 6.2, 2.2, 0.9, 9.7, 1.4, 8.3, 5.3, 1.4, 9.2, 7.0, 9.2, 17.5, 7.0, 7.0, 9.5, 1.4, 0.8, 1.8, 17.8, 2.4, 10.3, 2.4, 17.8, 9.7, 7.8, 7.8, 14.3, 14.3, 14.3, 14.3, 13.2, 1.4, 15.55, 13.7, 7.8, 2.4, 2.7, 11.2, 2.0, 2.1, 1.4, 1.4, 10.4, 4.6, 16.2, 11.2, 3.5, 1.5, 1.1, 5.7, 5.7, 1.8, 0.9, 5.4, 19.45, 8.2, 13.6, 1.2, 19.45, 8.3, 6.9, 7.1, 7.0, 8.3, 7.0, 13.1, 5.1, 10.7, 8.9, 9.1, 1.1, 1.2, 4.7, 2.8, 5.0, 5.1, 15.6, 15.4, 7.2, 15.7, 14.2, 14.2, 14.2, 14.2, 5.8, 19.8, 7.3, 1.6, 1.2, 10.6, 3.8, 11.3, 2.1, 1.6, 0.9, 1.6, 1.3, 6.8, 6.8, 6.7, 6.7, 4.9, 4.8, 2.1, 8.3, 3.7, 6.7, 6.6, 1.1, 4.7, 7.7, 7.85, 3.3, 1.1, 7.7, 4.7, 9.3, 2.0, 5.8, 15.15, 6.3, 15.4, 1.3, 4.8, 4.6, 1.8, 9.9, 5.7, 1.8, 1.5, 12.3, 1.3, 1.5, 7.8, 1.5, 9.7, 3.0, 3.0, 4.6, 1.3, 15.6, 13.6, 11.2, 2.1, 6.85, 2.5, 6.85, 4.4, 14.3, 14.3, 1.1, 1.4, 4.7, 14.9, 1.2, 5.3, 14.3, 2.3, 3.5, 1.5, 1.0, 2.3, 12.5, 1.0, 2.3, 0.8, 2.4, 1.6, 1.6, 1.5, 1.4, 2.6, 1.5, 1.4, 12.5, 9.6, 2.4, 3.2, 1.6, 1.5, 5.9, 5.4, 14.9, 14.9, 1.1, 9.8, 2.0, 11.8, 1.3, 1.8, 1.5, 18.2, 2.9, 1.5, 11.8, 1.4, 18.2, 6.3, 1.3, 4.0, 1.2, 1.2, 1.8, 18.95, 12.7, 1.2, 1.2, 18.95, 12.7, 1.2, 7.7, 1.2, 1.6, 2.7, 8.7, 8.6, 8.6, 8.3, 8.7, 8.6, 13.3, 1.2, 13.6, 13.3, 1.1, 2.3, 1.5, 7.1, 6.2, 2.5, 1.4, 2.3, 1.2, 1.6, 6.6, 2.2, 12.2, 20.8, 11.0, 2.6, 14.0, 14.0, 1.6, 9.9, 1.2, 2.7, 1.6, 1.6, 14.0, 1.5, 8.6, 1.0, 9.1, 8.6, 15.2, 12.4, 2.2, 11.0, 10.0, 9.0, 2.2, 11.0, 18.15, 12.15, 3.4, 2.0, 4.8, 4.5, 11.0, 1.1, 1.5, 1.7, 1.7, 8.9, 1.3, 13.3, 8.9, 6.6, 1.5, 1.5, 1.3, 1.6, 7.6, 1.0, 1.6, 8.9, 1.8, 4.7, 7.0, 10.8, 1.6, 1.0, 10.8, 16.7, 14.2, 2.25, 2.25, 12.2, 12.2, 3.3, 1.9, 3.5, 1.9, 1.3, 1.8, 5.2, 1.4, 9.0, 2.2, 1.6, 1.7, 1.9, 12.6, 2.1, 2.1, 1.3, 1.7, 1.0, 3.3, 5.0, 1.8, 1.6, 4.5, 4.9, 4.0, 13.2, 13.2, 13.2, 15.3, 15.3, 13.2, 1.5, 4.55, 9.9, 10.4, 1.1, 8.7, 13.9, 1.4, 7.2, 1.9, 1.7, 10.4, 13.6, 1.2, 1.6, 13.6, 1.6, 1.2, 12.2, 1.6, 12.1, 4.2, 9.1, 1.6, 8.8, 1.4, 4.2, 10.3, 12.4, 2.2, 2.2, 5.2, 10.8, 2.7, 16.3, 2.7, 1.9, 1.8, 1.8, 2.8, 2.7, 6.0, 10.8, 4.2, 6.4, 10.7, 1.6, 10.7, 7.9, 1.7, 2.1, 5.8, 14.5, 1.2, 10.1, 1.1, 1.0, 0.9, 10.1, 4.9, 11.4, 1.4, 1.4, 1.4, 1.1, 1.0, 1.5, 1.5, 7.5, 7.5, 17.75, 5.7, 3.2, 4.3, 11.0, 10.9, 10.9, 11.0, 2.4, 1.9, 1.8, 1.4, 16.8, 8.9, 6.6, 1.1, 1.4, 13.3, 10.4, 6.2, 6.2, 1.2, 18.05, 18.05, 4.8, 12.6, 4.5, 4.5, 4.7, 11.9, 5.0, 5.8, 1.2, 12.9, 13.9, 11.5, 11.4, 7.8, 1.1, 12.2, 5.9, 14.4, 14.4, 14.4, 14.4, 12.3, 2.5, 5.3, 4.9, 3.2, 3.1, 1.4, 1.9, 14.0, 8.3, 1.9, 7.4, 1.4, 1.7, 7.4, 1.3, 11.6, 9.2, 1.7, 14.5, 2.1, 10.7, 6.9, 12.0, 4.75, 4.75, 4.75, 1.4, 1.3, 11.3, 4.5, 1.7, 13.2, 5.5, 13.0, 10.7, 2.0, 1.9, 6.4, 6.3, 6.3, 17.5, 14.7, 1.0, 14.8, 14.8, 14.2, 13.1, 2.4, 1.2, 1.1, 3.6, 6.1, 6.3, 4.4, 13.9, 1.0, 12.1, 1.6, 1.6, 7.5, 2.7, 1.7, 9.2, 18.75, 1.7, 1.8, 9.0, 1.5, 1.6, 1.3, 1.8, 1.2, 2.8, 8.0, 5.3, 0.8, 13.8, 10.0, 6.0, 14.4, 9.8, 5.5, 14.4, 9.8, 3.8, 5.5, 4.6, 1.5, 2.0, 12.5, 7.5, 2.1, 8.5, 7.6, 8.1, 7.35, 7.6, 10.0, 14.0, 8.5, 2.2, 1.8, 11.1, 15.2, 1.2, 1.2, 8.1, 8.1, 19.3, 18.6, 5.0, 1.6, 13.4, 1.6, 8.8, 8.8, 5.2, 1.1, 1.1, 16.7, 10.6, 2.2, 1.5, 13.7, 15.0, 14.1, 1.4, 9.5, 15.0, 14.1, 13.7, 10.6, 11.0, 11.0, 7.0, 10.6, 11.0, 2.0, 4.9, 2.0, 1.4, 7.3, 8.8, 12.5, 4.8, 1.4, 11.2, 6.8, 11.6, 11.4, 6.6, 11.2, 13.2, 13.1, 12.0, 13.4, 6.8, 1.5, 1.1, 16.5, 15.6, 2.8, 5.1, 8.9, 2.8, 8.9, 1.6, 10.2, 5.4, 6.2, 5.1, 7.2, 8.1, 2.0, 10.7, 1.2, 1.7, 5.5, 2.7, 1.3, 1.7, 4.3, 2.1, 2.1, 2.1, 6.0, 6.3, 13.4, 13.4, 1.7, 1.6, 1.6, 15.4, 1.2, 13.4, 1.8, 1.7, 13.6, 1.3, 11.1, 1.3, 2.1, 13.6, 13.7, 10.2, 4.8, 1.7, 14.2, 6.5, 1.7, 6.5, 2.2, 1.3, 12.6, 14.4, 1.2, 9.7, 11.1, 4.0, 1.4, 1.3, 2.2, 4.2, 1.6, 1.4, 2.2, 2.2, 1.9, 6.0, 6.0, 12.2, 2.6, 1.6, 2.0, 1.6, 2.0, 6.4, 1.2, 1.0, 1.4, 1.2, 1.1, 8.7, 1.6, 1.4, 8.5, 7.9, 13.2, 12.8, 7.7, 2.4, 1.6, 1.3, 2.1, 9.6, 13.0, 5.8, 5.6, 5.8, 0.8, 1.2, 0.8, 4.7, 1.6, 1.4, 1.1, 4.6, 10.6, 10.6, 12.9, 2.0, 2.9, 2.8, 2.9, 12.0, 15.0, 5.9, 12.6, 12.5, 7.4, 13.0, 6.7, 1.4, 15.0, 7.3, 15.3, 7.5, 11.75, 1.6, 3.5, 8.0, 5.0, 18.2, 1.6, 3.2, 6.4, 1.9, 9.7, 1.4, 1.3, 9.7, 1.9, 1.8, 0.9, 1.1, 15.4, 15.4, 5.0, 15.4, 4.0, 12.5, 3.2, 1.8, 17.5, 1.9, 1.6, 1.5, 1.7, 17.5, 1.9, 1.8, 4.3, 4.9, 7.6, 6.7, 1.6, 1.5, 6.7, 1.7, 4.2, 1.6, 1.9, 5.3, 5.2, 1.6, 8.9, 3.4, 1.3, 15.2, 1.3, 1.8, 1.6, 1.3, 9.8, 17.45, 2.1, 1.8, 6.8, 7.1, 1.3, 3.3, 3.3, 3.3, 2.7, 1.2, 1.3, 1.4, 1.7, 1.7, 11.6, 12.6, 2.8, 11.2, 1.4, 1.4, 1.5, 1.5, 1.5, 7.05, 10.6, 2.8, 1.6, 1.6, 8.2, 1.5, 11.0, 2.1, 7.6, 6.8, 6.8, 10.6, 1.8, 1.5, 6.0, 5.3, 1.4, 2.1, 1.0, 5.2, 19.45, 1.6, 2.0, 1.3, 1.7, 1.1, 4.7, 7.5, 1.1, 1.3, 2.2, 1.7, 2.2, 1.2, 10.0, 1.2, 1.1, 2.0, 7.5, 4.0, 4.1, 1.9, 14.8, 5.5, 2.6, 6.35, 1.5, 1.6, 14.8, 9.1, 8.0, 12.0, 3.2, 7.7, 1.3, 16.3, 6.2, 4.7, 5.8, 5.8, 16.95, 5.1, 10.2, 3.8, 1.3, 16.95, 4.0, 17.1, 14.5, 1.2, 6.7, 8.4, 1.3, 8.8, 5.1, 10.6, 2.6, 5.8, 2.8, 1.6, 6.7, 10.2, 4.9, 3.8, 2.0, 1.7, 2.4, 2.0, 1.3, 12.0, 1.6, 1.4, 2.2, 1.4, 8.3, 1.4, 12.9, 12.4, 1.6, 1.45, 1.6, 1.5, 13.0, 1.95, 5.85, 1.3, 5.3, 1.2, 4.0, 14.35, 2.3, 4.7, 12.85, 5.2, 8.4, 3.2, 6.9, 1.0, 7.3, 3.2, 1.6, 1.5, 7.5, 1.1, 1.3, 5.7, 7.2, 7.6, 7.5, 1.3, 1.3, 4.8, 2.1, 16.85, 16.85, 4.1, 16.85, 1.3, 16.85, 7.4, 4.1, 0.9, 6.3, 0.8, 1.4, 1.0, 1.3, 4.5, 5.4, 1.9, 2.0, 12.7, 8.6, 11.1, 2.0, 2.8, 4.5, 5.4, 9.1, 2.0, 2.2, 4.1, 11.8, 1.2, 2.2, 1.4, 1.6, 1.9, 5.2, 14.45, 14.75, 8.3, 14.2, 5.2, 14.75, 14.45, 6.9, 1.5, 1.5, 7.8, 1.5, 1.5, 1.5, 15.55, 7.8, 1.7, 1.5, 1.0, 6.7, 1.6, 1.5, 1.2, 7.1, 1.0, 12.1, 7.1, 1.0, 1.15, 6.0, 1.2, 7.1, 2.3, 3.4, 2.3, 6.5, 1.2, 1.15, 13.0, 4.5, 5.85, 5.2, 1.3, 1.7, 3.4, 3.6, 2.7, 2.2, 1.8, 2.1, 6.6, 2.7, 5.1, 17.55, 5.2, 1.1, 1.1, 10.7, 5.15, 1.1, 1.1, 1.8, 1.0, 1.8, 1.4, 6.7, 6.8, 6.8, 7.1, 1.5, 1.4, 1.45, 1.2, 7.4, 4.2, 14.6, 10.5, 13.8, 14.6, 1.1, 5.3, 1.1, 13.8, 1.5, 14.6, 10.5, 4.9, 1.9, 1.7, 1.8, 10.7, 8.8, 1.8, 1.7, 1.8, 1.6, 1.4, 1.6, 1.5, 9.2, 1.1, 4.3, 4.2, 9.2, 5.0, 6.8, 5.2, 4.75, 5.9, 7.7, 6.4, 8.1, 17.35, 1.8, 1.8, 1.5, 1.6, 1.6, 13.5, 3.75, 1.3, 1.3, 6.2, 9.7, 2.0, 1.4, 1.1, 6.6, 1.5, 4.6, 0.9, 1.7, 1.3, 2.3, 6.8, 1.5, 1.2, 2.1, 2.2, 8.8, 17.0, 1.8, 1.2, 2.3, 7.0, 6.4, 6.4, 12.15, 0.8, 2.0, 12.15, 12.4, 1.9, 1.1, 6.2, 6.3, 7.0, 0.9, 3.9, 3.9, 2.3, 6.4, 1.8, 1.1, 15.4, 6.5, 2.4, 1.4, 5.7, 5.7, 1.4, 1.4, 1.1, 1.2, 0.9, 3.8, 15.4, 14.15, 1.9, 9.2, 8.2, 8.2, 2.7, 1.7, 1.7, 1.7, 1.6, 0.9, 8.5, 5.7, 3.1, 1.3, 1.6, 1.3, 4.5, 2.8, 1.2, 0.9, 7.4, 1.4, 2.8, 4.6, 4.5, 1.6, 1.2, 13.1, 13.1, 5.7, 13.1, 12.6, 5.4, 2.2, 6.6, 6.9, 6.8, 3.6, 13.1, 1.2, 5.9, 5.7, 4.1, 7.1, 1.4, 0.8, 1.3, 15.55, 1.0, 1.6, 11.8, 4.5, 1.1, 10.0, 7.1, 0.9, 1.0, 1.1, 1.1, 1.5, 1.0, 1.5, 2.9, 1.2, 1.1, 14.2, 17.85, 1.1, 7.1, 7.6, 13.6, 1.4, 1.2, 5.3, 13.0, 4.45, 1.2, 1.2, 1.2, 6.6, 13.0, 1.1, 13.0, 14.0, 19.8, 11.2, 1.2, 4.45, 2.7, 4.5, 3.6, 5.4, 1.2, 1.2, 9.1, 0.8, 6.6, 3.0, 1.6, 1.1, 14.0, 13.0, 1.35, 5.9, 3.6, 1.3, 8.7, 1.2, 4.6, 2.0, 1.3, 1.7, 2.4, 3.6, 4.6, 1.5, 0.9, 19.4, 13.3, 17.85, 1.1, 1.3, 9.4, 9.0, 8.65, 9.6, 1.1, 1.2, 1.8, 2.1, 9.3, 1.7, 1.2, 4.6, 12.8, 1.5, 8.1, 7.5, 14.5, 1.6, 1.8, 3.2, 9.4, 9.9, 9.3, 1.3, 9.6, 9.3, 7.8, 1.7, 1.7, 1.2, 1.4, 9.0, 18.15, 12.5, 1.1, 7.7, 1.1, 18.15, 12.5, 8.1, 7.7, 5.0, 15.6, 1.5, 1.0, 1.0, 1.0, 5.0, 15.6, 0.8, 1.3, 1.1, 1.0, 5.2, 4.7, 4.8, 1.8, 1.5, 5.2, 1.2, 1.8, 7.6, 8.2, 1.3, 10.0, 5.2, 6.4, 1.0, 1.8, 2.0, 1.5, 1.8, 2.0, 3.5, 18.05, 1.8, 8.2, 5.0, 4.6, 8.2, 7.0, 3.1, 8.8, 2.4, 1.4, 1.8, 11.9, 13.4, 15.4, 7.2, 7.4, 11.9, 7.8, 13.5, 1.3, 7.8, 13.5, 1.5, 1.2, 6.4, 1.2, 7.3, 3.8, 7.3, 2.7, 7.3, 1.4, 1.6, 3.8, 8.0, 4.7, 1.9, 2.8, 2.2, 1.7, 4.9, 23.5, 11.8, 1.7, 5.2, 3.2, 1.5, 11.7, 1.6, 7.3, 1.6, 13.3, 6.0, 3.2, 1.6, 7.7, 5.9, 1.0, 6.4, 10.4, 1.6, 3.5, 7.7, 1.8, 3.5, 6.5, 6.7, 16.8, 1.6, 7.0, 1.2, 13.0, 3.8, 7.0, 7.5, 15.55, 12.3, 15.55, 9.4, 7.5, 1.2, 1.6, 1.6, 3.95, 12.8, 1.0, 31.6, 7.4, 1.3, 1.1, 7.8, 7.7, 7.7, 7.8, 8.3, 1.4, 31.6, 15.4, 1.0, 1.0, 5.9, 1.2, 1.2, 6.7, 6.5, 4.8, 5.3, 8.9, 8.8, 6.5, 1.8, 6.6, 5.7, 18.35, 10.0, 14.4, 14.4, 17.85, 14.4, 9.4, 17.85, 1.55, 1.3, 1.5, 14.4, 4.2, 8.6, 1.4, 3.0, 14.9, 2.0, 4.2, 2.2, 1.8, 11.1, 4.2, 6.4, 6.4, 1.5, 1.5, 16.95, 1.7, 13.6, 1.6, 6.1, 3.0, 6.2, 6.6, 1.6, 6.0, 1.5, 1.7, 1.8, 2.6, 2.7, 19.5, 1.6, 5.0, 4.6, 6.4, 14.5, 6.6, 6.7, 1.1, 13.8, 2.5, 1.1, 9.6, 4.6, 3.1, 9.6, 1.8, 1.05, 6.5, 2.0, 14.9, 10.0, 1.1, 12.0, 4.8, 10.0, 1.2, 1.5, 10.4, 2.5, 12.3, 1.4, 2.2, 14.9, 8.0, 8.6, 1.2, 8.7, 5.4, 5.7, 1.5, 10.9, 8.0, 8.6, 7.7, 1.7, 1.4, 4.5, 9.9, 1.4, 9.5, 9.9, 3.0, 19.6, 14.0, 1.4, 1.8, 1.8, 5.4, 1.8, 9.25, 1.5, 6.8, 9.25, 2.6, 2.1, 7.6, 1.6, 1.4, 1.6, 1.4, 17.95, 2.0, 6.3, 1.4, 9.05, 15.4, 7.1, 10.2, 1.6, 15.4, 7.1, 10.2, 12.4, 3.4, 15.0, 12.4, 15.0, 1.5, 3.4, 4.8, 1.2, 4.1, 2.8, 13.9, 0.8, 1.0, 1.2, 2.8, 1.7, 4.1, 6.3, 11.5, 1.5, 11.5, 6.3, 6.3, 11.2, 11.2, 15.1, 11.2, 15.1, 5.6, 12.6, 5.7, 2.2, 11.0, 5.7, 1.0, 2.2, 11.8, 7.8, 4.7, 11.5, 7.9, 11.5, 17.8, 1.7, 1.6, 4.7, 1.6, 2.4, 6.4, 14.0, 1.6, 1.6, 1.4, 14.0, 4.7, 10.2, 2.3, 7.8, 2.4, 8.3, 13.9, 13.8, 7.3, 8.5, 1.8, 5.1, 16.05, 16.05, 7.35, 16.05, 2.0, 1.2, 14.0, 16.2, 14.3, 16.2, 14.3, 14.3, 7.9, 16.2, 1.0, 13.6, 17.85, 1.3, 14.3, 13.7, 18.3, 18.3, 6.5, 10.3, 8.1, 8.1, 8.6, 1.9, 14.1, 7.9, 10.7, 1.2, 10.3, 8.1, 1.9, 2.7, 5.7, 9.3, 8.1, 1.3, 8.5, 2.1, 3.5, 5.7, 8.1, 1.3, 14.4, 13.7, 11.1, 9.3, 4.9, 1.7, 1.6, 1.4, 1.4, 1.6, 6.8, 9.7, 10.5, 2.0, 6.2, 8.3, 1.6, 9.7, 10.5, 4.6, 1.2, 7.6, 15.0, 7.6, 15.0, 11.6, 1.1, 7.8, 12.0, 1.1, 1.4, 6.7, 17.2, 1.4, 8.2, 4.3, 17.2, 11.1, 1.1, 7.2, 16.6, 1.0, 1.0, 1.3, 1.4, 16.6, 6.9, 13.3, 8.1, 9.2, 1.4, 3.2, 2.5, 3.2, 4.5, 18.35, 18.35, 7.8, 7.0, 6.3, 1.2, 12.8, 14.2, 14.2, 14.2, 14.2, 14.2, 16.65, 1.9, 7.7, 10.4, 3.3, 1.1, 14.2, 6.3, 5.25, 14.6, 11.8, 14.6, 11.8, 11.8, 17.3, 1.4, 1.3, 1.6, 5.25, 2.4, 14.6, 11.8, 1.5, 1.8, 7.7, 2.0, 1.8, 1.4, 16.7, 8.1, 8.0, 4.7, 8.1, 2.1, 16.7, 6.4, 1.5, 7.6, 1.5, 12.4, 1.3, 1.7, 8.1, 7.1, 7.6, 2.3, 6.5, 1.4, 12.7, 1.6, 1.1, 1.2, 6.5, 4.6, 0.6, 10.6, 4.6, 4.8, 2.7, 12.6, 0.6, 9.2, 6.6, 7.0, 8.45, 11.1, 18.15, 18.15, 4.1, 4.1, 4.6, 18.15, 4.9, 8.3, 1.4, 11.5, 1.8, 1.6, 2.4, 4.9, 1.8, 4.3, 4.4, 1.4, 1.6, 1.3, 5.2, 5.6, 5.3, 4.9, 2.4, 1.6, 2.1, 1.4, 7.1, 1.6, 10.7, 11.1, 10.7, 1.6, 1.6, 1.5, 1.5, 1.6, 1.6, 8.0, 7.7, 2.7, 15.1, 15.1, 8.9, 6.0, 12.3, 13.1, 6.7, 12.3, 2.3, 11.1, 1.5, 6.7, 6.0, 15.2, 10.2, 13.1, 10.7, 17.1, 17.1, 17.1, 1.9, 10.7, 17.1, 1.2, 1.2, 3.1, 1.5, 10.7, 4.9, 12.6, 10.7, 4.9, 12.15, 12.0, 1.7, 2.6, 1.4, 1.9, 16.9, 16.9, 2.1, 7.0, 7.1, 5.9, 7.1, 8.7, 13.2, 15.3, 15.3, 13.2, 2.7, 10.65, 10.0, 6.8, 15.6, 13.2, 5.1, 3.0, 15.3, 2.1, 1.9, 8.6, 8.75, 3.6, 4.7, 1.3, 1.8, 9.7, 4.0, 2.4, 4.7, 18.8, 1.8, 1.8, 12.8, 12.8, 12.8, 12.8, 12.8, 7.8, 16.75, 12.8, 12.8, 7.8, 5.4, 16.75, 1.3, 10.1, 3.8, 10.9, 6.6, 9.8, 11.7, 1.2, 1.4, 9.6, 12.2, 2.6, 10.7, 4.9, 12.2, 9.6, 1.4, 1.1, 1.0, 8.2, 11.3, 7.3, 2.3, 8.2, 2.1, 2.0, 10.0, 15.75, 3.9, 2.0, 1.5, 1.6, 1.4, 1.5, 1.4, 2.0, 13.8, 1.3, 3.8, 6.9, 2.2, 1.6, 13.8, 10.8, 12.8, 10.8, 15.3, 12.1, 12.0, 11.6, 9.2, 11.6, 9.2, 2.8, 1.6, 6.1, 8.5, 7.8, 14.9, 6.2, 8.5, 8.2, 7.8, 10.6, 11.2, 11.6, 7.1, 14.9, 6.2, 1.7, 7.7, 17.3, 1.4, 7.7, 7.7, 3.4, 1.6, 1.4, 1.4, 10.4, 1.4, 10.4, 4.1, 2.8, 15.7, 10.9, 15.7, 6.5, 10.9, 5.9, 17.3, 1.4, 13.5, 8.5, 6.2, 1.4, 14.95, 7.7, 1.3, 7.7, 1.3, 1.3, 1.3, 15.6, 15.6, 15.6, 15.6, 4.9, 5.0, 15.6, 6.5, 1.4, 2.7, 1.2, 6.5, 6.4, 6.9, 7.2, 10.6, 3.5, 6.4, 2.3, 12.05, 7.0, 11.8, 1.4, 5.0, 2.2, 14.6, 1.6, 1.3, 14.6, 2.8, 1.6, 3.3, 6.3, 8.1, 1.6, 10.6, 11.8, 1.7, 8.1, 1.4, 1.3, 1.8, 7.2, 1.1, 11.95, 1.1, 11.95, 2.2, 12.7, 1.4, 10.6, 1.9, 17.8, 10.2, 4.8, 9.8, 8.4, 7.2, 4.8, 8.4, 4.5, 1.4, 7.2, 11.0, 11.1, 2.6, 2.0, 10.1, 13.3, 11.4, 1.3, 1.4, 1.4, 7.0, 2.0, 1.2, 12.9, 5.0, 10.1, 3.75, 1.7, 12.6, 1.3, 1.6, 7.6, 8.1, 14.9, 6.0, 6.0, 7.2, 3.0, 1.2, 2.0, 4.9, 2.0, 8.9, 16.45, 2.0, 1.9, 5.1, 4.4, 5.8, 4.4, 12.9, 1.3, 1.3, 1.2, 2.7, 1.7, 8.2, 1.5, 1.5, 12.9, 3.9, 17.75, 4.9, 1.6, 1.4, 2.0, 2.0, 8.2, 2.1, 1.8, 8.5, 4.45, 5.8, 13.0, 2.7, 7.3, 19.1, 8.8, 2.7, 7.4, 2.3, 6.85, 11.4, 0.9, 19.35, 7.9, 11.75, 7.7, 3.0, 7.7, 3.0, 1.5, 7.5, 1.5, 7.5, 8.3, 7.05, 8.4, 13.9, 17.5, 5.6, 9.4, 4.8, 9.4, 9.7, 6.3, 1.6, 14.6, 2.5, 14.6, 2.6, 2.5, 8.2, 1.5, 2.3, 10.0, 10.0, 1.6, 1.6, 16.0, 10.4, 7.4, 7.4, 10.4, 16.05, 16.05, 2.6, 2.5, 10.8, 1.2, 12.1, 11.95, 1.7, 0.8, 1.4, 1.3, 6.3, 10.3, 15.55, 1.5, 1.5, 1.4, 1.5, 7.9, 13.0, 1.0, 4.85, 7.1, 7.9, 7.5, 7.6, 10.3, 1.7, 1.7, 19.95, 7.7, 5.3, 19.95, 12.7, 12.7, 1.5, 11.3, 18.1, 18.1, 7.0, 18.1, 6.4, 1.4, 1.4, 3.1, 14.1, 7.7, 5.2, 11.6, 10.4, 7.5, 11.2, 0.8, 1.4, 4.7, 3.1, 4.0, 11.3, 3.1, 8.1, 14.8, 1.4, 8.1, 3.5, 14.8, 8.1, 1.4, 1.5, 1.5, 12.8, 1.6, 7.1, 7.1, 11.2, 1.7, 6.7, 17.3, 8.6, 8.6, 1.5, 12.1, 6.7, 10.7, 17.3, 1.8, 1.4, 7.5, 4.8, 7.1, 16.9, 4.8, 7.1, 11.3, 1.1, 1.2, 1.1, 12.9, 1.2, 1.1, 1.2, 2.3, 10.0, 2.3, 1.2, 1.4, 14.9, 1.8, 1.8, 7.0, 8.6, 1.8, 1.1, 1.3, 4.9, 1.9, 10.4, 10.0, 8.6, 1.7, 1.7, 18.95, 12.8, 12.8, 12.8, 12.8, 12.8, 12.8, 0.7, 12.8, 1.4, 13.3, 8.5, 1.5, 11.7, 5.0, 1.2, 2.1, 1.4, 2.1, 16.0, 1.1, 15.3, 1.4, 2.8, 2.8, 0.9, 2.5, 8.1, 8.2, 0.9, 11.1, 7.8, 2.8, 10.1, 3.2, 14.2, 14.2, 14.2, 2.9, 6.0, 20.4, 10.1, 2.9, 14.2, 3.2, 0.95, 1.7, 1.7, 9.0, 1.3, 1.4, 2.4, 16.0, 11.4, 14.35, 2.1, 11.4, 14.35, 1.1, 1.1, 1.2, 15.8, 5.2, 5.2, 9.6, 5.2, 1.2, 0.8, 14.45, 9.6, 6.9, 3.4, 2.3, 11.0, 5.95, 5.1, 5.4, 1.2, 12.6, 1.0, 6.6, 1.5, 1.0, 1.1, 6.6, 8.2, 2.0, 1.4, 2.0, 7.5, 2.0, 2.0, 13.3, 2.85, 5.6, 5.6, 1.0, 3.2, 1.0, 7.1, 2.4, 11.2, 9.5, 1.0, 1.8, 2.6, 2.4, 8.0, 11.2, 7.1, 3.3, 10.3, 1.2, 1.6, 10.3, 9.65, 16.4, 1.5, 1.2, 3.3, 5.0, 16.3, 16.3, 16.3, 6.5, 6.4, 10.2, 16.3, 7.4, 13.7, 13.7, 1.3, 7.4, 7.4, 7.45, 7.2, 13.7, 10.4, 1.1, 6.5, 4.6, 13.9, 5.2, 1.7, 6.5, 16.4, 3.6, 1.5, 12.4, 1.7, 6.2, 6.2, 2.6, 1.7, 9.3, 12.4, 1.5, 9.1, 12.0, 4.8, 12.3, 12.0, 2.7, 3.6, 3.6, 4.3, 1.8, 11.8, 1.8, 11.8, 1.8, 1.4, 6.6, 1.55, 0.7, 6.4, 11.8, 4.3, 5.1, 5.8, 5.9, 1.3, 1.4, 1.2, 7.4, 10.8, 1.8, 7.4, 1.2, 1.4, 14.4, 1.7, 3.6, 3.6, 10.05, 10.05, 10.5, 1.9, 3.6, 1.65, 1.9, 65.8, 6.85, 7.4, 7.4, 20.2, 11.0, 20.2, 6.2, 6.2, 6.85, 8.0, 8.2, 2.2, 10.1, 7.2, 2.2, 10.1, 1.6, 1.3, 8.0, 8.2, 5.3, 14.0, 7.2, 1.6, 11.8, 9.6, 6.1, 2.7, 3.6, 1.7, 1.6, 2.7, 1.0, 0.9, 1.6, 1.0, 10.6, 2.0, 1.2, 6.2, 9.2, 5.0, 6.3, 3.3, 8.0, 1.2, 1.2, 16.2, 11.6, 7.2, 1.1, 3.4, 1.4, 3.3, 8.0, 9.3, 2.3, 0.9, 3.5, 1.7, 1.3, 1.3, 5.6, 7.4, 2.3, 1.0, 1.5, 10.0, 14.9, 9.3, 1.0, 1.0, 5.9, 5.0, 1.25, 3.9, 5.0, 0.8, 1.0, 5.9, 1.6, 1.3, 1.0, 1.1, 1.25, 1.4, 1.2, 5.0, 1.4, 1.7, 1.8, 1.6, 1.5, 1.7, 13.9, 5.9, 2.1, 1.1, 6.7, 2.7, 6.7, 3.95, 7.75, 10.6, 1.6, 2.5, 0.7, 11.1, 5.15, 4.7, 9.7, 1.7, 1.4, 2.0, 7.5, 9.7, 0.8, 13.1, 1.1, 2.2, 8.9, 1.1, 0.9, 1.7, 6.9, 1.1, 1.0, 1.0, 7.6, 8.9, 2.2, 1.2, 1.0, 1.0, 3.1, 1.95, 2.2, 8.75, 11.9, 2.7, 5.45, 6.3, 14.4, 7.8, 1.6, 9.1, 9.1, 14.4, 1.3, 1.6, 11.3, 6.3, 0.7, 1.25, 0.7, 7.8, 10.3, 10.3, 7.8, 8.7, 8.3, 10.3, 7.8, 1.2, 8.3, 8.3, 6.2, 5.0, 1.8, 1.6, 1.8, 1.8, 2.9, 6.0, 0.9, 1.1, 1.6, 5.45, 14.05, 8.0, 13.1, 4.9, 1.3, 2.2, 14.9, 14.9, 0.95, 1.4, 0.95, 1.7, 5.6, 14.9, 7.1, 1.2, 9.6, 11.4, 11.4, 7.9, 5.0, 11.1, 8.0, 3.8, 10.55, 10.2, 10.2, 9.8, 6.3, 1.1, 4.5, 6.3, 10.9, 9.8, 9.8, 0.8, 0.8, 1.2, 1.3, 9.8, 10.2, 10.9, 6.3, 6.3, 1.2, 0.9, 1.1, 4.5, 3.7, 18.1, 1.35, 5.5, 3.1, 12.85, 19.8, 8.25, 12.85, 3.8, 6.9, 8.25, 11.7, 4.6, 4.0, 19.8, 12.85, 1.2, 8.9, 11.7, 6.2, 14.8, 14.8, 10.8, 1.6, 8.3, 8.4, 2.5, 3.5, 17.2, 2.1, 12.2, 11.8, 16.8, 17.2, 1.1, 14.7, 5.5, 6.1, 1.2, 1.3, 8.7, 1.7, 8.7, 10.2, 4.5, 5.9, 1.7, 1.4, 5.4, 7.9, 1.1, 7.0, 7.0, 7.6, 7.0, 12.3, 15.3, 12.3, 1.2, 2.3, 6.1, 7.6, 10.2, 4.1, 2.9, 8.5, 1.5, 3.1, 7.9, 3.5, 4.9, 1.1, 7.0, 1.2, 4.5, 2.6, 9.9, 4.5, 9.5, 1.5, 3.2, 2.6, 11.2, 3.2, 2.3, 4.9, 4.9, 1.4, 1.5, 6.7, 2.1, 4.3, 10.9, 7.0, 2.3, 2.5, 2.6, 3.2, 2.5, 14.7, 4.5, 2.2, 1.9, 1.6, 17.3, 4.2, 4.2, 2.5, 1.9, 1.4, 0.8, 8.0, 1.6, 1.7, 5.5, 17.3, 8.6, 6.9, 2.1, 2.2, 1.5, 2.5, 17.6, 4.2, 2.9, 4.8, 11.9, 0.9, 1.3, 6.4, 4.3, 11.9, 8.1, 1.3, 0.9, 17.2, 17.2, 17.2, 8.7, 17.2, 8.7, 7.5, 17.2, 4.6, 3.7, 2.2, 7.4, 15.1, 7.4, 4.8, 7.9, 1.0, 15.1, 7.4, 4.8, 4.6, 1.4, 6.2, 6.1, 5.1, 6.3, 0.9, 2.3, 6.6, 7.5, 8.6, 11.9, 2.3, 7.1, 4.3, 1.1, 1.0, 7.9, 1.0, 1.0, 1.0, 7.3, 1.7, 1.3, 6.4, 1.8, 1.5, 3.8, 7.9, 1.0, 1.2, 5.3, 9.1, 6.5, 9.1, 6.3, 5.1, 6.5, 2.4, 9.1, 7.5, 5.0, 6.75, 1.2, 1.6, 16.05, 5.0, 12.4, 0.95, 4.6, 1.7, 1.0, 1.3, 5.0, 2.5, 2.6, 2.1, 12.75, 1.1, 12.4, 3.7, 2.65, 2.5, 8.2, 7.3, 1.1, 6.6, 7.0, 14.5, 11.8, 3.0, 3.7, 6.0, 4.6, 2.5, 3.3, 1.0, 1.1, 1.4, 3.3, 8.55, 2.5, 6.7, 3.8, 4.5, 4.6, 4.2, 11.3, 5.5, 4.2, 2.2, 14.5, 14.5, 14.5, 14.5, 14.5, 14.5, 1.5, 18.75, 3.6, 1.4, 5.1, 10.5, 2.0, 2.6, 9.2, 1.8, 5.7, 2.4, 1.9, 1.4, 0.9, 4.6, 1.4, 9.2, 1.4, 1.8, 2.3, 2.3, 4.4, 6.4, 2.9, 2.8, 2.9, 4.4, 8.2, 1.0, 2.9, 7.0, 1.8, 1.5, 7.0, 8.2, 7.6, 2.3, 8.7, 1.0, 2.9, 6.7, 5.0, 1.9, 2.0, 1.9, 8.5, 12.6, 5.2, 2.1, 1.1, 1.3, 1.1, 9.2, 1.2, 1.1, 8.3, 1.8, 1.4, 15.7, 4.35, 1.8, 1.6, 2.0, 5.0, 1.8, 1.3, 1.0, 1.4, 8.1, 8.6, 3.7, 5.7, 2.35, 13.65, 13.65, 13.65, 15.2, 4.6, 1.2, 4.6, 6.65, 13.55, 13.65, 9.8, 10.3, 6.7, 15.2, 9.9, 7.2, 1.1, 8.3, 11.25, 12.8, 9.65, 12.6, 12.2, 8.3, 11.25, 1.3, 9.9, 7.2, 1.1, 1.1, 4.8, 1.1, 1.4, 1.7, 10.6, 1.4, 1.1, 5.55, 2.1, 1.7, 9.0, 1.7, 1.8, 4.7, 11.3, 3.6, 6.9, 3.6, 4.9, 6.95, 1.9, 4.7, 11.3, 1.8, 11.3, 8.2, 8.3, 9.55, 8.4, 7.8, 7.8, 10.2, 5.5, 7.8, 7.4, 3.3, 5.0, 3.3, 5.0, 1.3, 1.2, 7.4, 7.8, 9.9, 0.7, 4.6, 5.6, 9.5, 14.8, 4.6, 2.1, 11.6, 1.2, 11.6, 2.1, 20.15, 4.7, 4.3, 14.5, 4.9, 14.55, 14.55, 10.05, 4.9, 14.5, 14.55, 15.25, 3.15, 1.3, 5.2, 1.1, 7.1, 8.8, 18.5, 8.8, 1.4, 1.2, 5.0, 1.6, 18.75, 6.0, 9.4, 9.7, 4.75, 6.0, 5.35, 5.35, 6.8, 6.9, 1.4, 0.9, 1.2, 1.3, 2.6, 12.0, 9.85, 3.85, 2.0, 1.6, 7.8, 1.9, 2.0, 10.3, 1.1, 12.0, 3.85, 9.85, 2.0, 4.0, 1.1, 10.4, 6.1, 1.8, 10.4, 4.7, 4.0, 1.1, 6.4, 8.15, 6.1, 4.8, 1.2, 1.1, 1.4, 7.4, 1.8, 1.0, 15.5, 15.5, 8.4, 2.4, 3.95, 19.95, 2.0, 3.0, 15.5, 8.4, 14.3, 4.2, 1.4, 3.0, 4.9, 2.4, 14.3, 10.7, 11.0, 1.4, 1.2, 12.9, 10.8, 1.3, 2.0, 1.8, 1.2, 7.5, 9.7, 3.8, 7.2, 9.7, 6.3, 6.3, 0.8, 8.6, 6.3, 3.1, 7.2, 7.1, 6.4, 14.7, 7.2, 7.1, 1.9, 1.2, 4.8, 1.2, 3.4, 4.3, 8.5, 1.8, 1.8, 19.5, 8.5, 19.9, 8.3, 1.8, 1.1, 16.65, 16.65, 16.65, 0.9, 6.1, 10.2, 0.9, 16.65, 3.85, 4.4, 4.5, 3.2, 4.5, 4.4, 9.7, 4.2, 4.2, 1.1, 9.7, 4.2, 5.6, 4.2, 1.6, 1.6, 1.1, 14.6, 2.6, 1.2, 7.25, 6.55, 7.0, 1.5, 1.4, 7.25, 1.0, 4.2, 17.5, 17.5, 17.5, 1.5, 1.3, 3.9, 4.2, 7.6, 1.0, 1.1, 11.8, 1.4, 9.7, 12.9, 1.6, 7.2, 7.1, 1.9, 8.8, 7.2, 1.4, 14.3, 14.3, 8.8, 1.4, 1.8, 14.3, 7.2, 1.2, 11.8, 0.9, 12.6, 26.05, 4.7, 12.6, 1.2, 26.05, 6.1, 11.8, 0.9, 5.6, 5.3, 5.7, 8.0, 8.0, 17.6, 8.0, 8.8, 1.5, 1.4, 4.8, 2.4, 3.7, 4.9, 5.7, 5.7, 4.9, 2.0, 5.1, 4.5, 3.2, 6.65, 1.6, 4.0, 17.75, 1.4, 17.75, 7.2, 5.7, 8.5, 11.4, 5.4, 2.7, 4.3, 1.2, 1.8, 1.3, 5.7, 2.7, 11.7, 4.3, 11.0, 1.6, 11.6, 6.2, 1.8, 1.2, 1.0, 2.4, 1.2, 8.2, 18.8, 9.6, 12.9, 9.2, 1.2, 12.9, 8.0, 12.9, 1.6, 12.0, 2.5, 9.2, 4.4, 8.8, 9.6, 8.0, 18.8, 1.3, 1.2, 12.9, 1.2, 1.6, 1.5, 18.15, 13.1, 13.1, 13.1, 13.1, 1.0, 1.6, 11.8, 1.4, 1.0, 13.1, 10.6, 10.4, 1.1, 7.4, 1.2, 3.4, 18.15, 8.0, 2.5, 2.0, 2.0, 6.9, 1.2, 9.4, 2.9, 6.9, 5.4, 1.3, 20.8, 10.3, 1.3, 1.6, 13.1, 1.8, 8.0, 1.6, 1.4, 14.7, 14.7, 14.7, 14.7, 14.7, 14.7, 14.7, 1.8, 10.6, 12.5, 6.8, 14.7, 2.9, 1.4, 1.4, 2.1, 7.4, 2.9, 1.4, 1.4, 7.4, 5.0, 2.5, 6.1, 2.7, 2.1, 12.9, 12.9, 12.9, 13.7, 12.9, 2.4, 9.8, 13.7, 1.3, 12.1, 6.1, 7.7, 6.1, 1.4, 7.7, 12.1, 6.8, 9.2, 8.3, 17.4, 2.7, 12.8, 8.2, 8.1, 8.2, 8.3, 8.0, 11.8, 12.0, 1.7, 17.4, 13.9, 10.7, 2.0, 2.2, 1.3, 1.1, 2.0, 6.4, 1.3, 1.1, 10.7, 6.4, 6.3, 6.4, 15.1, 2.0, 2.0, 2.2, 12.1, 8.8, 8.8, 5.1, 6.8, 6.8, 3.7, 12.2, 5.7, 8.1, 2.5, 4.0, 6.8, 1.0, 5.1, 5.8, 10.6, 3.5, 3.5, 16.4, 4.8, 3.3, 1.2, 1.2, 4.8, 3.3, 2.5, 8.7, 1.6, 4.0, 2.5, 16.2, 9.0, 16.2, 1.4, 7.0, 9.0, 3.1, 1.5, 4.6, 4.8, 4.6, 1.5, 2.7, 6.3, 7.2, 7.2, 12.4, 6.6, 6.6, 4.0, 4.8, 1.3, 7.2, 11.1, 12.4, 9.8, 6.6, 13.3, 11.7, 8.0, 1.6, 16.55, 1.5, 10.2, 6.6, 17.8, 17.8, 1.5, 7.4, 17.8, 2.0, 7.4, 2.0, 17.8, 12.1, 8.2, 1.5, 8.7, 3.5, 6.4, 2.1, 7.7, 12.3, 1.3, 8.7, 3.5, 1.1, 2.8, 3.5, 1.9, 3.8, 3.8, 2.4, 4.8, 4.8, 6.2, 1.3, 3.8, 1.5, 4.8, 1.9, 6.2, 7.9, 1.6, 1.4, 2.6, 14.8, 2.4, 0.9, 0.9, 1.2, 9.9, 3.9, 15.6, 15.6, 1.5, 1.6, 7.8, 5.6, 1.3, 16.7, 7.95, 6.7, 1.1, 6.3, 8.9, 1.0, 1.5, 6.6, 6.2, 6.3, 2.1, 2.2, 5.4, 8.9, 1.0, 17.9, 2.6, 1.3, 17.9, 2.6, 2.3, 4.3, 7.1, 7.1, 11.9, 11.7, 5.8, 3.8, 12.4, 6.5, 7.1, 7.6, 7.9, 2.8, 10.6, 2.8, 1.5, 7.6, 7.9, 1.7, 7.6, 7.5, 1.7, 1.7, 12.1, 4.5, 1.7, 8.0, 7.6, 8.6, 8.6, 14.6, 1.6, 8.6, 14.6, 1.1, 3.7, 8.9, 8.9, 4.7, 8.9, 3.1, 5.8, 5.8, 5.8, 1.0, 15.8, 1.5, 5.2, 1.5, 2.5, 1.0, 15.8, 5.9, 3.1, 3.1, 5.8, 11.5, 18.0, 4.8, 8.5, 1.6, 18.0, 4.8, 5.9, 1.1, 8.5, 13.1, 4.1, 2.9, 13.1, 1.1, 1.5, 7.75, 1.15, 1.0, 17.8, 5.7, 17.8, 7.4, 1.4, 1.4, 1.0, 4.4, 1.6, 7.9, 15.5, 15.5, 15.5, 15.5, 17.55, 13.5, 13.5, 1.3, 15.5, 11.6, 7.9, 15.5, 17.55, 11.6, 13.15, 1.9, 13.5, 1.3, 6.1, 6.1, 1.9, 1.9, 1.6, 11.3, 8.4, 8.3, 8.4, 12.2, 8.0, 1.3, 12.7, 1.3, 10.5, 12.5, 9.6, 1.5, 1.5, 7.8, 10.8, 12.5, 8.6, 1.2, 14.5, 3.7, 1.1, 1.1, 3.8, 4.6, 10.2, 7.9, 2.4, 10.7, 4.9, 10.7, 1.1, 7.9, 5.6, 2.4, 14.2, 9.5, 9.5, 4.1, 4.7, 1.4, 0.9, 20.3, 3.5, 2.7, 1.2, 1.2, 2.0, 1.1, 1.5, 1.2, 18.1, 18.1, 3.6, 3.5, 12.1, 17.45, 12.1, 3.0, 1.6, 5.7, 5.6, 6.8, 15.6, 6.0, 1.8, 8.6, 8.6, 11.5, 7.8, 2.4, 5.0, 8.6, 1.5, 5.4, 11.9, 11.9, 9.0, 10.0, 11.9, 11.9, 15.5, 5.4, 15.0, 1.4, 9.4, 3.7, 15.0, 1.4, 6.5, 1.4, 6.3, 13.7, 13.7, 13.7, 13.7, 13.7, 13.7, 1.5, 1.6, 1.4, 3.5, 1.0, 1.4, 1.5, 13.7, 1.6, 5.2, 1.4, 11.9, 2.4, 3.2, 1.7, 4.2, 15.4, 13.0, 5.6, 9.7, 2.5, 4.0, 15.4, 1.2, 2.0, 1.2, 5.1, 1.4, 1.2, 6.5, 1.3, 6.5, 2.7, 1.3, 7.4, 12.9, 1.3, 1.2, 2.6, 2.3, 1.3, 10.5, 2.6, 14.4, 1.2, 3.1, 1.7, 6.0, 11.8, 6.2, 1.4, 12.1, 12.1, 12.1, 3.9, 4.6, 12.1, 1.2, 8.1, 3.9, 1.1, 6.5, 10.1, 10.7, 3.2, 12.4, 5.2, 5.0, 2.5, 9.2, 6.9, 2.0, 15.0, 15.0, 1.2, 15.0, 1.8, 10.8, 3.9, 4.2, 2.0, 13.5, 13.3, 2.2, 1.4, 1.6, 2.2, 14.8, 1.8, 14.8, 1.3, 9.9, 5.1, 5.1, 1.5, 1.5, 11.1, 5.25, 2.3, 7.9, 8.0, 1.4, 5.25, 2.3, 2.3, 3.5, 13.7, 9.9, 15.4, 16.0, 16.0, 16.0, 16.0, 2.4, 5.5, 2.3, 16.8, 16.0, 17.8, 17.8, 6.8, 6.8, 6.8, 6.8, 1.6, 4.7, 11.8, 17.8, 15.7, 5.8, 15.7, 9.0, 15.7, 5.8, 8.8, 10.2, 6.6, 6.5, 8.9, 11.1, 4.2, 1.6, 7.4, 11.5, 1.6, 2.0, 4.8, 9.8, 1.9, 4.2, 1.6, 7.3, 5.4, 10.4, 1.9, 7.3, 5.4, 7.7, 11.5, 1.2, 2.2, 1.0, 8.2, 8.3, 8.2, 9.3, 8.1, 8.2, 8.3, 13.9, 13.9, 13.9, 13.9, 13.9, 13.9, 13.9, 2.0, 13.9, 15.7, 1.2, 1.5, 1.2, 3.2, 1.2, 2.6, 13.2, 10.4, 5.7, 2.5, 1.6, 1.4, 7.4, 2.5, 5.6, 3.6, 7.5, 5.8, 1.6, 1.5, 2.9, 11.2, 9.65, 10.1, 3.2, 11.2, 11.45, 9.65, 4.5, 2.7, 3.5, 1.7, 2.1, 4.8, 5.0, 2.6, 6.6, 5.0, 7.3, 5.0, 1.7, 2.6, 8.2, 8.2, 5.0, 1.2, 7.1, 9.5, 15.8, 15.5, 15.8, 17.05, 12.7, 12.3, 11.8, 11.8, 11.8, 12.3, 11.8, 13.6, 5.2, 6.2, 7.9, 7.9, 3.3, 2.8, 7.9, 3.3, 6.3, 4.9, 10.4, 4.9, 10.4, 16.0, 6.3, 2.2, 17.3, 17.3, 17.3, 17.3, 2.2, 2.2, 17.3, 6.6, 6.5, 12.3, 5.0, 2.8, 13.6, 2.8, 5.4, 10.9, 1.7, 9.15, 4.5, 9.15, 1.4, 5.9, 16.4, 1.2, 16.4, 5.9, 7.8, 7.8, 2.8, 2.9, 2.5, 12.8, 12.2, 7.7, 2.8, 2.9, 17.3, 19.3, 19.3, 19.3, 2.7, 6.4, 17.3, 2.4, 2.8, 1.7, 15.4, 15.4, 4.1, 6.6, 1.2, 2.1, 1.0, 1.1, 1.4, 1.6, 9.8, 1.9, 1.3, 7.9, 7.9, 4.5, 22.6, 7.9, 3.5, 1.2, 4.5, 2.0, 7.8, 0.9, 2.9, 2.9, 3.5, 4.2, 9.7, 10.5, 1.1, 16.1, 1.1, 8.1, 6.2, 7.7, 2.4, 16.3, 2.3, 8.4, 8.5, 6.0, 1.1, 1.75, 2.6, 1.3, 2.1, 1.1, 1.1, 2.8, 9.0, 2.8, 2.2, 5.1, 3.5, 12.7, 7.5, 2.0, 3.5, 14.3, 9.8, 12.7, 12.7, 5.1, 3.5, 12.7, 12.9, 12.9, 1.3, 10.5, 1.5, 12.7, 12.9, 1.2, 6.2, 8.8, 3.9, 1.3, 9.1, 9.1, 3.9, 1.8, 2.1, 1.4, 14.7, 9.1, 1.9, 1.8, 9.6, 3.9, 1.3, 11.8, 1.9, 12.0, 7.9, 9.3, 4.6, 2.2, 10.2, 10.6, 1.4, 9.1, 11.1, 9.1, 4.4, 2.8, 1.1, 1.3, 1.2, 3.3, 9.7, 2.3, 1.1, 11.4, 1.2, 14.7, 13.8, 1.3, 6.3, 7.9, 2.0, 11.8, 1.2, 10.0, 5.2, 1.2, 7.2, 9.9, 5.3, 13.55, 2.2, 9.9, 4.3, 13.0, 13.55, 1.0, 1.1, 6.9, 13.4, 4.6, 9.9, 3.0, 5.8, 12.9, 3.2, 0.8, 2.5, 2.4, 7.2, 7.3, 6.3, 4.25, 1.2, 2.0, 4.25, 4.7, 4.5, 1.4, 4.1, 5.3, 4.2, 6.65, 8.2, 2.6, 2.6, 2.0, 12.2, 2.3, 8.2, 5.0, 10.7, 10.8, 1.7, 1.3, 1.7, 12.7, 1.3, 1.2, 1.3, 5.7, 3.4, 1.1, 1.0, 1.0, 1.65, 6.8, 6.8, 4.9, 1.4, 2.5, 10.8, 10.8, 10.8, 10.8, 2.8, 1.3, 2.0, 1.1, 8.2, 6.0, 6.1, 8.2, 8.8, 6.1, 6.0, 1.2, 11.4, 1.3, 1.3, 6.2, 3.2, 4.5, 9.9, 6.2, 11.4, 1.3, 1.3, 0.9, 0.7, 1.0, 1.0, 10.4, 1.3, 12.5, 12.5, 12.5, 12.5, 19.25, 1.1, 12.5, 19.25, 9.0, 1.2, 9.0, 1.3, 12.8, 12.8, 7.6, 7.6, 1.4, 8.3, 9.0, 1.85, 12.55, 1.4, 1.8, 4.0, 12.55, 9.0, 3.0, 1.85, 7.9, 2.6, 1.2, 7.1, 7.9, 1.3, 10.7, 7.7, 8.4, 10.7, 12.7, 1.8, 7.7, 10.5, 1.6, 1.85, 10.5, 10.5, 1.0, 1.2, 1.7, 1.6, 9.0, 1.9, 1.2, 1.5, 3.9, 3.6, 1.2, 5.0, 2.9, 10.4, 11.4, 18.35, 18.4, 1.2, 7.1, 1.3, 1.5, 10.2, 2.2, 3.5, 3.5, 3.9, 7.4, 7.4, 11.0, 1.5, 3.9, 5.4, 1.5, 5.0, 1.2, 13.0, 13.0, 13.0, 13.0, 8.6, 1.7, 1.2, 1.2, 1.2, 2.0, 19.4, 0.8, 6.3, 6.4, 12.1, 12.1, 12.9, 2.4, 4.3, 4.2, 12.9, 1.7, 2.2, 12.1, 3.4, 7.4, 7.3, 1.1, 1.1, 1.4, 14.5, 8.0, 1.1, 1.1, 2.2, 5.8, 0.9, 6.4, 10.9, 7.3, 8.3, 1.3, 3.3, 1.0, 1.1, 1.0, 5.1, 3.2, 12.6, 3.7, 1.7, 5.1, 1.0, 1.3, 1.5, 4.6, 10.3, 6.1, 6.1, 1.2, 10.3, 9.9, 1.6, 1.1, 1.5, 1.2, 1.5, 1.1, 11.5, 7.8, 7.4, 1.45, 8.9, 1.1, 1.0, 2.5, 1.1, 2.4, 2.3, 5.1, 2.5, 8.9, 2.5, 8.9, 1.6, 1.4, 3.9, 13.7, 13.7, 9.2, 7.8, 7.6, 7.7, 3.0, 1.3, 4.0, 1.1, 2.0, 1.9, 1.4, 4.5, 10.1, 6.6, 1.9, 12.4, 1.6, 2.5, 1.2, 2.5, 0.8, 0.9, 8.1, 8.1, 11.75, 1.3, 1.9, 8.3, 8.1, 5.7, 1.9, 1.2, 11.75, 2.2, 0.9, 1.3, 1.6, 8.0, 1.2, 1.1, 0.8]\n","The mean value of each column: \n"," att0       6.854788\n","att1       0.278241\n","att2       0.334192\n","att3       6.391415\n","att4       0.045772\n","att5      35.308085\n","att6     138.360657\n","att7       0.994027\n","att8       3.188267\n","att9       0.489847\n","att10     10.514267\n","att11      5.877909\n","dtype: float64\n","There is no missing values in this data: \n"," att0     0\n","att1     0\n","att2     0\n","att3     0\n","att4     0\n","att5     0\n","att6     0\n","att7     0\n","att8     0\n","att9     0\n","att10    0\n","att11    0\n","dtype: int64\n","All the data expect the last column: \n","       att0  att1  att2  att3   att4  att5   att6     att7  att8  att9  att10\n","0      7.0  0.27  0.36  20.7  0.045  45.0  170.0  1.00100  3.00  0.45    8.8\n","1      6.3  0.30  0.34   1.6  0.049  14.0  132.0  0.99400  3.30  0.49    9.5\n","2      8.1  0.28  0.40   6.9  0.050  30.0   97.0  0.99510  3.26  0.44   10.1\n","3      7.2  0.23  0.32   8.5  0.058  47.0  186.0  0.99560  3.19  0.40    9.9\n","4      7.2  0.23  0.32   8.5  0.058  47.0  186.0  0.99560  3.19  0.40    9.9\n","...    ...   ...   ...   ...    ...   ...    ...      ...   ...   ...    ...\n","4893   6.2  0.21  0.29   1.6  0.039  24.0   92.0  0.99114  3.27  0.50   11.2\n","4894   6.6  0.32  0.36   8.0  0.047  57.0  168.0  0.99490  3.15  0.46    9.6\n","4895   6.5  0.24  0.19   1.2  0.041  30.0  111.0  0.99254  2.99  0.46    9.4\n","4896   5.5  0.29  0.30   1.1  0.022  20.0  110.0  0.98869  3.34  0.38   12.8\n","4897   6.0  0.21  0.38   0.8  0.020  22.0   98.0  0.98941  3.26  0.32   11.8\n","\n","[4898 rows x 11 columns]\n","The structure of X is: \n"," (4898, 11)\n","Only the value of last column: \n"," [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 7, 6, 8, 6, 5, 8, 7, 8, 5, 6, 6, 6, 6, 6, 7, 6, 6, 6, 6, 5, 5, 5, 6, 5, 5, 6, 6, 6, 6, 6, 7, 4, 5, 6, 5, 6, 7, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 5, 7, 5, 8, 5, 6, 5, 5, 6, 8, 5, 7, 7, 5, 5, 6, 6, 5, 6, 5, 6, 6, 6, 5, 6, 6, 5, 7, 7, 7, 6, 6, 7, 4, 6, 5, 5, 5, 5, 5, 6, 5, 6, 6, 5, 6, 5, 5, 5, 5, 4, 6, 6, 5, 5, 5, 5, 5, 6, 6, 6, 5, 7, 7, 6, 5, 7, 5, 5, 5, 5, 6, 5, 7, 6, 5, 5, 6, 6, 6, 6, 6, 4, 7, 6, 7, 6, 6, 5, 6, 6, 6, 7, 8, 8, 7, 5, 5, 6, 5, 5, 6, 7, 5, 5, 6, 6, 4, 7, 5, 6, 4, 5, 4, 6, 6, 5, 5, 6, 5, 5, 6, 5, 8, 4, 6, 5, 6, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 6, 4, 5, 5, 4, 5, 6, 5, 7, 5, 6, 7, 5, 5, 5, 5, 5, 5, 6, 7, 6, 6, 5, 6, 6, 6, 5, 4, 6, 6, 6, 6, 6, 6, 6, 7, 6, 5, 5, 7, 6, 5, 6, 7, 7, 7, 5, 4, 3, 5, 3, 6, 8, 7, 7, 6, 4, 6, 5, 5, 6, 6, 5, 6, 5, 6, 6, 6, 5, 5, 5, 5, 6, 6, 5, 4, 7, 8, 8, 4, 5, 5, 5, 6, 7, 7, 7, 7, 6, 5, 7, 3, 6, 5, 7, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 7, 6, 7, 8, 6, 6, 5, 6, 6, 5, 7, 6, 7, 5, 6, 6, 5, 5, 6, 6, 6, 5, 8, 5, 6, 5, 5, 6, 6, 6, 5, 7, 7, 6, 6, 5, 6, 6, 7, 6, 6, 5, 7, 7, 6, 7, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 7, 7, 6, 6, 6, 5, 6, 6, 5, 6, 7, 7, 7, 7, 6, 7, 7, 6, 6, 6, 7, 7, 7, 5, 6, 7, 7, 5, 6, 6, 5, 5, 5, 6, 5, 6, 6, 5, 5, 5, 6, 5, 7, 5, 6, 5, 6, 6, 5, 5, 6, 6, 6, 5, 6, 6, 7, 6, 6, 6, 7, 6, 6, 5, 5, 5, 5, 5, 7, 4, 8, 7, 5, 8, 7, 5, 7, 6, 8, 6, 6, 3, 5, 6, 6, 7, 5, 5, 7, 7, 7, 6, 7, 5, 6, 5, 5, 5, 5, 6, 5, 5, 6, 6, 5, 5, 6, 5, 6, 7, 6, 5, 7, 6, 6, 6, 5, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 6, 6, 6, 6, 4, 6, 5, 4, 6, 6, 6, 5, 6, 6, 5, 7, 6, 7, 6, 6, 6, 6, 7, 6, 5, 6, 6, 5, 5, 5, 6, 6, 5, 6, 4, 6, 6, 6, 6, 5, 6, 6, 6, 5, 5, 6, 5, 6, 4, 5, 6, 6, 6, 6, 6, 6, 7, 5, 7, 7, 7, 7, 7, 7, 5, 6, 5, 6, 7, 5, 6, 7, 5, 6, 6, 5, 6, 6, 5, 7, 5, 7, 7, 6, 6, 7, 7, 7, 5, 5, 6, 6, 7, 6, 6, 7, 7, 6, 5, 6, 5, 5, 5, 7, 5, 6, 8, 7, 6, 6, 5, 5, 6, 6, 5, 5, 5, 6, 8, 6, 5, 5, 5, 5, 7, 6, 6, 6, 5, 5, 6, 5, 5, 8, 4, 6, 6, 6, 5, 5, 6, 5, 6, 6, 7, 5, 5, 5, 7, 4, 6, 5, 5, 5, 4, 6, 5, 7, 7, 7, 7, 6, 7, 6, 6, 5, 5, 4, 5, 7, 4, 5, 6, 5, 6, 6, 6, 5, 6, 6, 8, 6, 5, 6, 6, 7, 7, 7, 5, 5, 6, 5, 5, 5, 7, 4, 6, 7, 4, 6, 5, 5, 6, 5, 6, 5, 5, 5, 5, 7, 4, 6, 6, 5, 5, 6, 6, 5, 5, 6, 6, 5, 6, 7, 6, 5, 7, 7, 5, 5, 6, 8, 7, 5, 7, 5, 5, 5, 6, 6, 7, 6, 5, 6, 6, 5, 7, 6, 3, 6, 6, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 7, 5, 6, 7, 7, 6, 6, 5, 6, 6, 6, 5, 6, 7, 7, 7, 7, 7, 5, 9, 6, 6, 5, 7, 8, 4, 6, 7, 8, 5, 6, 6, 6, 7, 6, 6, 7, 5, 7, 5, 5, 6, 6, 6, 8, 6, 5, 5, 7, 6, 6, 5, 6, 6, 6, 5, 6, 7, 6, 6, 5, 5, 5, 5, 5, 9, 6, 5, 6, 5, 6, 6, 9, 7, 7, 6, 4, 8, 6, 6, 8, 8, 8, 8, 7, 7, 7, 7, 7, 8, 8, 5, 5, 7, 6, 7, 5, 7, 5, 7, 7, 5, 5, 7, 5, 8, 7, 6, 6, 5, 6, 7, 8, 7, 6, 5, 5, 6, 3, 5, 7, 9, 6, 6, 8, 7, 6, 6, 6, 6, 7, 6, 7, 6, 7, 6, 7, 5, 7, 7, 6, 6, 6, 6, 6, 7, 6, 5, 6, 8, 4, 4, 8, 4, 5, 5, 5, 5, 5, 4, 5, 7, 6, 6, 7, 7, 6, 6, 6, 8, 5, 5, 7, 5, 5, 7, 5, 6, 5, 5, 5, 5, 5, 6, 7, 5, 6, 7, 7, 7, 7, 5, 5, 4, 5, 5, 6, 6, 5, 6, 5, 5, 7, 7, 6, 6, 7, 6, 6, 6, 5, 6, 6, 7, 7, 7, 7, 6, 7, 6, 5, 6, 5, 7, 5, 6, 7, 6, 6, 5, 6, 6, 6, 5, 7, 6, 4, 5, 4, 6, 6, 5, 6, 6, 7, 7, 5, 6, 7, 6, 6, 6, 5, 7, 6, 6, 7, 6, 5, 5, 5, 5, 6, 6, 6, 7, 7, 8, 6, 6, 6, 6, 4, 7, 4, 6, 6, 6, 6, 3, 6, 5, 5, 7, 5, 4, 5, 4, 5, 7, 5, 5, 5, 5, 6, 5, 6, 5, 4, 5, 5, 6, 5, 6, 4, 7, 5, 5, 5, 6, 5, 6, 7, 7, 6, 7, 5, 7, 5, 6, 7, 6, 5, 5, 6, 7, 6, 6, 6, 7, 5, 8, 8, 6, 7, 6, 6, 6, 7, 5, 8, 6, 7, 6, 7, 6, 6, 5, 5, 5, 7, 8, 7, 7, 4, 7, 6, 6, 5, 4, 8, 5, 5, 5, 5, 6, 6, 7, 5, 5, 6, 7, 7, 5, 7, 6, 6, 5, 5, 5, 6, 8, 8, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 5, 4, 6, 4, 4, 6, 6, 6, 6, 6, 6, 7, 6, 6, 5, 5, 6, 5, 6, 6, 5, 6, 5, 6, 5, 7, 6, 5, 5, 5, 6, 5, 6, 7, 5, 5, 8, 6, 5, 6, 7, 6, 7, 6, 6, 7, 7, 6, 7, 6, 7, 5, 6, 6, 5, 6, 5, 6, 6, 6, 5, 6, 6, 6, 5, 8, 5, 8, 8, 6, 7, 6, 5, 7, 6, 7, 5, 6, 3, 6, 7, 7, 6, 6, 5, 6, 5, 7, 5, 6, 7, 7, 7, 5, 4, 7, 6, 7, 5, 7, 5, 6, 7, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 7, 8, 7, 5, 6, 7, 5, 5, 5, 6, 6, 7, 5, 6, 6, 6, 7, 5, 8, 7, 6, 7, 7, 7, 6, 6, 6, 6, 4, 4, 6, 6, 7, 6, 5, 6, 5, 6, 6, 5, 7, 8, 5, 6, 6, 6, 6, 5, 5, 6, 6, 6, 5, 6, 5, 6, 6, 6, 5, 6, 7, 6, 6, 6, 5, 5, 6, 7, 8, 6, 6, 8, 5, 5, 6, 6, 5, 6, 6, 8, 8, 7, 7, 8, 4, 7, 7, 6, 5, 5, 5, 6, 6, 8, 7, 6, 7, 7, 4, 5, 7, 6, 5, 6, 5, 6, 7, 6, 6, 7, 7, 6, 6, 7, 6, 7, 7, 6, 6, 6, 5, 7, 6, 7, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 6, 7, 6, 8, 8, 5, 4, 8, 6, 7, 6, 6, 6, 8, 6, 6, 5, 6, 3, 5, 7, 4, 6, 5, 4, 6, 6, 6, 5, 7, 5, 4, 5, 7, 6, 5, 5, 5, 7, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 7, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 8, 5, 6, 7, 5, 5, 7, 6, 5, 6, 4, 6, 5, 6, 6, 6, 6, 6, 6, 4, 3, 6, 6, 6, 6, 5, 6, 5, 5, 8, 8, 7, 5, 7, 6, 6, 7, 5, 5, 7, 8, 7, 6, 6, 6, 5, 5, 6, 7, 6, 7, 6, 6, 6, 6, 5, 6, 5, 5, 6, 6, 5, 6, 6, 6, 6, 5, 7, 7, 6, 6, 6, 5, 6, 6, 6, 6, 4, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 6, 6, 4, 4, 6, 5, 7, 6, 5, 6, 5, 5, 6, 5, 5, 6, 6, 5, 4, 6, 6, 4, 5, 4, 5, 6, 7, 5, 6, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 6, 5, 7, 5, 9, 6, 7, 5, 7, 6, 6, 7, 7, 5, 6, 6, 6, 6, 8, 7, 5, 6, 6, 7, 6, 6, 5, 6, 6, 5, 6, 8, 7, 6, 6, 5, 6, 5, 5, 5, 6, 7, 7, 7, 7, 6, 7, 5, 4, 7, 6, 4, 6, 5, 5, 5, 6, 5, 5, 6, 6, 7, 6, 4, 8, 5, 6, 7, 6, 6, 7, 5, 5, 6, 5, 7, 6, 6, 5, 5, 6, 7, 7, 7, 7, 5, 7, 3, 6, 4, 7, 6, 5, 6, 6, 6, 6, 6, 6, 6, 5, 4, 5, 5, 6, 6, 5, 4, 5, 5, 5, 6, 6, 5, 8, 6, 6, 4, 6, 7, 7, 6, 8, 6, 6, 6, 6, 5, 6, 6, 6, 5, 6, 6, 6, 5, 6, 5, 4, 6, 6, 6, 6, 5, 5, 5, 6, 5, 6, 6, 7, 6, 7, 6, 6, 5, 5, 5, 5, 6, 6, 7, 6, 5, 5, 5, 5, 5, 7, 6, 6, 6, 6, 6, 6, 5, 6, 8, 8, 5, 4, 6, 6, 7, 6, 7, 7, 5, 7, 5, 5, 6, 5, 5, 6, 5, 8, 6, 6, 6, 5, 6, 6, 6, 5, 5, 6, 5, 6, 6, 5, 6, 6, 7, 6, 7, 4, 6, 6, 6, 5, 7, 6, 5, 5, 5, 6, 6, 7, 7, 7, 7, 7, 6, 5, 6, 7, 6, 6, 5, 7, 6, 6, 6, 5, 5, 5, 5, 6, 7, 6, 7, 5, 7, 6, 4, 5, 5, 6, 6, 6, 7, 5, 6, 6, 6, 6, 7, 7, 6, 6, 5, 5, 5, 5, 6, 6, 6, 6, 5, 6, 5, 5, 5, 6, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 7, 6, 5, 6, 5, 6, 6, 5, 7, 7, 5, 5, 6, 6, 6, 6, 7, 5, 6, 6, 6, 7, 5, 5, 5, 4, 6, 6, 5, 6, 5, 6, 3, 6, 5, 6, 5, 6, 7, 5, 5, 5, 5, 5, 5, 5, 6, 6, 5, 7, 5, 5, 4, 7, 6, 5, 5, 5, 6, 6, 5, 5, 5, 5, 6, 5, 6, 6, 7, 6, 7, 6, 7, 5, 5, 5, 6, 5, 6, 6, 6, 8, 8, 8, 8, 8, 6, 6, 5, 6, 7, 4, 8, 5, 6, 6, 6, 6, 6, 6, 6, 5, 5, 6, 6, 6, 6, 6, 6, 5, 7, 6, 5, 5, 6, 5, 6, 7, 5, 7, 6, 6, 6, 6, 6, 5, 5, 5, 5, 6, 6, 6, 5, 6, 5, 5, 5, 6, 5, 5, 5, 6, 5, 6, 6, 6, 5, 5, 5, 5, 7, 3, 5, 5, 5, 5, 6, 5, 7, 5, 5, 5, 6, 5, 5, 6, 5, 5, 5, 6, 7, 6, 6, 5, 5, 6, 5, 5, 6, 6, 4, 5, 5, 6, 5, 6, 6, 5, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 6, 5, 5, 5, 5, 5, 6, 6, 6, 5, 5, 5, 5, 6, 5, 4, 5, 6, 4, 5, 7, 5, 5, 7, 5, 5, 5, 6, 5, 5, 6, 6, 6, 6, 5, 6, 5, 5, 5, 5, 5, 5, 7, 6, 6, 6, 5, 5, 6, 6, 5, 6, 6, 4, 5, 4, 6, 6, 4, 6, 7, 6, 6, 5, 5, 5, 7, 7, 7, 7, 7, 5, 5, 7, 7, 5, 7, 5, 6, 6, 6, 5, 5, 6, 7, 5, 5, 6, 6, 6, 5, 6, 6, 6, 5, 6, 6, 5, 6, 5, 6, 5, 5, 5, 5, 6, 7, 5, 6, 7, 6, 6, 6, 5, 6, 5, 6, 6, 7, 6, 6, 7, 6, 7, 4, 5, 5, 7, 6, 7, 6, 6, 5, 5, 6, 5, 4, 6, 6, 5, 5, 5, 5, 5, 7, 4, 6, 6, 5, 6, 7, 5, 5, 6, 6, 5, 6, 5, 6, 5, 6, 6, 5, 6, 5, 5, 6, 6, 6, 7, 6, 6, 5, 7, 4, 6, 6, 6, 5, 6, 6, 5, 5, 5, 5, 5, 5, 5, 7, 7, 6, 6, 7, 7, 6, 7, 6, 8, 7, 7, 5, 5, 5, 6, 7, 5, 5, 5, 6, 5, 7, 5, 7, 6, 6, 7, 5, 4, 7, 6, 5, 6, 6, 5, 6, 5, 6, 5, 5, 6, 5, 6, 8, 5, 5, 5, 4, 5, 6, 6, 5, 8, 5, 6, 6, 4, 6, 6, 5, 5, 6, 6, 7, 6, 7, 6, 5, 5, 5, 6, 5, 6, 5, 5, 5, 5, 6, 6, 6, 7, 5, 4, 3, 6, 6, 6, 6, 5, 4, 4, 6, 8, 6, 8, 5, 4, 4, 4, 8, 8, 6, 7, 6, 5, 5, 5, 6, 6, 6, 4, 4, 6, 6, 5, 5, 6, 6, 5, 4, 6, 6, 4, 4, 4, 5, 6, 5, 5, 5, 7, 5, 5, 6, 5, 6, 6, 6, 6, 6, 5, 5, 5, 6, 6, 4, 5, 6, 5, 6, 5, 6, 5, 5, 5, 5, 5, 6, 6, 6, 5, 5, 7, 6, 5, 6, 6, 6, 6, 5, 5, 5, 5, 7, 5, 5, 5, 6, 6, 5, 5, 7, 6, 6, 7, 5, 6, 7, 6, 6, 5, 6, 6, 6, 5, 6, 5, 6, 6, 6, 6, 5, 6, 4, 4, 5, 5, 5, 5, 6, 5, 5, 4, 4, 6, 6, 6, 7, 6, 5, 6, 6, 6, 5, 7, 7, 6, 6, 7, 5, 5, 7, 8, 5, 5, 8, 7, 6, 6, 5, 6, 4, 4, 5, 7, 6, 6, 6, 6, 5, 5, 6, 5, 6, 6, 6, 5, 5, 6, 5, 5, 6, 5, 7, 7, 7, 5, 7, 7, 5, 6, 5, 6, 5, 6, 5, 7, 5, 6, 6, 5, 6, 5, 5, 5, 6, 5, 5, 7, 6, 5, 7, 7, 7, 7, 7, 7, 6, 7, 4, 6, 7, 5, 5, 7, 5, 5, 6, 5, 6, 7, 5, 6, 7, 7, 6, 6, 6, 6, 6, 5, 5, 7, 5, 5, 7, 7, 7, 7, 6, 5, 5, 7, 7, 5, 5, 7, 7, 6, 5, 6, 7, 5, 5, 5, 6, 5, 5, 6, 6, 6, 5, 5, 5, 6, 7, 7, 5, 6, 6, 5, 5, 7, 5, 5, 5, 4, 7, 7, 6, 6, 6, 6, 8, 7, 5, 7, 7, 6, 5, 7, 7, 7, 6, 7, 7, 6, 6, 5, 6, 6, 6, 6, 5, 6, 7, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 7, 5, 5, 5, 6, 6, 6, 5, 6, 5, 5, 5, 5, 6, 6, 6, 5, 6, 6, 6, 7, 5, 7, 6, 6, 5, 6, 6, 6, 7, 5, 5, 5, 7, 6, 6, 6, 7, 5, 7, 6, 5, 6, 6, 6, 7, 6, 6, 8, 6, 8, 6, 6, 8, 5, 5, 6, 5, 6, 6, 6, 5, 5, 6, 6, 6, 6, 6, 6, 5, 7, 6, 7, 7, 8, 8, 8, 6, 7, 5, 5, 6, 6, 6, 6, 5, 6, 5, 5, 5, 6, 5, 5, 7, 5, 8, 7, 5, 7, 7, 5, 5, 6, 8, 8, 6, 5, 5, 6, 7, 7, 5, 6, 7, 7, 5, 5, 7, 4, 6, 5, 5, 5, 6, 6, 6, 6, 7, 7, 5, 5, 5, 6, 7, 6, 6, 6, 6, 7, 7, 6, 6, 6, 6, 5, 7, 7, 5, 5, 5, 5, 5, 5, 6, 6, 7, 7, 8, 7, 6, 6, 6, 6, 6, 6, 6, 7, 7, 5, 6, 7, 6, 7, 8, 7, 6, 5, 5, 6, 6, 5, 7, 5, 7, 7, 6, 5, 5, 4, 6, 8, 6, 5, 7, 7, 5, 5, 5, 5, 5, 6, 7, 5, 6, 7, 5, 5, 6, 6, 6, 5, 5, 7, 5, 6, 6, 6, 7, 6, 5, 4, 6, 8, 6, 6, 5, 8, 7, 7, 6, 8, 8, 6, 6, 5, 4, 5, 5, 8, 8, 6, 5, 5, 8, 6, 8, 6, 6, 6, 6, 5, 5, 5, 5, 7, 5, 6, 6, 5, 7, 7, 6, 5, 7, 5, 8, 6, 6, 5, 6, 5, 5, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 6, 6, 6, 7, 7, 7, 7, 6, 7, 7, 7, 7, 7, 6, 8, 6, 7, 7, 7, 7, 7, 7, 6, 6, 6, 7, 7, 6, 5, 6, 6, 6, 6, 6, 6, 6, 8, 6, 6, 4, 5, 6, 6, 7, 6, 5, 6, 8, 8, 6, 5, 6, 6, 7, 5, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 5, 6, 5, 4, 5, 7, 5, 6, 6, 5, 5, 5, 6, 5, 5, 8, 5, 5, 6, 5, 4, 6, 8, 8, 7, 6, 5, 5, 7, 5, 5, 5, 5, 6, 5, 6, 7, 6, 6, 7, 3, 6, 7, 6, 6, 6, 7, 6, 6, 7, 5, 7, 7, 7, 6, 6, 7, 5, 6, 6, 6, 5, 4, 6, 7, 5, 6, 6, 6, 7, 7, 7, 5, 6, 5, 6, 5, 6, 5, 7, 5, 6, 6, 6, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, 7, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 5, 7, 7, 6, 6, 5, 7, 7, 6, 6, 6, 7, 6, 6, 6, 6, 8, 8, 8, 6, 6, 5, 6, 4, 6, 6, 5, 6, 7, 6, 4, 7, 8, 5, 7, 6, 6, 5, 5, 6, 7, 6, 7, 7, 7, 6, 7, 7, 5, 7, 7, 6, 5, 5, 5, 6, 5, 6, 6, 6, 5, 5, 4, 7, 5, 6, 6, 6, 6, 7, 6, 6, 6, 7, 6, 6, 5, 6, 6, 6, 6, 7, 5, 7, 6, 7, 6, 7, 7, 5, 6, 6, 7, 7, 6, 6, 8, 8, 5, 6, 6, 6, 6, 6, 6, 5, 7, 8, 6, 3, 6, 6, 6, 5, 5, 7, 7, 7, 5, 4, 8, 7, 5, 6, 5, 6, 7, 6, 6, 7, 7, 7, 6, 5, 5, 8, 5, 7, 6, 5, 5, 5, 6, 7, 5, 8, 6, 7, 7, 7, 7, 3, 6, 7, 7, 7, 7, 7, 6, 7, 6, 6, 7, 5, 5, 6, 7, 6, 6, 7, 5, 7, 5, 6, 6, 6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 6, 7, 6, 6, 7, 6, 6, 6, 6, 7, 6, 6, 6, 7, 6, 6, 6, 7, 5, 6, 6, 6, 8, 5, 7, 7, 7, 6, 6, 6, 8, 5, 6, 7, 6, 8, 6, 6, 5, 5, 6, 6, 5, 6, 6, 5, 6, 5, 6, 6, 6, 5, 6, 3, 7, 6, 6, 6, 7, 7, 6, 4, 6, 7, 5, 8, 8, 5, 6, 6, 6, 6, 6, 5, 6, 6, 5, 7, 6, 6, 6, 5, 5, 5, 5, 5, 7, 6, 5, 8, 6, 6, 7, 8, 7, 7, 5, 6, 5, 6, 5, 7, 7, 6, 6, 8, 6, 7, 5, 6, 6, 5, 8, 6, 8, 6, 6, 8, 6, 6, 8, 7, 6, 7, 8, 5, 8, 7, 8, 7, 6, 6, 6, 8, 7, 6, 7, 7, 6, 7, 7, 6, 6, 7, 7, 6, 5, 7, 7, 7, 6, 7, 5, 6, 6, 7, 6, 6, 6, 7, 7, 7, 5, 7, 5, 7, 5, 5, 6, 6, 6, 6, 4, 7, 5, 5, 6, 6, 5, 5, 6, 5, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 6, 4, 6, 5, 6, 5, 6, 6, 6, 6, 7, 6, 6, 4, 6, 6, 6, 7, 5, 7, 4, 7, 5, 7, 6, 6, 7, 7, 7, 6, 6, 6, 7, 5, 7, 7, 7, 7, 7, 6, 7, 6, 6, 7, 6, 7, 6, 5, 5, 6, 6, 6, 5, 6, 6, 6, 5, 6, 5, 5, 7, 6, 7, 7, 6, 6, 7, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 6, 7, 7, 7, 6, 6, 6, 6, 6, 7, 6, 7, 7, 5, 6, 4, 6, 6, 6, 6, 7, 7, 8, 7, 8, 8, 6, 4, 6, 8, 8, 7, 8, 5, 7, 6, 7, 8, 7, 5, 6, 7, 6, 5, 7, 6, 8, 7, 6, 7, 6, 5, 5, 6, 8, 7, 6, 7, 7, 5, 6, 7, 6, 6, 5, 5, 5, 6, 6, 6, 6, 6, 6, 5, 5, 5, 6, 5, 5, 4, 6, 5, 6, 5, 5, 7, 5, 5, 7, 6, 6, 7, 7, 7, 5, 6, 6, 5, 6, 5, 6, 4, 5, 6, 7, 7, 7, 7, 7, 7, 7, 5, 6, 5, 6, 7, 5, 5, 5, 8, 6, 5, 5, 5, 6, 6, 7, 5, 5, 8, 5, 5, 5, 6, 5, 4, 6, 6, 5, 5, 6, 5, 6, 7, 7, 5, 6, 6, 5, 6, 7, 5, 5, 5, 6, 5, 5, 6, 6, 5, 6, 6, 5, 5, 5, 5, 6, 5, 6, 5, 6, 5, 5, 6, 6, 3, 5, 5, 5, 5, 7, 5, 6, 6, 6, 5, 6, 6, 5, 7, 6, 6, 5, 6, 7, 7, 5, 5, 6, 5, 5, 6, 6, 5, 5, 6, 6, 6, 7, 6, 5, 6, 5, 6, 5, 6, 7, 6, 7, 6, 6, 6, 6, 6, 5, 5, 6, 6, 6, 7, 6, 6, 5, 6, 6, 6, 6, 4, 5, 5, 7, 5, 5, 5, 4, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 6, 6, 6, 6, 7, 5, 5, 4, 6, 6, 7, 7, 7, 7, 7, 7, 6, 6, 7, 6, 7, 7, 6, 5, 7, 6, 6, 5, 7, 5, 5, 5, 6, 5, 5, 5, 6, 6, 8, 4, 6, 5, 6, 5, 6, 6, 5, 5, 6, 6, 6, 6, 6, 7, 5, 5, 5, 5, 6, 7, 5, 6, 5, 5, 6, 6, 6, 5, 7, 7, 5, 4, 6, 4, 6, 6, 7, 6, 6, 4, 6, 7, 7, 6, 7, 6, 5, 7, 7, 6, 7, 6, 6, 6, 6, 6, 6, 5, 7, 7, 7, 5, 7, 7, 6, 6, 6, 5, 6, 6, 7, 6, 6, 5, 6, 6, 6, 7, 6, 6, 6, 5, 5, 6, 6, 5, 4, 5, 5, 6, 6, 6, 6, 5, 6, 6, 5, 5, 5, 6, 5, 6, 5, 5, 5, 4, 5, 5, 7, 7, 7, 7, 7, 6, 6, 6, 7, 6, 5, 7, 7, 6, 5, 6, 6, 6, 5, 5, 6, 5, 5, 8, 6, 5, 6, 6, 5, 6, 7, 5, 4, 6, 5, 6, 6, 6, 6, 6, 6, 5, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 5, 6, 5, 6, 6, 5, 5, 7, 7, 7, 5, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 8, 7, 5, 6, 6, 6, 7, 5, 5, 5, 5, 7, 6, 5, 5, 6, 6, 5, 6, 5, 6, 7, 6, 6, 6, 6, 6, 6, 7, 7, 6, 7, 5, 6, 6, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 5, 5, 7, 7, 8, 6, 7, 7, 5, 6, 5, 6, 7, 6, 6, 7, 5, 6, 6, 6, 7, 7, 5, 5, 6, 6, 5, 6, 5, 6, 6, 6, 8, 6, 5, 5, 6, 6, 6, 6, 5, 6, 6, 6, 7, 6, 6, 6, 6, 4, 4, 5, 5, 5, 4, 6, 5, 6, 6, 4, 4, 7, 5, 7, 7, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 7, 6, 6, 7, 6, 5, 5, 6, 6, 6, 6, 6, 6, 6, 4, 5, 5, 5, 5, 6, 6, 6, 7, 6, 6, 6, 6, 7, 7, 6, 5, 6, 5, 6, 6, 6, 6, 7, 5, 4, 6, 6, 5, 5, 6, 6, 6, 6, 5, 6, 6, 5, 5, 6, 5, 5, 5, 6, 6, 6, 6, 5, 5, 6, 6, 6, 6, 6, 7, 6, 6, 6, 5, 6, 6, 7, 7, 5, 7, 7, 7, 5, 6, 7, 6, 6, 5, 5, 5, 5, 5, 5, 5, 8, 8, 8, 8, 8, 8, 8, 8, 8, 6, 6, 6, 6, 6, 6, 7, 6, 5, 6, 6, 5, 6, 6, 6, 5, 6, 5, 6, 5, 6, 6, 5, 6, 5, 6, 5, 6, 6, 5, 5, 6, 6, 5, 6, 5, 6, 5, 5, 6, 6, 6, 6, 5, 5, 6, 6, 6, 4, 5, 5, 5, 6, 6, 5, 5, 5, 5, 5, 5, 6, 6, 5, 5, 5, 7, 6, 5, 7, 5, 7, 7, 7, 7, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 5, 6, 6, 5, 6, 6, 6, 6, 5, 5, 5, 6, 5, 5, 6, 6, 6, 6, 5, 6, 6, 5, 5, 6, 6, 5, 5, 5, 7, 5, 6, 6, 6, 6, 5, 5, 5, 5, 6, 6, 6, 6, 5, 5, 6, 6, 6, 5, 5, 5, 5, 5, 6, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 6, 6, 6, 6, 7, 5, 5, 6, 5, 4, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, 6, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 5, 6, 5, 6, 6, 5, 5, 6, 6, 6, 6, 5, 5, 6, 6, 6, 6, 6, 5, 5, 6, 6, 7, 6, 6, 7, 7, 7, 5, 5, 5, 6, 6, 6, 7, 7, 6, 7, 7, 5, 6, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 6, 6, 6, 6, 6, 5, 6, 7, 4, 6, 5, 5, 5, 5, 7, 5, 7, 7, 5, 6, 7, 7, 6, 6, 5, 6, 6, 6, 7, 7, 7, 6, 6, 6, 6, 5, 6, 5, 6, 6, 6, 7, 6, 6, 7, 7, 7, 5, 5, 5, 7, 5, 7, 7, 7, 7, 7, 6, 6, 6, 5, 6, 7, 7, 6, 5, 7, 7, 5, 6, 5, 5, 7, 7, 6, 6, 7, 6, 5, 4, 6, 6, 6, 6, 5, 4, 6, 6, 6, 6, 7, 5, 6, 7, 7, 6, 7, 6, 5, 5, 6, 6, 7, 6, 6, 7, 6, 6, 6, 7, 6, 6, 6, 7, 6, 6, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 6, 6, 6, 6, 6, 6, 7, 6, 6, 5, 6, 6, 5, 5, 3, 6, 6, 5, 6, 5, 6, 7, 6, 6, 6, 7, 7, 6, 6, 6, 6, 7, 7, 6, 8, 8, 7, 6, 6, 6, 6, 5, 7, 4, 6, 6, 6, 6, 4, 5, 5, 6, 6, 5, 7, 8, 8, 5, 6, 6, 6, 6, 6, 5, 7, 7, 6, 5, 6, 7, 7, 8, 7, 4, 6, 6, 6, 5, 6, 5, 6, 7, 5, 6, 7, 6, 6, 6, 6, 5, 6, 6, 7, 5, 6, 6, 6, 5, 7, 6, 6, 5, 6, 7, 6, 5, 6, 6, 4, 7, 6, 6, 5, 6, 5, 6, 7, 6, 5, 6, 5, 5, 5, 6, 6, 6, 6, 5, 6, 6, 6, 6, 7, 5, 6, 6, 6, 6, 6, 7, 6, 5, 6, 6, 6, 7, 5, 4, 6, 6, 6, 5, 6, 5, 6, 7, 7, 5, 6, 6, 6, 5, 6, 5, 6, 7, 6]\n","The structure of y is: \n"," (4898,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5L_dI7XnuHrP"},"source":["## Honor Code Affirmation ##\n","\n","As a student at Union College, I am part of a community that values intellectual effort, curiosity and discovery. I understand that in order to truly claim my educational and academic achievements, I am obligated to act with academic integrity. Therefore, I affirm that I will carry out my academic endeavors with full academic honesty, and I rely on my fellow students to do the same.\n","\n","**Irene Yin**"]}]}
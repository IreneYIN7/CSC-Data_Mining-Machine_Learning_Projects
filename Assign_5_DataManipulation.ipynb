{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Irene CSC-321 Assignment 5_DataManipulation.ipynb","provenance":[{"file_id":"1kxEEHaXo3eAgp1bSDDSdUkd4BcGRKC1G","timestamp":1620288063797},{"file_id":"1XGeG7MdrpY2vn4_UpspFYBrBfWukEiUB","timestamp":1588017449903}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"NdO0ZhXOvwGa"},"source":["# CSC-321: Data Mining and Machine Learning\n","# Irene Yin\n","\n","## Assignment 5: Working with real data\n","\n","For this notebook, you should use your own implementation of ML code. That means use your own functions for SLR, MLR, SGD, , Logistic Regression, zeroR etc. I will tell you if you should use scikit implementations.\n","\n","**FORMATTING NOTE:** What follows is a series of experiements, using different datasets, with different numbers of instances and features. You're going to end up with a long notebook containing all kinds of print statements and cells containing numbers. It is VERY easy to get confused. I strongly suggest the following:\n","- minimize output. This is tricky, given that you should ALWAYS do sanity checks on your data (using functions like .info(),.head() and .describe() that I introduce below). I recommend performing them, but then comment out the print statement. Once you've checked your data you don't have to always display it.\n","- Keep what output you have COMPACT. Don't overdo it, squashing things up together, but be mindful of spacing, like I suggested with the output of your SGD function. \n","- Feel free, once you are convinced things like SGD work, to replace the constant output of epoch/learning rate etc. I edit mine to print just TWICE. Once at the start (epoch 0), and once at the end. This isn't optimal, but I'll explain why later.\n","- ALWAYS print the following information at the top of any cell when you run an experiment: Data set name, number of instances, number of features\n","- ALWAYS include algorithm name when printing a result. \n","\n","In grading your notebook, I WILL BE ATTENTIVE TO LAYOUT AND PRESENTATION OF RESULTS. If you're just dumping numbers to the screen you're going to have a bad time.\n","\n","There are cells below where you have to edit things, or write things, and there are cells where you have to just run them, and observe the results. I recommend running this notebook one cell at a time, and making sure you are clear on what the cell is doing, and why.\n","\n","### Part 1: Loading Data\n","\n","We're going to make use of the pandas library (because pandas are cute/not_cute(*)) to load data and to get a quick overview. For the first data set, I'm going to walk you through some typical commands. Note that in your work, you don't always have to print these out, it's just good practice as you're working through a problem. I have lost track of the number of times I have sliced a data set, and left behind more instances or features than I intended to. \n","\n","For reference:\n","- [Pandas](https://pandas.pydata.org/)\n","\n","(*)delete as appropriate\n","\n","Run the following cell."]},{"cell_type":"code","metadata":{"id":"kGrXgUsQvwGb","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1622146105373,"user_tz":240,"elapsed":566,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"5032feee-6035-4207-f210-c51d3fd2cc3f"},"source":["%matplotlib inline\n","import pandas as pd\n","\n","# Load the data\n","\n","insurance_data = pd.read_csv('https://raw.githubusercontent.com/nixwebb/CSV_Data/master/insurance.csv')\n","\n","# Show the head - just the first 5 entries\n","\n","insurance_data.head()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>108</th>\n","      <th>392.5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>19</td>\n","      <td>46.2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>13</td>\n","      <td>15.7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>124</td>\n","      <td>422.2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>40</td>\n","      <td>119.4</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>57</td>\n","      <td>170.9</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   108  392.5\n","0   19   46.2\n","1   13   15.7\n","2  124  422.2\n","3   40  119.4\n","4   57  170.9"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"MJHgLalGdZPI"},"source":["Look at the data above. The column headings should look weird. That's because pandas decided to treat the first row as column headings. I can prevent that by supplying my own column headings, which I demonstrate below. \n","\n","I chose X and y. While this is illustrative for this example, it's bad. Why?\n","Those aren't the actual column headings. What are they? You'll need to find out by looking at the information for the data set. I include a link, below. You should ALWAYS know what the data represents. \n","\n","It's a good idea in pandas to use descriptive but short names for your columns, as we are going to use them for indexing. I would avoid using spaces in column names, and use underscores instead: e.g. 'birth date', should be 'birth_date' or 'DOB'. That said, also avoid acronyms that ONLY mean something to you.\n","\n","(a) Edit the column headings below, to be more descriptive based on your understanding of the data described below.\n","\n","- [Insurance Data](http://college.cengage.com/mathematics/brase/understandable_statistics/7e/students/datasets/slr/frames/slr06.html)"]},{"cell_type":"code","metadata":{"id":"FXR3Caoxaee8","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1622146105376,"user_tz":240,"elapsed":46,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"00666735-30e3-47c0-bae7-5b77acda73bb"},"source":["\n","\n","insurance_data = pd.read_csv('https://raw.githubusercontent.com/nixwebb/CSV_Data/master/insurance.csv', names=['X','y'])\n","\n","insurance_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>X</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>108</td>\n","      <td>392.5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>19</td>\n","      <td>46.2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>13</td>\n","      <td>15.7</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>124</td>\n","      <td>422.2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>40</td>\n","      <td>119.4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     X      y\n","0  108  392.5\n","1   19   46.2\n","2   13   15.7\n","3  124  422.2\n","4   40  119.4"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"udHTG1ckeCpc"},"source":["Now that I have column headings, I can use them in pandas. For example, if I wanted to extract just the column called 'X' into a new data frame, I would write:\n","\n","new_df = insurance_data['X'].copy()\n","\n","The .copy() is so that I make a new copy of the values, rather than references to entries in a data frame, which can have unintended side effects. Dataframes are MUTABLE - any change I make to the contents of a dataframe can be permanent. \n"]},{"cell_type":"markdown","metadata":{"id":"GObyV3sNwawT"},"source":["The next few cells are showing typical information we want to know about our data. We can use the .info() method, to see counts of instances, columns, and data types."]},{"cell_type":"code","metadata":{"id":"5XS2-qNjayIW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622146105377,"user_tz":240,"elapsed":44,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"b8dbc00d-e045-448c-edbc-211daa6c8160"},"source":["insurance_data.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 63 entries, 0 to 62\n","Data columns (total 2 columns):\n"," #   Column  Non-Null Count  Dtype  \n","---  ------  --------------  -----  \n"," 0   X       63 non-null     int64  \n"," 1   y       63 non-null     float64\n","dtypes: float64(1), int64(1)\n","memory usage: 1.1 KB\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BsL4Q7f-fFKy"},"source":["And we can use .describe() to get more detailed information. \n","- The count is the number of entries for that column. \n","- The mean is the average value. \n","- The std is the value for one standard deviation \n","  - i.e. in our y column, the mean is 98.19, and the std is 87.33\n","  - That means that one standard deviation either side of the mean ranges from 10.86 to 185.52\n","  - We can expect around 68% of our data to\n","fall in this range, IF our data is normally distributed. \n","- The min and the max are...well, I think you know\n","- The quartile values (25/50/75) show what the value is at those markers\n","  - i.e. 25% of all our values of X are below 7.5"]},{"cell_type":"code","metadata":{"id":"2VfFy07ra4iK","colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"status":"ok","timestamp":1622146105379,"user_tz":240,"elapsed":41,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"b295625a-0172-4754-8b82-3776c39fdb09"},"source":["insurance_data.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>X</th>\n","      <th>y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>63.000000</td>\n","      <td>63.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>22.904762</td>\n","      <td>98.187302</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>23.351946</td>\n","      <td>87.327553</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>7.500000</td>\n","      <td>38.850000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>14.000000</td>\n","      <td>73.400000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>29.000000</td>\n","      <td>140.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>124.000000</td>\n","      <td>422.200000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                X           y\n","count   63.000000   63.000000\n","mean    22.904762   98.187302\n","std     23.351946   87.327553\n","min      0.000000    0.000000\n","25%      7.500000   38.850000\n","50%     14.000000   73.400000\n","75%     29.000000  140.000000\n","max    124.000000  422.200000"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"7N2b3cvcgawc"},"source":["Let's take a graphical look at the data. As there are only two features, we can plot one against the other, just as you did with the data in assignment 2.\n","\n","First I'm going to extract the values from the pandas dataframe, using the .values attribute. That puts the values into a numpy array, that I've called insurance_values\n","\n","(b) Slice insurance_values into 2 datasets, X_values and y_values. Note that to work with scikit, the X_values should be an array of 63 instances, and 1 column (NOT a 1-Dimensional vector). i.e. it should have shape (63,1). y_values SHOULD be a 1-Dimensional vector, i.e. have shape (63,). Refer back to Assignment 1 if you need to refresh your memory. IT's ALWAYS a good idea to print out the number of instances (rows) and the number of features (columns) to make sure you're not accidentally messing with things. I've done that for you. If you're slicing is correct, the y values will have the SAME number of instances. It doesn't have a column value - because it's a one dimensional vector.\n","\n","(c) Plot X_values against y_values so we can visualize the instances. Make each instance a blue triangle. Label the axes appropriately, using the same names as your column headings from a.\n"]},{"cell_type":"code","metadata":{"id":"qUh67U1xgbUr","colab":{"base_uri":"https://localhost:8080/","height":337},"executionInfo":{"status":"ok","timestamp":1622146105380,"user_tz":240,"elapsed":40,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"57b35b1c-f951-44a1-dddb-eb6a54367e97"},"source":["\n","# Slice your data\n","\n","insurance_values = insurance_data.values\n","\n","X_values = insurance_values[:,:-1]\n","y_values = insurance_values[:,len(insurance_values[0])-1]\n","\n","rows,cols = X_values.shape\n","print(\"This is the insurance data training set. It has\", rows, \"instances, and it has\", cols, \"input features.\")\n","rows, = y_values.shape\n","print(\"This is the insurance data y values. It has\", rows, \"instances.\")\n","\n","import matplotlib.pyplot as plt\n","\n","# Plot your graph\n","plotDataX = [i[0] for i in insurance_values]\n","plotDataY = [i[1] for i in insurance_values]\n","plt.plot(plotDataX, plotDataY,'b^')\n","plt.axis([0,150,0,450])\n","plt.xlabel(\"X\")\n","plt.ylabel(\"y\")\n","plt.show()\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is the insurance data training set. It has 63 instances, and it has 1 input features.\n","This is the insurance data y values. It has 63 instances.\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"IyAf7m3PiAyl"},"source":["### Part 2: Learning from real data\n","\n","If you've done it right, you should have a visual sense of a relationship in the data between the X and y values. That tells us what kind of model we might want to learn.\n","\n","We'll start by using a training and test set that are the same, so create X_train and y_train, and X_test and y_test as copies of X_values and y_values. \n","\n","\n","(d) Copy your linear regression code from Assignment 2 below. Run simple linear regression on this data, to get predicted y values for X_test.\n","\n","(e) Copy your zeroR code from Assignment 2. Run zeroR on the data, to get predicted y values\n","\n","(f) Plot both zeroR and slr predicted relationships as lines (red for slr, green for zeroR), as well as the blue data points above.\n","\n","(g) Calculate and print RMSE for both zeroR and slr, compared to y_test.\n","\n","(h) In the text box, INTERPRET these scores for me. **This is the important bit.** Can you tell me what these RMSE scores REALLY mean with respect to this data? To answer that, you have to understand the insurance data. Refer back to the link I gave you for the data earlier. ALSO tell me if you think we can do better? Do you believe there are other models that will generate LOWER error scores?\n","\n","REMEMBER: We're predicting y values. What are those? Remember too that RMSE returns you the average error over all predictions, and that the error is expressed in the same units as the thing we're predicting. How does this help us?\n"]},{"cell_type":"code","metadata":{"id":"qC58-rWN0rcR","colab":{"base_uri":"https://localhost:8080/","height":317},"executionInfo":{"status":"ok","timestamp":1622146105381,"user_tz":240,"elapsed":33,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"b4594b7f-800d-43b7-a183-8460c1dd916c"},"source":["import numpy as np\n","# Add your code here\n","#insurance_values = insurance_values.tolist()\n","\n","X_train = [i[0] for i in insurance_values]\n","X_test = X_train\n","y_train = y_values\n","y_test = y_train\n","\n","#d)\n","\n","def getMean(inputList):\n","  # return: the average (the mean) for any list of values(numbers).\n","  return np.mean(inputList)\n","\n","\n","def getVariance(inputList):\n","  # return: the total variance of a list of values.\n","  meanValue = getMean(inputList)\n","  variance = 0\n","  for num in inputList:\n","    variance += (num-meanValue)**2\n","  return variance\n","\n","def getCovariance(inputListX, inputListY, meanX, meanY):\n","  # return: covariance between two lists of numbers.\n","  covariance = 0\n","  for i in range(len(inputListX)):\n","    covariance += (inputListX[i]-meanX)*(inputListY[i] - meanY)\n","  return covariance\n","\n","\n","def getCoefficient(inputListX, inputListY):\n","  # return: coefficient b0 and b1 in a single list.\n","  meanOfX = getMean(inputListX)\n","  meanOfY = getMean(inputListY)\n","  b1 = getCovariance(inputListX,inputListY,meanOfX,meanOfY)/getVariance(inputListX)\n","  b0 = getMean(inputListY)-b1*meanOfX\n","  return [b0,b1]\n","\n","\n","def slr(Xtrain,ytrain,Xtest):\n","  # return: a list that hold the predictions. \n","  coef = getCoefficient(Xtrain,ytrain)\n","  b0 = coef[0]\n","  b1 = coef[1]\n","  prediction = []\n","  for num in Xtest:\n","    prediction.append(b0+b1*num)\n","  return prediction\n","\n","slr_predY = slr(X_train,y_train,X_test)\n","\n","#e\n","def zeroRR(ytrain, Xtest):\n","  # return: compute the mean from the ytrain values and return the list of predictions.\n","  meanOfytrain = np.mean(ytrain)\n","  prediction = []\n","  for num in Xtest:\n","    prediction.append(meanOfytrain)\n","  return prediction\n","\n","ZeroR_predict = zeroRR(y_train, X_test)\n","\n","#f\n","plotDataX = [i[0] for i in insurance_values]\n","plotDataY = [i[1] for i in insurance_values]\n","plt.plot(plotDataX, plotDataY,'b^')\n","plt.axis([0,150,0,450])\n","plt.xlabel(\"X\")\n","plt.ylabel(\"y\")\n","plt.plot(plotDataX, slr_predY, 'r')\n","plt.plot(plotDataX, ZeroR_predict,'g')\n","plt.show()\n","\n","#g)\n","def rmse(actual, predicted):\n","  # return: prediction error.\n","  predictionError = 0.0\n","  for i in range(len(actual)):\n","    predictionError += (actual[i] - predicted[i])**2\n","  predictionError_avg = predictionError/len(actual)\n","  predictionError_avg = predictionError_avg**0.5\n","  return predictionError_avg\n","\n","slr_rmse = rmse(y_test,slr_predY)\n","zr_rmse = rmse(y_test,ZeroR_predict)\n","\n","print('RMSE score for linear regression: {:.2f}'.format(slr_rmse))\n","print('RMSE score for zeroR: {:.2f}'.format(zr_rmse))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"stream","text":["RMSE score for linear regression: 35.37\n","RMSE score for zeroR: 86.63\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"z4AvaNsU0teG"},"source":["RMSE is to measure the error of a model in predicting numeric data. It tells us the average error of payments predictions for each set of claims. The RMSE here means for each set of claims, the payments should be (RMSE value)thousands of Swedish Kronor for geographical zones in Sweden + - predicted payments. According to these data, Linear Regression did a better job than zeroR because for each set of claims, the predicted payments would in error of 35.37 thousands of Swedish Kronor for geographical zones in Sweden, which is better than the error of 86.63 thousands of Swedish Kronor for geographical zones in Sweden. \n","\n","No I don't think we can't do better because the line that LR has is already fitting very well on the data via the graph. \n"]},{"cell_type":"markdown","metadata":{"id":"WFGsOiLmvwGm"},"source":["### Part 3: Normalization\n","\n","When working with data that has multiple inputs, we often want to normalize the data, so that it's all on the same scale (usually 0-1, using min-max normalization). The steps to do that are below. \n","\n","We're going to use min-max normalization, that works by:\n","\n","normalized value = (value - minOfFeature) / (maxOfFeature - minOfFeature)\n","\n","Below I'm going to use pandas to load a different dataset. This one is about wine. You can [read a bit about the data](http://archive.ics.uci.edu/ml/datasets/Wine+Quality). \n","\n","(i) Choose meaningful headings for the columns\n"]},{"cell_type":"code","metadata":{"id":"rkjQx736vwGn","colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"status":"ok","timestamp":1622146105870,"user_tz":240,"elapsed":521,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"3a07e1a5-63d0-4f89-b0b7-dee09a5ffe00"},"source":[" labels = ['fixed_acidity','volatile_acidity','citric_acid','residual_sugar','chlorides','free_sulfur_dioxide','total_sulfur_dioxide'\n","                        ,'density','pH','sulphates','alcohol','quality']\n","\n","\n","wine_data = pd.read_csv('https://raw.githubusercontent.com/nixwebb/CSV_Data/master/winequality-white.csv',names= labels\n","                        )\n","\n","wine_data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fixed_acidity</th>\n","      <th>volatile_acidity</th>\n","      <th>citric_acid</th>\n","      <th>residual_sugar</th>\n","      <th>chlorides</th>\n","      <th>free_sulfur_dioxide</th>\n","      <th>total_sulfur_dioxide</th>\n","      <th>density</th>\n","      <th>pH</th>\n","      <th>sulphates</th>\n","      <th>alcohol</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>7.0</td>\n","      <td>0.27</td>\n","      <td>0.36</td>\n","      <td>20.7</td>\n","      <td>0.045</td>\n","      <td>45.0</td>\n","      <td>170.0</td>\n","      <td>1.0010</td>\n","      <td>3.00</td>\n","      <td>0.45</td>\n","      <td>8.8</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6.3</td>\n","      <td>0.30</td>\n","      <td>0.34</td>\n","      <td>1.6</td>\n","      <td>0.049</td>\n","      <td>14.0</td>\n","      <td>132.0</td>\n","      <td>0.9940</td>\n","      <td>3.30</td>\n","      <td>0.49</td>\n","      <td>9.5</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8.1</td>\n","      <td>0.28</td>\n","      <td>0.40</td>\n","      <td>6.9</td>\n","      <td>0.050</td>\n","      <td>30.0</td>\n","      <td>97.0</td>\n","      <td>0.9951</td>\n","      <td>3.26</td>\n","      <td>0.44</td>\n","      <td>10.1</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>7.2</td>\n","      <td>0.23</td>\n","      <td>0.32</td>\n","      <td>8.5</td>\n","      <td>0.058</td>\n","      <td>47.0</td>\n","      <td>186.0</td>\n","      <td>0.9956</td>\n","      <td>3.19</td>\n","      <td>0.40</td>\n","      <td>9.9</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>7.2</td>\n","      <td>0.23</td>\n","      <td>0.32</td>\n","      <td>8.5</td>\n","      <td>0.058</td>\n","      <td>47.0</td>\n","      <td>186.0</td>\n","      <td>0.9956</td>\n","      <td>3.19</td>\n","      <td>0.40</td>\n","      <td>9.9</td>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   fixed_acidity  volatile_acidity  citric_acid  ...  sulphates  alcohol  quality\n","0            7.0              0.27         0.36  ...       0.45      8.8        6\n","1            6.3              0.30         0.34  ...       0.49      9.5        6\n","2            8.1              0.28         0.40  ...       0.44     10.1        6\n","3            7.2              0.23         0.32  ...       0.40      9.9        6\n","4            7.2              0.23         0.32  ...       0.40      9.9        6\n","\n","[5 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"gcTDsdqe-l_x"},"source":["(j) Get a little more information about our data, using the .describe() method. \n","\n","Pay attention to the values for each feature. Note that they are NOT on the same scale (and look at the min and max values for each). All the features are numerical, including the last feature, which is a representation of a quality of wine, on the scale 0-10. \n","\n","Also note how many features (columns) there are, and how many instances."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BTTem0BtK-sD","executionInfo":{"status":"ok","timestamp":1622146105871,"user_tz":240,"elapsed":16,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"04986a6e-5474-4913-e379-b0133f887961"},"source":["wine_data.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 4898 entries, 0 to 4897\n","Data columns (total 12 columns):\n"," #   Column                Non-Null Count  Dtype  \n","---  ------                --------------  -----  \n"," 0   fixed_acidity         4898 non-null   float64\n"," 1   volatile_acidity      4898 non-null   float64\n"," 2   citric_acid           4898 non-null   float64\n"," 3   residual_sugar        4898 non-null   float64\n"," 4   chlorides             4898 non-null   float64\n"," 5   free_sulfur_dioxide   4898 non-null   float64\n"," 6   total_sulfur_dioxide  4898 non-null   float64\n"," 7   density               4898 non-null   float64\n"," 8   pH                    4898 non-null   float64\n"," 9   sulphates             4898 non-null   float64\n"," 10  alcohol               4898 non-null   float64\n"," 11  quality               4898 non-null   int64  \n","dtypes: float64(11), int64(1)\n","memory usage: 459.3 KB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fcK0rTCB_gIS","colab":{"base_uri":"https://localhost:8080/","height":317},"executionInfo":{"status":"ok","timestamp":1622146105872,"user_tz":240,"elapsed":12,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"4bed2eb6-588c-4394-efb3-bae6d07efb72"},"source":["# Call describe here\n","wine_data.describe()\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fixed_acidity</th>\n","      <th>volatile_acidity</th>\n","      <th>citric_acid</th>\n","      <th>residual_sugar</th>\n","      <th>chlorides</th>\n","      <th>free_sulfur_dioxide</th>\n","      <th>total_sulfur_dioxide</th>\n","      <th>density</th>\n","      <th>pH</th>\n","      <th>sulphates</th>\n","      <th>alcohol</th>\n","      <th>quality</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","      <td>4898.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>6.854788</td>\n","      <td>0.278241</td>\n","      <td>0.334192</td>\n","      <td>6.391415</td>\n","      <td>0.045772</td>\n","      <td>35.308085</td>\n","      <td>138.360657</td>\n","      <td>0.994027</td>\n","      <td>3.188267</td>\n","      <td>0.489847</td>\n","      <td>10.514267</td>\n","      <td>5.877909</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.843868</td>\n","      <td>0.100795</td>\n","      <td>0.121020</td>\n","      <td>5.072058</td>\n","      <td>0.021848</td>\n","      <td>17.007137</td>\n","      <td>42.498065</td>\n","      <td>0.002991</td>\n","      <td>0.151001</td>\n","      <td>0.114126</td>\n","      <td>1.230621</td>\n","      <td>0.885639</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>3.800000</td>\n","      <td>0.080000</td>\n","      <td>0.000000</td>\n","      <td>0.600000</td>\n","      <td>0.009000</td>\n","      <td>2.000000</td>\n","      <td>9.000000</td>\n","      <td>0.987110</td>\n","      <td>2.720000</td>\n","      <td>0.220000</td>\n","      <td>8.000000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>6.300000</td>\n","      <td>0.210000</td>\n","      <td>0.270000</td>\n","      <td>1.700000</td>\n","      <td>0.036000</td>\n","      <td>23.000000</td>\n","      <td>108.000000</td>\n","      <td>0.991723</td>\n","      <td>3.090000</td>\n","      <td>0.410000</td>\n","      <td>9.500000</td>\n","      <td>5.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>6.800000</td>\n","      <td>0.260000</td>\n","      <td>0.320000</td>\n","      <td>5.200000</td>\n","      <td>0.043000</td>\n","      <td>34.000000</td>\n","      <td>134.000000</td>\n","      <td>0.993740</td>\n","      <td>3.180000</td>\n","      <td>0.470000</td>\n","      <td>10.400000</td>\n","      <td>6.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>7.300000</td>\n","      <td>0.320000</td>\n","      <td>0.390000</td>\n","      <td>9.900000</td>\n","      <td>0.050000</td>\n","      <td>46.000000</td>\n","      <td>167.000000</td>\n","      <td>0.996100</td>\n","      <td>3.280000</td>\n","      <td>0.550000</td>\n","      <td>11.400000</td>\n","      <td>6.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>14.200000</td>\n","      <td>1.100000</td>\n","      <td>1.660000</td>\n","      <td>65.800000</td>\n","      <td>0.346000</td>\n","      <td>289.000000</td>\n","      <td>440.000000</td>\n","      <td>1.038980</td>\n","      <td>3.820000</td>\n","      <td>1.080000</td>\n","      <td>14.200000</td>\n","      <td>9.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       fixed_acidity  volatile_acidity  ...      alcohol      quality\n","count    4898.000000       4898.000000  ...  4898.000000  4898.000000\n","mean        6.854788          0.278241  ...    10.514267     5.877909\n","std         0.843868          0.100795  ...     1.230621     0.885639\n","min         3.800000          0.080000  ...     8.000000     3.000000\n","25%         6.300000          0.210000  ...     9.500000     5.000000\n","50%         6.800000          0.260000  ...    10.400000     6.000000\n","75%         7.300000          0.320000  ...    11.400000     6.000000\n","max        14.200000          1.100000  ...    14.200000     9.000000\n","\n","[8 rows x 12 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"5YhuZ8N4A_N2"},"source":["Because there are lots of features (more than 2) it makes no sense to plot this data on a graph - as humans we find it quite hard to visualize more than 3 dimensions (and each feature is a dimension).\n","\n","You should see above that there are different ranges for the feature values. We want to NORMALIZE this data, to make learning easier for the algorithm, and so that we can realistically compare things like coefficient values.\n","\n","We're going to use the sklearn.preprocessing library to help us. I've done most of the work, you just need to extract the X_values and the y_values from the data. I always print out the shape of my data as a sanity check that I haven't lost anything.\n","\n","(k) Extract the wine values from the wine data frame, and then slice out the X_values and y_values from the numpy array of values. Check that the number of instances and features matches what you expect looking at the output in the cell above.\n","\n","Then see how I apply the minmax scaler below. The transform function works on my data to perform normalization, and I can apply it to any new incoming data. But ONLY if that data has the same number of features as the data I used here! You can read more at:\n","\n","- [MinMax Scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)\n"]},{"cell_type":"code","metadata":{"id":"hOb0kh1RBRyx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622146138442,"user_tz":240,"elapsed":5,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"7da59b2d-ca47-44d3-e178-825726a9055f"},"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","# Extract the values from the pandas dataframe\n","\n","\n","wine_value = wine_data.values\n","\n","X_values = wine_value[:,:-1]\n","y_values = wine_value[:,len(wine_value[0])-1]\n","\n","# Get the shape, and print meaningful things\n","# Make sure this agrees with the decribe method, above\n","\n","rows,cols = X_values.shape\n","print(\"This is the wine data set. It has\", rows, \"instances, and it has\", cols, \"input features.\\n\")\n","print(\"The first FIVE instances look like:\")\n","\n","# Show the first five instances\n","print(X_values[:5])\n","print()\n","\n","# Load and fit the scaler\n","\n","scaler = MinMaxScaler()\n","scaler.fit(X_values)\n","\n","# Use some attributes of the scaler to show min and max values per feature\n","# Note these should align with the information from the pandas .describe\n","# method, used above\n","\n","print(\"MAX values:\",scaler.data_max_)\n","print(\"MIN values:\",scaler.data_min_)\n","print()\n","\n","# Transform our X_values, so that data is now scaled\n","# Note we can apply this transform to any data, including new data\n","# and it will preserve the min and max values given above\n","\n","X_values = scaler.transform(X_values)\n","\n","# Take another look at those first five instances that should now be \n","# normalized\n","\n","print(\"After normalization, the first FIVE instances look like:\")\n","print(X_values[:5])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[7.000e+00 2.700e-01 3.600e-01 2.070e+01 4.500e-02 4.500e+01 1.700e+02\n"," 1.001e+00 3.000e+00 4.500e-01 8.800e+00]\n","This is the wine data set. It has 4898 instances, and it has 11 input features.\n","\n","The first FIVE instances look like:\n","[[7.000e+00 2.700e-01 3.600e-01 2.070e+01 4.500e-02 4.500e+01 1.700e+02\n","  1.001e+00 3.000e+00 4.500e-01 8.800e+00]\n"," [6.300e+00 3.000e-01 3.400e-01 1.600e+00 4.900e-02 1.400e+01 1.320e+02\n","  9.940e-01 3.300e+00 4.900e-01 9.500e+00]\n"," [8.100e+00 2.800e-01 4.000e-01 6.900e+00 5.000e-02 3.000e+01 9.700e+01\n","  9.951e-01 3.260e+00 4.400e-01 1.010e+01]\n"," [7.200e+00 2.300e-01 3.200e-01 8.500e+00 5.800e-02 4.700e+01 1.860e+02\n","  9.956e-01 3.190e+00 4.000e-01 9.900e+00]\n"," [7.200e+00 2.300e-01 3.200e-01 8.500e+00 5.800e-02 4.700e+01 1.860e+02\n","  9.956e-01 3.190e+00 4.000e-01 9.900e+00]]\n","\n","MAX values: [1.42000e+01 1.10000e+00 1.66000e+00 6.58000e+01 3.46000e-01 2.89000e+02\n"," 4.40000e+02 1.03898e+00 3.82000e+00 1.08000e+00 1.42000e+01]\n","MIN values: [3.8     0.08    0.      0.6     0.009   2.      9.      0.98711 2.72\n"," 0.22    8.     ]\n","\n","After normalization, the first FIVE instances look like:\n","[[0.30769231 0.18627451 0.21686747 0.30828221 0.10682493 0.14982578\n","  0.37354988 0.26778485 0.25454545 0.26744186 0.12903226]\n"," [0.24038462 0.21568627 0.20481928 0.01533742 0.11869436 0.04181185\n","  0.28538283 0.13283208 0.52727273 0.31395349 0.24193548]\n"," [0.41346154 0.19607843 0.24096386 0.09662577 0.12166172 0.09756098\n","  0.20417633 0.15403894 0.49090909 0.25581395 0.33870968]\n"," [0.32692308 0.14705882 0.19277108 0.12116564 0.14540059 0.15679443\n","  0.41067285 0.16367843 0.42727273 0.20930233 0.30645161]\n"," [0.32692308 0.14705882 0.19277108 0.12116564 0.14540059 0.15679443\n","  0.41067285 0.16367843 0.42727273 0.20930233 0.30645161]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sB89aiMtFUHQ"},"source":["Now that we've normalized the data, let's try a couple of learning tasks. The class in this data is a numerical value in the range 0-10. Let's first try multivariate linear regression (mlr, because there's more than one input feature), and see if we can predict that value.\n","\n","As usual, you'll need to create X_train and y_train sets, and X_test and y_test sets that are copies of X_values and y_values.\n","\n","(l) Copy over your MLR and SGD code. Train your MLR using the normalized data above. Use 100 epochs, and a learning rate of 0.001. Predict y values. Calculate RMSE. Compare to zeroR RMSE.\n","\n","(m) Those scaled coefficients mean we can compare them to each other. Interpret the results for me. What do they really mean with respect to this data? What IS this data? What are we predicting, and how do those coefficients help us understand?  \n","\n","(n) Print (NICELY) the feature names, and their associated coefficient values that you learned using SGD. Print them side by side please. If you had to pick only three features, which would you pick and why? \n","\n","(o) If you've copied over your code from assignment 3, you should be getting an output from your SGD algorithm at the end of each epoch, including the total error. Make a note of the total training error after 100 epochs of training. Increase the epochs to 500. Estimate what impact this will have on the overall final error of your classifier (the RMSE). Write down the final training error after 500 epochs. Also note the RMSE for both models, one trained for 100 epochs and one trained for 500 epochs. What's the difference in RMSE? "]},{"cell_type":"code","metadata":{"id":"DrHiS1RuNtby","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622146116863,"user_tz":240,"elapsed":10490,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"26019886-6f30-4a91-f8a1-8b6fd4d2298d"},"source":["import math\n","\n","# Add you code for (l) through (o) here\n","X_train = X_values\n","X_test = X_train\n","y_train = y_values\n","y_test = y_train\n","\n","#i)\n","#MLR SGD\n","def predFunc(inputlist, coefficient):\n","  # input: the length of coeficent is one more than the length of inputlist. \n","  #        We assume the coefficient list contains all the coefficients, including b0.\n","  # return: the predicted output value.\n","  predOutput = coefficient[0]\n","  for i in range(len(coefficient)-1):\n","    nextIndex = i+1\n","    predOutput += coefficient[nextIndex]*inputlist[i]\n","  return predOutput\n","\n","def coefficientsSGD(train,test,learning_rate,epochs):\n","  # return: the coefficients\n","  coefficient = [0.0 for j in range(len(train[0])+1)]\n","  for iteration in range(epochs):\n","    Totalerror = 0\n","    for i in range(len(train)):\n","      prediction = predFunc(train[i],coefficient)\n","      error = prediction - test[i]\n","      Totalerror += error**2\n","      coefficient[0] -= learning_rate * error\n","      for index in range(len(coefficient)-1):\n","        coefficient[index+1] -= learning_rate * error * train[i][index]\n","    print(\"epoch number =\", iteration, \"learning rate = \", learning_rate,\n","          \"total error = %.3f\" %Totalerror)\n","  return coefficient\n","\n","def mlr(X_train, y_train, X_test, Learning_rate, Epochs):\n","  coefficient = coefficientsSGD(X_train, y_train, Learning_rate, Epochs)\n","  predictions = []\n","  for instance in X_test:\n","    predictions.append(predFunc(instance, coefficient))\n","  return predictions\n","\n","learning_rate = 0.001\n","epochs = 100\n","mlr_predY = mlr(X_train.tolist(), y_train.tolist(), X_test.tolist(), learning_rate, epochs)\n","mlr_predY = np.array(mlr_predY)\n","\n","#zeroR\n","ZeroR_predict = zeroRR(y_train, X_test)\n","\n","#RMSE\n","mlr_rmse = rmse(y_test,mlr_predY)\n","zr_rmse = rmse(y_test,ZeroR_predict)\n","\n","print('RMSE score for MLR: {:.2f}'.format(mlr_rmse))\n","print('RMSE score for zeroR: {:.2f}'.format(zr_rmse))\n","\n","#n)\n","coefficient = coefficientsSGD(X_train, y_train, learning_rate, epochs)\n","fetures_name = ['INTERCEPT','fixed acidity','volatile acidity','citric acid','residual sugar','chlorides','free sulfur dioxide','total sulfur dioxide'\n","                        ,'density','pH','sulphates','alcohol']\n","\n","for i in range(len(coefficient)):\n","  data = '%.3f'%coefficient[i]\n","  print('{:^20}'.format(fetures_name[i]), \":\", '%10s'%data)\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch number = 0 learning rate =  0.001 total error = 13411.349\n","epoch number = 1 learning rate =  0.001 total error = 3531.277\n","epoch number = 2 learning rate =  0.001 total error = 3435.699\n","epoch number = 3 learning rate =  0.001 total error = 3362.113\n","epoch number = 4 learning rate =  0.001 total error = 3303.830\n","epoch number = 5 learning rate =  0.001 total error = 3256.379\n","epoch number = 6 learning rate =  0.001 total error = 3216.764\n","epoch number = 7 learning rate =  0.001 total error = 3182.954\n","epoch number = 8 learning rate =  0.001 total error = 3153.558\n","epoch number = 9 learning rate =  0.001 total error = 3127.608\n","epoch number = 10 learning rate =  0.001 total error = 3104.418\n","epoch number = 11 learning rate =  0.001 total error = 3083.493\n","epoch number = 12 learning rate =  0.001 total error = 3064.465\n","epoch number = 13 learning rate =  0.001 total error = 3047.058\n","epoch number = 14 learning rate =  0.001 total error = 3031.056\n","epoch number = 15 learning rate =  0.001 total error = 3016.288\n","epoch number = 16 learning rate =  0.001 total error = 3002.616\n","epoch number = 17 learning rate =  0.001 total error = 2989.924\n","epoch number = 18 learning rate =  0.001 total error = 2978.115\n","epoch number = 19 learning rate =  0.001 total error = 2967.107\n","epoch number = 20 learning rate =  0.001 total error = 2956.827\n","epoch number = 21 learning rate =  0.001 total error = 2947.213\n","epoch number = 22 learning rate =  0.001 total error = 2938.208\n","epoch number = 23 learning rate =  0.001 total error = 2929.764\n","epoch number = 24 learning rate =  0.001 total error = 2921.836\n","epoch number = 25 learning rate =  0.001 total error = 2914.385\n","epoch number = 26 learning rate =  0.001 total error = 2907.374\n","epoch number = 27 learning rate =  0.001 total error = 2900.771\n","epoch number = 28 learning rate =  0.001 total error = 2894.547\n","epoch number = 29 learning rate =  0.001 total error = 2888.674\n","epoch number = 30 learning rate =  0.001 total error = 2883.129\n","epoch number = 31 learning rate =  0.001 total error = 2877.889\n","epoch number = 32 learning rate =  0.001 total error = 2872.934\n","epoch number = 33 learning rate =  0.001 total error = 2868.244\n","epoch number = 34 learning rate =  0.001 total error = 2863.803\n","epoch number = 35 learning rate =  0.001 total error = 2859.594\n","epoch number = 36 learning rate =  0.001 total error = 2855.604\n","epoch number = 37 learning rate =  0.001 total error = 2851.817\n","epoch number = 38 learning rate =  0.001 total error = 2848.222\n","epoch number = 39 learning rate =  0.001 total error = 2844.808\n","epoch number = 40 learning rate =  0.001 total error = 2841.563\n","epoch number = 41 learning rate =  0.001 total error = 2838.477\n","epoch number = 42 learning rate =  0.001 total error = 2835.542\n","epoch number = 43 learning rate =  0.001 total error = 2832.749\n","epoch number = 44 learning rate =  0.001 total error = 2830.089\n","epoch number = 45 learning rate =  0.001 total error = 2827.555\n","epoch number = 46 learning rate =  0.001 total error = 2825.140\n","epoch number = 47 learning rate =  0.001 total error = 2822.839\n","epoch number = 48 learning rate =  0.001 total error = 2820.644\n","epoch number = 49 learning rate =  0.001 total error = 2818.549\n","epoch number = 50 learning rate =  0.001 total error = 2816.551\n","epoch number = 51 learning rate =  0.001 total error = 2814.643\n","epoch number = 52 learning rate =  0.001 total error = 2812.821\n","epoch number = 53 learning rate =  0.001 total error = 2811.081\n","epoch number = 54 learning rate =  0.001 total error = 2809.418\n","epoch number = 55 learning rate =  0.001 total error = 2807.829\n","epoch number = 56 learning rate =  0.001 total error = 2806.310\n","epoch number = 57 learning rate =  0.001 total error = 2804.857\n","epoch number = 58 learning rate =  0.001 total error = 2803.467\n","epoch number = 59 learning rate =  0.001 total error = 2802.138\n","epoch number = 60 learning rate =  0.001 total error = 2800.865\n","epoch number = 61 learning rate =  0.001 total error = 2799.648\n","epoch number = 62 learning rate =  0.001 total error = 2798.482\n","epoch number = 63 learning rate =  0.001 total error = 2797.366\n","epoch number = 64 learning rate =  0.001 total error = 2796.297\n","epoch number = 65 learning rate =  0.001 total error = 2795.273\n","epoch number = 66 learning rate =  0.001 total error = 2794.292\n","epoch number = 67 learning rate =  0.001 total error = 2793.352\n","epoch number = 68 learning rate =  0.001 total error = 2792.451\n","epoch number = 69 learning rate =  0.001 total error = 2791.588\n","epoch number = 70 learning rate =  0.001 total error = 2790.760\n","epoch number = 71 learning rate =  0.001 total error = 2789.966\n","epoch number = 72 learning rate =  0.001 total error = 2789.205\n","epoch number = 73 learning rate =  0.001 total error = 2788.475\n","epoch number = 74 learning rate =  0.001 total error = 2787.775\n","epoch number = 75 learning rate =  0.001 total error = 2787.103\n","epoch number = 76 learning rate =  0.001 total error = 2786.459\n","epoch number = 77 learning rate =  0.001 total error = 2785.840\n","epoch number = 78 learning rate =  0.001 total error = 2785.246\n","epoch number = 79 learning rate =  0.001 total error = 2784.676\n","epoch number = 80 learning rate =  0.001 total error = 2784.129\n","epoch number = 81 learning rate =  0.001 total error = 2783.604\n","epoch number = 82 learning rate =  0.001 total error = 2783.099\n","epoch number = 83 learning rate =  0.001 total error = 2782.615\n","epoch number = 84 learning rate =  0.001 total error = 2782.149\n","epoch number = 85 learning rate =  0.001 total error = 2781.702\n","epoch number = 86 learning rate =  0.001 total error = 2781.273\n","epoch number = 87 learning rate =  0.001 total error = 2780.860\n","epoch number = 88 learning rate =  0.001 total error = 2780.464\n","epoch number = 89 learning rate =  0.001 total error = 2780.082\n","epoch number = 90 learning rate =  0.001 total error = 2779.716\n","epoch number = 91 learning rate =  0.001 total error = 2779.364\n","epoch number = 92 learning rate =  0.001 total error = 2779.025\n","epoch number = 93 learning rate =  0.001 total error = 2778.699\n","epoch number = 94 learning rate =  0.001 total error = 2778.386\n","epoch number = 95 learning rate =  0.001 total error = 2778.084\n","epoch number = 96 learning rate =  0.001 total error = 2777.794\n","epoch number = 97 learning rate =  0.001 total error = 2777.515\n","epoch number = 98 learning rate =  0.001 total error = 2777.247\n","epoch number = 99 learning rate =  0.001 total error = 2776.988\n","RMSE score for MLR: 0.76\n","RMSE score for zeroR: 0.89\n","epoch number = 0 learning rate =  0.001 total error = 13411.349\n","epoch number = 1 learning rate =  0.001 total error = 3531.277\n","epoch number = 2 learning rate =  0.001 total error = 3435.699\n","epoch number = 3 learning rate =  0.001 total error = 3362.113\n","epoch number = 4 learning rate =  0.001 total error = 3303.830\n","epoch number = 5 learning rate =  0.001 total error = 3256.379\n","epoch number = 6 learning rate =  0.001 total error = 3216.764\n","epoch number = 7 learning rate =  0.001 total error = 3182.954\n","epoch number = 8 learning rate =  0.001 total error = 3153.558\n","epoch number = 9 learning rate =  0.001 total error = 3127.608\n","epoch number = 10 learning rate =  0.001 total error = 3104.418\n","epoch number = 11 learning rate =  0.001 total error = 3083.493\n","epoch number = 12 learning rate =  0.001 total error = 3064.465\n","epoch number = 13 learning rate =  0.001 total error = 3047.058\n","epoch number = 14 learning rate =  0.001 total error = 3031.056\n","epoch number = 15 learning rate =  0.001 total error = 3016.288\n","epoch number = 16 learning rate =  0.001 total error = 3002.616\n","epoch number = 17 learning rate =  0.001 total error = 2989.924\n","epoch number = 18 learning rate =  0.001 total error = 2978.115\n","epoch number = 19 learning rate =  0.001 total error = 2967.107\n","epoch number = 20 learning rate =  0.001 total error = 2956.827\n","epoch number = 21 learning rate =  0.001 total error = 2947.213\n","epoch number = 22 learning rate =  0.001 total error = 2938.208\n","epoch number = 23 learning rate =  0.001 total error = 2929.764\n","epoch number = 24 learning rate =  0.001 total error = 2921.836\n","epoch number = 25 learning rate =  0.001 total error = 2914.385\n","epoch number = 26 learning rate =  0.001 total error = 2907.374\n","epoch number = 27 learning rate =  0.001 total error = 2900.771\n","epoch number = 28 learning rate =  0.001 total error = 2894.547\n","epoch number = 29 learning rate =  0.001 total error = 2888.674\n","epoch number = 30 learning rate =  0.001 total error = 2883.129\n","epoch number = 31 learning rate =  0.001 total error = 2877.889\n","epoch number = 32 learning rate =  0.001 total error = 2872.934\n","epoch number = 33 learning rate =  0.001 total error = 2868.244\n","epoch number = 34 learning rate =  0.001 total error = 2863.803\n","epoch number = 35 learning rate =  0.001 total error = 2859.594\n","epoch number = 36 learning rate =  0.001 total error = 2855.604\n","epoch number = 37 learning rate =  0.001 total error = 2851.817\n","epoch number = 38 learning rate =  0.001 total error = 2848.222\n","epoch number = 39 learning rate =  0.001 total error = 2844.808\n","epoch number = 40 learning rate =  0.001 total error = 2841.563\n","epoch number = 41 learning rate =  0.001 total error = 2838.477\n","epoch number = 42 learning rate =  0.001 total error = 2835.542\n","epoch number = 43 learning rate =  0.001 total error = 2832.749\n","epoch number = 44 learning rate =  0.001 total error = 2830.089\n","epoch number = 45 learning rate =  0.001 total error = 2827.555\n","epoch number = 46 learning rate =  0.001 total error = 2825.140\n","epoch number = 47 learning rate =  0.001 total error = 2822.839\n","epoch number = 48 learning rate =  0.001 total error = 2820.644\n","epoch number = 49 learning rate =  0.001 total error = 2818.549\n","epoch number = 50 learning rate =  0.001 total error = 2816.551\n","epoch number = 51 learning rate =  0.001 total error = 2814.643\n","epoch number = 52 learning rate =  0.001 total error = 2812.821\n","epoch number = 53 learning rate =  0.001 total error = 2811.081\n","epoch number = 54 learning rate =  0.001 total error = 2809.418\n","epoch number = 55 learning rate =  0.001 total error = 2807.829\n","epoch number = 56 learning rate =  0.001 total error = 2806.310\n","epoch number = 57 learning rate =  0.001 total error = 2804.857\n","epoch number = 58 learning rate =  0.001 total error = 2803.467\n","epoch number = 59 learning rate =  0.001 total error = 2802.138\n","epoch number = 60 learning rate =  0.001 total error = 2800.865\n","epoch number = 61 learning rate =  0.001 total error = 2799.648\n","epoch number = 62 learning rate =  0.001 total error = 2798.482\n","epoch number = 63 learning rate =  0.001 total error = 2797.366\n","epoch number = 64 learning rate =  0.001 total error = 2796.297\n","epoch number = 65 learning rate =  0.001 total error = 2795.273\n","epoch number = 66 learning rate =  0.001 total error = 2794.292\n","epoch number = 67 learning rate =  0.001 total error = 2793.352\n","epoch number = 68 learning rate =  0.001 total error = 2792.451\n","epoch number = 69 learning rate =  0.001 total error = 2791.588\n","epoch number = 70 learning rate =  0.001 total error = 2790.760\n","epoch number = 71 learning rate =  0.001 total error = 2789.966\n","epoch number = 72 learning rate =  0.001 total error = 2789.205\n","epoch number = 73 learning rate =  0.001 total error = 2788.475\n","epoch number = 74 learning rate =  0.001 total error = 2787.775\n","epoch number = 75 learning rate =  0.001 total error = 2787.103\n","epoch number = 76 learning rate =  0.001 total error = 2786.459\n","epoch number = 77 learning rate =  0.001 total error = 2785.840\n","epoch number = 78 learning rate =  0.001 total error = 2785.246\n","epoch number = 79 learning rate =  0.001 total error = 2784.676\n","epoch number = 80 learning rate =  0.001 total error = 2784.129\n","epoch number = 81 learning rate =  0.001 total error = 2783.604\n","epoch number = 82 learning rate =  0.001 total error = 2783.099\n","epoch number = 83 learning rate =  0.001 total error = 2782.615\n","epoch number = 84 learning rate =  0.001 total error = 2782.149\n","epoch number = 85 learning rate =  0.001 total error = 2781.702\n","epoch number = 86 learning rate =  0.001 total error = 2781.273\n","epoch number = 87 learning rate =  0.001 total error = 2780.860\n","epoch number = 88 learning rate =  0.001 total error = 2780.464\n","epoch number = 89 learning rate =  0.001 total error = 2780.082\n","epoch number = 90 learning rate =  0.001 total error = 2779.716\n","epoch number = 91 learning rate =  0.001 total error = 2779.364\n","epoch number = 92 learning rate =  0.001 total error = 2779.025\n","epoch number = 93 learning rate =  0.001 total error = 2778.699\n","epoch number = 94 learning rate =  0.001 total error = 2778.386\n","epoch number = 95 learning rate =  0.001 total error = 2778.084\n","epoch number = 96 learning rate =  0.001 total error = 2777.794\n","epoch number = 97 learning rate =  0.001 total error = 2777.515\n","epoch number = 98 learning rate =  0.001 total error = 2777.247\n","epoch number = 99 learning rate =  0.001 total error = 2776.988\n","     INTERCEPT       :      5.047\n","   fixed acidity     :     -0.566\n","  volatile acidity   :     -1.955\n","    citric acid      :     -0.067\n","   residual sugar    :      1.565\n","     chlorides       :     -0.212\n","free sulfur dioxide  :      1.124\n","total sulfur dioxide :     -0.202\n","      density        :      0.022\n","         pH          :      0.137\n","     sulphates       :      0.358\n","      alcohol        :      2.298\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ulfT6XuVCqhf"},"source":["(m) The coefficients are the data that could help us to know which features would have greater influence. The coefficients would influence our prediction. The bigger magnitude of the coefficient, the greater influence the feature would have on the predicted output.\n"]},{"cell_type":"markdown","metadata":{"id":"CS4LOoQs6BFv"},"source":["(n) If I have to pick 3 coefficient, I would pick volatile acidity, residual sugar, and alcohol becasue these three features have relatively more influences towards the output data. Their absolute values are the three that are higher than the others."]},{"cell_type":"markdown","metadata":{"id":"TWO0z6g885SF"},"source":["(o) The total error for epoch = 100 is 2776.988. The RMSE score for MLR is 0.76, which is smaller than the zeroR 0.89.\n","\n","The total error for epoch = 500 is 2760.047. The RMSE score for MLR is still 0.76.\n","\n","The RMSE doesn't change, but the total error gets smaller by using 500 iterations.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ke2o0sokz2L7"},"source":["### Part 4: Feature Selection\n","\n","Let's see how well you perform at selecting features. I asked you in part (n) above to pick 3 features using the coefficient scores above. When you loaded the data into pandas, you should have named your features something sensible. You should be able to take those string names, and replace them in my code, below. I'll take those string names, and copy ONLY those columns out of the original wine data frame, along with the class value - the last one in the table.\n","\n","(p) You need to rescale the data, because we're taking a copy of the unscaled data, which is annoying. Extract the values from the new data frame, create X and y value arrays, and scale the X values using the same process as above. You need to create a new scaler, because there are now a different number of features.\n","\n","(q) Then call YOUR MLR function on this new, reduced data set, with the learning rate of 0.001 and 100 epochs. Also perform zeroR, and calcualte RMSE for both. \n","\n","(r) Compare the RMSE score for THIS experiment, with the previous one with all the features. What's the difference? What did we learn? Tell me in a text box. ALSO - tell me about this experiment as a whole. Did it seem reasonable? Was using MLR a good idea? Are there alternatives? I want your thoughts and impressions."]},{"cell_type":"code","metadata":{"id":"DNd4WPUsSPB0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622146120007,"user_tz":240,"elapsed":2620,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"d703b1a7-56c4-4bef-e86b-632c9bd58407"},"source":["feature1 = 'volatile_acidity'\n","feature2 = 'residual_sugar'\n","feature3 = 'alcohol'\n","feature4 = 'quality' # This one MUST be the name of your class column\n","\n","reduced_wine = wine_data[[feature1,feature2,feature3,feature4]].copy()\n","\n","#p)\n","# Get the values from the data above, and slice out the X_values and y_values\n","reduced_wine_value = reduced_wine.values\n","\n","X_values = reduced_wine_value[:,:-1]\n","y_values = reduced_wine_value[:,len(reduced_wine_value[0])-1]\n","\n","# Get the shape, and print meaningful things\n","rows,cols = X_values.shape\n","print(\"This is the reduced wine data set. It has\", rows, \"instances, and it has\", cols, \"input features.\\n\")\n","\n","# Call the scaler, then run the experiments\n","scaler = MinMaxScaler()\n","scaler.fit(X_values)\n","\n","print(\"MAX values:\",scaler.data_max_)\n","print(\"MIN values:\",scaler.data_min_)\n","print()\n","\n","# Transform our X_values, so that data is now scaled\n","# Note we can apply this transform to any data, including new data\n","# and it will preserve the min and max values given above\n","\n","X_values = scaler.transform(X_values)\n","\n","# Take another look at those first five instances that should now be \n","# normalized\n","\n","print(\"After normalization, the first FIVE instances look like:\")\n","print(X_values[:5])\n","\n","\n","#q)\n","X_train = X_values\n","X_test = X_train\n","y_train = y_values\n","y_test = y_train\n","\n","prediction_Y = mlr(X_train, y_train, X_test, learning_rate, epochs)\n","prediction_zeroR = zeroRR(y_train, X_test)\n","\n","mlr_rmse2 = rmse(y_test,prediction_Y)\n","zr_rmse2 = rmse(y_test,prediction_zeroR)\n","\n","print('RMSE score for MLR: {:.2f}'.format(mlr_rmse2))\n","print('RMSE score for zeroR: {:.2f}'.format(zr_rmse2))\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is the reduced wine data set. It has 4898 instances, and it has 3 input features.\n","\n","MAX values: [ 1.1 65.8 14.2]\n","MIN values: [0.08 0.6  8.  ]\n","\n","After normalization, the first FIVE instances look like:\n","[[0.18627451 0.30828221 0.12903226]\n"," [0.21568627 0.01533742 0.24193548]\n"," [0.19607843 0.09662577 0.33870968]\n"," [0.14705882 0.12116564 0.30645161]\n"," [0.14705882 0.12116564 0.30645161]]\n","epoch number = 0 learning rate =  0.001 total error = 17827.822\n","epoch number = 1 learning rate =  0.001 total error = 3269.655\n","epoch number = 2 learning rate =  0.001 total error = 3232.696\n","epoch number = 3 learning rate =  0.001 total error = 3200.487\n","epoch number = 4 learning rate =  0.001 total error = 3171.144\n","epoch number = 5 learning rate =  0.001 total error = 3144.373\n","epoch number = 6 learning rate =  0.001 total error = 3119.924\n","epoch number = 7 learning rate =  0.001 total error = 3097.574\n","epoch number = 8 learning rate =  0.001 total error = 3077.127\n","epoch number = 9 learning rate =  0.001 total error = 3058.409\n","epoch number = 10 learning rate =  0.001 total error = 3041.264\n","epoch number = 11 learning rate =  0.001 total error = 3025.551\n","epoch number = 12 learning rate =  0.001 total error = 3011.143\n","epoch number = 13 learning rate =  0.001 total error = 2997.926\n","epoch number = 14 learning rate =  0.001 total error = 2985.796\n","epoch number = 15 learning rate =  0.001 total error = 2974.660\n","epoch number = 16 learning rate =  0.001 total error = 2964.431\n","epoch number = 17 learning rate =  0.001 total error = 2955.032\n","epoch number = 18 learning rate =  0.001 total error = 2946.392\n","epoch number = 19 learning rate =  0.001 total error = 2938.447\n","epoch number = 20 learning rate =  0.001 total error = 2931.139\n","epoch number = 21 learning rate =  0.001 total error = 2924.412\n","epoch number = 22 learning rate =  0.001 total error = 2918.219\n","epoch number = 23 learning rate =  0.001 total error = 2912.515\n","epoch number = 24 learning rate =  0.001 total error = 2907.259\n","epoch number = 25 learning rate =  0.001 total error = 2902.413\n","epoch number = 26 learning rate =  0.001 total error = 2897.945\n","epoch number = 27 learning rate =  0.001 total error = 2893.821\n","epoch number = 28 learning rate =  0.001 total error = 2890.015\n","epoch number = 29 learning rate =  0.001 total error = 2886.500\n","epoch number = 30 learning rate =  0.001 total error = 2883.252\n","epoch number = 31 learning rate =  0.001 total error = 2880.250\n","epoch number = 32 learning rate =  0.001 total error = 2877.473\n","epoch number = 33 learning rate =  0.001 total error = 2874.903\n","epoch number = 34 learning rate =  0.001 total error = 2872.525\n","epoch number = 35 learning rate =  0.001 total error = 2870.321\n","epoch number = 36 learning rate =  0.001 total error = 2868.279\n","epoch number = 37 learning rate =  0.001 total error = 2866.385\n","epoch number = 38 learning rate =  0.001 total error = 2864.628\n","epoch number = 39 learning rate =  0.001 total error = 2862.996\n","epoch number = 40 learning rate =  0.001 total error = 2861.481\n","epoch number = 41 learning rate =  0.001 total error = 2860.073\n","epoch number = 42 learning rate =  0.001 total error = 2858.764\n","epoch number = 43 learning rate =  0.001 total error = 2857.545\n","epoch number = 44 learning rate =  0.001 total error = 2856.411\n","epoch number = 45 learning rate =  0.001 total error = 2855.354\n","epoch number = 46 learning rate =  0.001 total error = 2854.369\n","epoch number = 47 learning rate =  0.001 total error = 2853.451\n","epoch number = 48 learning rate =  0.001 total error = 2852.594\n","epoch number = 49 learning rate =  0.001 total error = 2851.793\n","epoch number = 50 learning rate =  0.001 total error = 2851.045\n","epoch number = 51 learning rate =  0.001 total error = 2850.346\n","epoch number = 52 learning rate =  0.001 total error = 2849.692\n","epoch number = 53 learning rate =  0.001 total error = 2849.080\n","epoch number = 54 learning rate =  0.001 total error = 2848.506\n","epoch number = 55 learning rate =  0.001 total error = 2847.969\n","epoch number = 56 learning rate =  0.001 total error = 2847.465\n","epoch number = 57 learning rate =  0.001 total error = 2846.993\n","epoch number = 58 learning rate =  0.001 total error = 2846.549\n","epoch number = 59 learning rate =  0.001 total error = 2846.132\n","epoch number = 60 learning rate =  0.001 total error = 2845.741\n","epoch number = 61 learning rate =  0.001 total error = 2845.372\n","epoch number = 62 learning rate =  0.001 total error = 2845.026\n","epoch number = 63 learning rate =  0.001 total error = 2844.700\n","epoch number = 64 learning rate =  0.001 total error = 2844.392\n","epoch number = 65 learning rate =  0.001 total error = 2844.103\n","epoch number = 66 learning rate =  0.001 total error = 2843.830\n","epoch number = 67 learning rate =  0.001 total error = 2843.572\n","epoch number = 68 learning rate =  0.001 total error = 2843.329\n","epoch number = 69 learning rate =  0.001 total error = 2843.100\n","epoch number = 70 learning rate =  0.001 total error = 2842.883\n","epoch number = 71 learning rate =  0.001 total error = 2842.678\n","epoch number = 72 learning rate =  0.001 total error = 2842.485\n","epoch number = 73 learning rate =  0.001 total error = 2842.301\n","epoch number = 74 learning rate =  0.001 total error = 2842.128\n","epoch number = 75 learning rate =  0.001 total error = 2841.964\n","epoch number = 76 learning rate =  0.001 total error = 2841.808\n","epoch number = 77 learning rate =  0.001 total error = 2841.661\n","epoch number = 78 learning rate =  0.001 total error = 2841.521\n","epoch number = 79 learning rate =  0.001 total error = 2841.389\n","epoch number = 80 learning rate =  0.001 total error = 2841.263\n","epoch number = 81 learning rate =  0.001 total error = 2841.144\n","epoch number = 82 learning rate =  0.001 total error = 2841.031\n","epoch number = 83 learning rate =  0.001 total error = 2840.923\n","epoch number = 84 learning rate =  0.001 total error = 2840.821\n","epoch number = 85 learning rate =  0.001 total error = 2840.724\n","epoch number = 86 learning rate =  0.001 total error = 2840.632\n","epoch number = 87 learning rate =  0.001 total error = 2840.545\n","epoch number = 88 learning rate =  0.001 total error = 2840.461\n","epoch number = 89 learning rate =  0.001 total error = 2840.382\n","epoch number = 90 learning rate =  0.001 total error = 2840.307\n","epoch number = 91 learning rate =  0.001 total error = 2840.235\n","epoch number = 92 learning rate =  0.001 total error = 2840.167\n","epoch number = 93 learning rate =  0.001 total error = 2840.102\n","epoch number = 94 learning rate =  0.001 total error = 2840.040\n","epoch number = 95 learning rate =  0.001 total error = 2839.981\n","epoch number = 96 learning rate =  0.001 total error = 2839.925\n","epoch number = 97 learning rate =  0.001 total error = 2839.871\n","epoch number = 98 learning rate =  0.001 total error = 2839.820\n","epoch number = 99 learning rate =  0.001 total error = 2839.772\n","RMSE score for MLR: 0.77\n","RMSE score for zeroR: 0.89\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"yj7ON7OGDznE"},"source":["The RMSE score for these three chosen features is 0.77, which is very similar to the RMSE for 11 features(0.76). These shows that the real dependent features are these three features. Theses three features are the features that really influence the predictions. This indeed seems reasonable because the coefficients of these features also have greater absolute value than the others', which illustrates that these features are the dominant features. \n","\n","I think MLR did a good job in this because MLR is doing predictions based on multiple variables(features). More importantly, the RMSE score of MLR is lower than the RMSE score of the baseline, which is pretty cool."]},{"cell_type":"markdown","metadata":{"id":"or4LLCn1RYyK"},"source":["Missing interpretation: The average error for each level of quality for wine is rising by just 0.01 point. [We have to interpret what the data is doing respect to what does the data means.!!!]"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"JFLspS-1SlJr","executionInfo":{"status":"ok","timestamp":1622146120565,"user_tz":240,"elapsed":561,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"27d736fc-7f79-4e94-a051-8ede63531a19"},"source":["vals_df = reduced_wine['quality'].copy()\n","vals_df.hist()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fcf9949d690>"]},"metadata":{"tags":[]},"execution_count":13},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"SFzs_cYzSk1l"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"PrBveDYL8zxK"},"source":["### Part 5: Evaluation\n","\n","In this final section, we'll do the following things. We're going to work with an actual diabetes data set. You can find more about this data set here: https://www.kaggle.com/uciml/pima-indians-diabetes-database\n","\n","(s) You are going to:\n","\n","- load the data into a pandas dataframe\n","- print out some basic information about the data (number of instances, features), and do it nicely\n","- Slice you data into X_values and y_values (**USE THESE NAMES!**)\n","- normalize all columns EXCEPT the class, we don't usually normalize the class column, in the range 0-1\n","- The class column contains two values, 0 and 1. You should understand what those represent from the information about the data set. This is a classification task\n","- Use YOUR logistic regression code, with a learning rate of 0.1, and 100 epochs\n","- Use your zeroRC code\n","- Calculate accuracy for both, and print\n","- Tell me something about the results. Using the coefficient values might help.\n","\n","Feel free to cut this up as you want. Add extra cells, or format the problem in a way that makes sense to you. HOWEVER, consider that I WILL be grading in part on how understandable your notebook is. BE MINDFUL of my advice at the beginning of this notebook. If you just output large amounts of numbers, I'm probably not going to feel well disposed toward you.\n","\n","You're going to be looking at the following functions:\n","- [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n","- [KFold selection](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold)\n","- [Stratified KFold selection](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)\n","\n"]},{"cell_type":"code","metadata":{"id":"lIbx8RpD1Wwo","colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"status":"ok","timestamp":1622146120566,"user_tz":240,"elapsed":22,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"a0f70554-390d-402e-d316-4a422f9e5ce8"},"source":["# Write your code here\n","\n","import pandas as pd\n","\n","filename = 'https://raw.githubusercontent.com/nixwebb/CSV_Data/master/pima-indians-diabetes.csv'\n","column_name = ['Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigree','Age','Outcome']\n","diabetes_data = pd.read_csv(filename,names = column_name)\n","diabetes_data.head()\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigree</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6</td>\n","      <td>148</td>\n","      <td>72</td>\n","      <td>35</td>\n","      <td>0</td>\n","      <td>33.6</td>\n","      <td>0.627</td>\n","      <td>50</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>85</td>\n","      <td>66</td>\n","      <td>29</td>\n","      <td>0</td>\n","      <td>26.6</td>\n","      <td>0.351</td>\n","      <td>31</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>8</td>\n","      <td>183</td>\n","      <td>64</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>23.3</td>\n","      <td>0.672</td>\n","      <td>32</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>89</td>\n","      <td>66</td>\n","      <td>23</td>\n","      <td>94</td>\n","      <td>28.1</td>\n","      <td>0.167</td>\n","      <td>21</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>137</td>\n","      <td>40</td>\n","      <td>35</td>\n","      <td>168</td>\n","      <td>43.1</td>\n","      <td>2.288</td>\n","      <td>33</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Pregnancies  Glucose  BloodPressure  ...  DiabetesPedigree  Age  Outcome\n","0            6      148             72  ...             0.627   50        1\n","1            1       85             66  ...             0.351   31        0\n","2            8      183             64  ...             0.672   32        1\n","3            1       89             66  ...             0.167   21        0\n","4            0      137             40  ...             2.288   33        1\n","\n","[5 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":589},"id":"QhMzcScV9ApM","executionInfo":{"status":"ok","timestamp":1622146120567,"user_tz":240,"elapsed":21,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"35769335-536c-4c3d-92f7-f62e103da480"},"source":["diabetes_data.info()\n","diabetes_data.describe()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 768 entries, 0 to 767\n","Data columns (total 9 columns):\n"," #   Column            Non-Null Count  Dtype  \n","---  ------            --------------  -----  \n"," 0   Pregnancies       768 non-null    int64  \n"," 1   Glucose           768 non-null    int64  \n"," 2   BloodPressure     768 non-null    int64  \n"," 3   SkinThickness     768 non-null    int64  \n"," 4   Insulin           768 non-null    int64  \n"," 5   BMI               768 non-null    float64\n"," 6   DiabetesPedigree  768 non-null    float64\n"," 7   Age               768 non-null    int64  \n"," 8   Outcome           768 non-null    int64  \n","dtypes: float64(2), int64(7)\n","memory usage: 54.1 KB\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pregnancies</th>\n","      <th>Glucose</th>\n","      <th>BloodPressure</th>\n","      <th>SkinThickness</th>\n","      <th>Insulin</th>\n","      <th>BMI</th>\n","      <th>DiabetesPedigree</th>\n","      <th>Age</th>\n","      <th>Outcome</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","      <td>768.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>3.845052</td>\n","      <td>120.894531</td>\n","      <td>69.105469</td>\n","      <td>20.536458</td>\n","      <td>79.799479</td>\n","      <td>31.992578</td>\n","      <td>0.471876</td>\n","      <td>33.240885</td>\n","      <td>0.348958</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>3.369578</td>\n","      <td>31.972618</td>\n","      <td>19.355807</td>\n","      <td>15.952218</td>\n","      <td>115.244002</td>\n","      <td>7.884160</td>\n","      <td>0.331329</td>\n","      <td>11.760232</td>\n","      <td>0.476951</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.078000</td>\n","      <td>21.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>1.000000</td>\n","      <td>99.000000</td>\n","      <td>62.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>27.300000</td>\n","      <td>0.243750</td>\n","      <td>24.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>3.000000</td>\n","      <td>117.000000</td>\n","      <td>72.000000</td>\n","      <td>23.000000</td>\n","      <td>30.500000</td>\n","      <td>32.000000</td>\n","      <td>0.372500</td>\n","      <td>29.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>6.000000</td>\n","      <td>140.250000</td>\n","      <td>80.000000</td>\n","      <td>32.000000</td>\n","      <td>127.250000</td>\n","      <td>36.600000</td>\n","      <td>0.626250</td>\n","      <td>41.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>17.000000</td>\n","      <td>199.000000</td>\n","      <td>122.000000</td>\n","      <td>99.000000</td>\n","      <td>846.000000</td>\n","      <td>67.100000</td>\n","      <td>2.420000</td>\n","      <td>81.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Pregnancies     Glucose  ...         Age     Outcome\n","count   768.000000  768.000000  ...  768.000000  768.000000\n","mean      3.845052  120.894531  ...   33.240885    0.348958\n","std       3.369578   31.972618  ...   11.760232    0.476951\n","min       0.000000    0.000000  ...   21.000000    0.000000\n","25%       1.000000   99.000000  ...   24.000000    0.000000\n","50%       3.000000  117.000000  ...   29.000000    0.000000\n","75%       6.000000  140.250000  ...   41.000000    1.000000\n","max      17.000000  199.000000  ...   81.000000    1.000000\n","\n","[8 rows x 9 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_fjSLgYvHB_Q","executionInfo":{"status":"ok","timestamp":1622146120567,"user_tz":240,"elapsed":17,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"a622ea41-38fa-4e39-8c69-a0d382856ee6"},"source":["#print info about data \n","from sklearn.preprocessing import MinMaxScaler\n","diabetes_values = diabetes_data.values\n","X_values = diabetes_values[:,:-1]\n","y_values = diabetes_values[:,len(diabetes_values[0])-1]\n","\n","rows,cols = X_values.shape\n","print(\"This is the wine data set. It has\", rows, \"instances, and it has\", cols, \"input features.\\n\")\n","\n","\n","#normalize \n","print(\"The first FIVE instances look like:\")\n","\n","# Show the first five instances\n","print(X_values[:5])\n","print()\n","\n","# Load and fit the scaler\n","\n","scaler = MinMaxScaler()\n","scaler.fit(X_values)\n","\n","# Use some attributes of the scaler to show min and max values per feature\n","# Note these should align with the information from the pandas .describe\n","# method, used above\n","\n","print(\"MAX values:\",scaler.data_max_)\n","print(\"MIN values:\",scaler.data_min_)\n","print()\n","\n","# Transform our X_values, so that data is now scaled\n","# Note we can apply this transform to any data, including new data\n","# and it will preserve the min and max values given above\n","\n","X_values = scaler.transform(X_values)\n","\n","# Take another look at those first five instances that should now be \n","# normalized\n","\n","print(\"After normalization, the first FIVE instances look like:\")\n","print(X_values[:5])\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is the wine data set. It has 768 instances, and it has 8 input features.\n","\n","The first FIVE instances look like:\n","[[6.000e+00 1.480e+02 7.200e+01 3.500e+01 0.000e+00 3.360e+01 6.270e-01\n","  5.000e+01]\n"," [1.000e+00 8.500e+01 6.600e+01 2.900e+01 0.000e+00 2.660e+01 3.510e-01\n","  3.100e+01]\n"," [8.000e+00 1.830e+02 6.400e+01 0.000e+00 0.000e+00 2.330e+01 6.720e-01\n","  3.200e+01]\n"," [1.000e+00 8.900e+01 6.600e+01 2.300e+01 9.400e+01 2.810e+01 1.670e-01\n","  2.100e+01]\n"," [0.000e+00 1.370e+02 4.000e+01 3.500e+01 1.680e+02 4.310e+01 2.288e+00\n","  3.300e+01]]\n","\n","MAX values: [ 17.   199.   122.    99.   846.    67.1    2.42  81.  ]\n","MIN values: [ 0.     0.     0.     0.     0.     0.     0.078 21.   ]\n","\n","After normalization, the first FIVE instances look like:\n","[[0.35294118 0.74371859 0.59016393 0.35353535 0.         0.50074516\n","  0.23441503 0.48333333]\n"," [0.05882353 0.42713568 0.54098361 0.29292929 0.         0.39642325\n","  0.11656704 0.16666667]\n"," [0.47058824 0.91959799 0.52459016 0.         0.         0.34724292\n","  0.25362938 0.18333333]\n"," [0.05882353 0.44723618 0.54098361 0.23232323 0.11111111 0.41877794\n","  0.03800171 0.        ]\n"," [0.         0.68844221 0.32786885 0.35353535 0.19858156 0.64232489\n","  0.94363792 0.2       ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCVt0h7E8-QT","executionInfo":{"status":"ok","timestamp":1622146122118,"user_tz":240,"elapsed":1566,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"46aaae73-7d96-46fa-f5db-516732c1620e"},"source":["# Logestic Regression\n","import math\n","X_train = X_values\n","X_test = X_train\n","y_train = y_values\n","y_test = y_train\n","\n","def pred(instance, coefficient):\n","  #return: a prediction\n","  exponent = coefficient[0]\n","  for i in range(len(coefficient)-1):\n","    exponent += coefficient[i+1]*instance[i]\n","  predY = 1.0/(1.0 + math.exp(-exponent))\n","  return predY\n","\n","def sgd_log(X_train, y_train, learning_rate, epochs):\n","  # return the coefficients.\n","  coefficient = [0.0 for j in range(len(X_train[0])+1)]\n","  for iteration in range(epochs):\n","    Totalerror = 0\n","    for i in range(len(X_train)):\n","      preditedY = pred(X_train[i],coefficient)\n","      error = y_train[i] - preditedY\n","      Totalerror += error**2\n","      coefficient[0] += learning_rate * error * preditedY*(1.0-preditedY)\n","      for index in range(len(coefficient)-1):\n","        coefficient[index+1] += learning_rate * error *preditedY*(1.0-preditedY)* X_train[i][index]\n","    print(\">epoch=\", iteration, \"lrate=\", learning_rate,\n","          \"error=%.3f\" %Totalerror)\n","  return coefficient\n","\n","def log_reg(X_train, y_train, X_test, learning_rate, epochs):\n","  # return prediction\n","  coefficient = sgd_log(X_train, y_train, learning_rate, epochs)\n","  print(\"Coefficient is: \", coefficient)\n","  predictions = []\n","  for i in range(len(X_test)):\n","    predictY = pred(X_test[i],coefficient)\n","    predictions.append(round(predictY))\n","  return predictions\n","\n","Learning_rate = 0.1\n","Epochs = 100\n","log_predectY = log_reg(X_train, y_train, X_test, Learning_rate, Epochs)\n","\n","#ZeroRC\n","def zeroRC(train, test):\n","  # return baseline\n","  commonElement = max(set(train), key = train.count)\n","  return [commonElement for i in range(len(test))]\n","\n","zeroRC_prediction = zeroRC(y_train.tolist(), X_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":[">epoch= 0 lrate= 0.1 error=174.355\n",">epoch= 1 lrate= 0.1 error=165.089\n",">epoch= 2 lrate= 0.1 error=158.077\n",">epoch= 3 lrate= 0.1 error=152.701\n",">epoch= 4 lrate= 0.1 error=148.490\n",">epoch= 5 lrate= 0.1 error=145.113\n",">epoch= 6 lrate= 0.1 error=142.346\n",">epoch= 7 lrate= 0.1 error=140.036\n",">epoch= 8 lrate= 0.1 error=138.078\n",">epoch= 9 lrate= 0.1 error=136.396\n",">epoch= 10 lrate= 0.1 error=134.935\n",">epoch= 11 lrate= 0.1 error=133.656\n",">epoch= 12 lrate= 0.1 error=132.526\n",">epoch= 13 lrate= 0.1 error=131.520\n",">epoch= 14 lrate= 0.1 error=130.620\n",">epoch= 15 lrate= 0.1 error=129.810\n",">epoch= 16 lrate= 0.1 error=129.077\n",">epoch= 17 lrate= 0.1 error=128.411\n",">epoch= 18 lrate= 0.1 error=127.803\n",">epoch= 19 lrate= 0.1 error=127.247\n",">epoch= 20 lrate= 0.1 error=126.735\n",">epoch= 21 lrate= 0.1 error=126.264\n",">epoch= 22 lrate= 0.1 error=125.829\n",">epoch= 23 lrate= 0.1 error=125.425\n",">epoch= 24 lrate= 0.1 error=125.050\n",">epoch= 25 lrate= 0.1 error=124.701\n",">epoch= 26 lrate= 0.1 error=124.376\n",">epoch= 27 lrate= 0.1 error=124.071\n",">epoch= 28 lrate= 0.1 error=123.786\n",">epoch= 29 lrate= 0.1 error=123.519\n",">epoch= 30 lrate= 0.1 error=123.268\n",">epoch= 31 lrate= 0.1 error=123.032\n",">epoch= 32 lrate= 0.1 error=122.810\n",">epoch= 33 lrate= 0.1 error=122.600\n",">epoch= 34 lrate= 0.1 error=122.402\n",">epoch= 35 lrate= 0.1 error=122.215\n",">epoch= 36 lrate= 0.1 error=122.037\n",">epoch= 37 lrate= 0.1 error=121.869\n",">epoch= 38 lrate= 0.1 error=121.710\n",">epoch= 39 lrate= 0.1 error=121.559\n",">epoch= 40 lrate= 0.1 error=121.415\n",">epoch= 41 lrate= 0.1 error=121.279\n",">epoch= 42 lrate= 0.1 error=121.149\n",">epoch= 43 lrate= 0.1 error=121.025\n",">epoch= 44 lrate= 0.1 error=120.907\n",">epoch= 45 lrate= 0.1 error=120.795\n",">epoch= 46 lrate= 0.1 error=120.687\n",">epoch= 47 lrate= 0.1 error=120.585\n",">epoch= 48 lrate= 0.1 error=120.487\n",">epoch= 49 lrate= 0.1 error=120.393\n",">epoch= 50 lrate= 0.1 error=120.304\n",">epoch= 51 lrate= 0.1 error=120.218\n",">epoch= 52 lrate= 0.1 error=120.136\n",">epoch= 53 lrate= 0.1 error=120.057\n",">epoch= 54 lrate= 0.1 error=119.982\n",">epoch= 55 lrate= 0.1 error=119.909\n",">epoch= 56 lrate= 0.1 error=119.840\n",">epoch= 57 lrate= 0.1 error=119.773\n",">epoch= 58 lrate= 0.1 error=119.709\n",">epoch= 59 lrate= 0.1 error=119.647\n",">epoch= 60 lrate= 0.1 error=119.588\n",">epoch= 61 lrate= 0.1 error=119.531\n",">epoch= 62 lrate= 0.1 error=119.476\n",">epoch= 63 lrate= 0.1 error=119.424\n",">epoch= 64 lrate= 0.1 error=119.373\n",">epoch= 65 lrate= 0.1 error=119.324\n",">epoch= 66 lrate= 0.1 error=119.277\n",">epoch= 67 lrate= 0.1 error=119.231\n",">epoch= 68 lrate= 0.1 error=119.188\n",">epoch= 69 lrate= 0.1 error=119.145\n",">epoch= 70 lrate= 0.1 error=119.105\n",">epoch= 71 lrate= 0.1 error=119.065\n",">epoch= 72 lrate= 0.1 error=119.027\n",">epoch= 73 lrate= 0.1 error=118.990\n",">epoch= 74 lrate= 0.1 error=118.955\n",">epoch= 75 lrate= 0.1 error=118.921\n",">epoch= 76 lrate= 0.1 error=118.888\n",">epoch= 77 lrate= 0.1 error=118.856\n",">epoch= 78 lrate= 0.1 error=118.825\n",">epoch= 79 lrate= 0.1 error=118.795\n",">epoch= 80 lrate= 0.1 error=118.766\n",">epoch= 81 lrate= 0.1 error=118.738\n",">epoch= 82 lrate= 0.1 error=118.710\n",">epoch= 83 lrate= 0.1 error=118.684\n",">epoch= 84 lrate= 0.1 error=118.659\n",">epoch= 85 lrate= 0.1 error=118.634\n",">epoch= 86 lrate= 0.1 error=118.610\n",">epoch= 87 lrate= 0.1 error=118.587\n",">epoch= 88 lrate= 0.1 error=118.564\n",">epoch= 89 lrate= 0.1 error=118.542\n",">epoch= 90 lrate= 0.1 error=118.521\n",">epoch= 91 lrate= 0.1 error=118.501\n",">epoch= 92 lrate= 0.1 error=118.481\n",">epoch= 93 lrate= 0.1 error=118.461\n",">epoch= 94 lrate= 0.1 error=118.443\n",">epoch= 95 lrate= 0.1 error=118.425\n",">epoch= 96 lrate= 0.1 error=118.407\n",">epoch= 97 lrate= 0.1 error=118.390\n",">epoch= 98 lrate= 0.1 error=118.373\n",">epoch= 99 lrate= 0.1 error=118.357\n","Coefficient is:  [-6.959040786301785, 1.8844553619549993, 6.566576209561069, -1.3661695567393184, -0.17169389617038247, -0.4709004582581407, 4.586682677514912, 2.3668855845472203, 0.5844757060700194]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dw2RCFf0G3M8","executionInfo":{"status":"ok","timestamp":1622146123708,"user_tz":240,"elapsed":1605,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"b83fa75f-058d-4680-ab4b-3bf53ccf67fb"},"source":["# accuracy for both\n","def accuracy(actual_value, predicted_value):\n","  # return how many times the function predicts correctly in ratio.\n","  counter = 0 \n","  for i in range(len(actual_value)):\n","    if actual_value[i] == predicted_value[i]:\n","      counter += 1\n","  return counter/len(actual_value)\n","\n","accuracy_logReg = accuracy(y_train, log_predectY) * 100\n","accuracy_zeroRC = accuracy(y_train, zeroRC_prediction) * 100\n","print(\"Logistic Regression prediction accuracy = %.3f\" %accuracy_logReg,\"%\")\n","print(\"ZeroR prediction accuracy = %.3f\" % accuracy_zeroRC,\"%\")\n","\n","coefficient = sgd_log(X_train, y_train, Learning_rate, Epochs)\n","fetures_name = ['INTERCEPT','Pregnancies','Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigree','Age']\n","\n","for i in range(len(coefficient)):\n","  data = '%.3f'%coefficient[i]\n","  print('{:^20}'.format(fetures_name[i]), \":\", '%10s'%data)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Logistic Regression prediction accuracy = 77.344 %\n","ZeroR prediction accuracy = 65.104 %\n",">epoch= 0 lrate= 0.1 error=174.355\n",">epoch= 1 lrate= 0.1 error=165.089\n",">epoch= 2 lrate= 0.1 error=158.077\n",">epoch= 3 lrate= 0.1 error=152.701\n",">epoch= 4 lrate= 0.1 error=148.490\n",">epoch= 5 lrate= 0.1 error=145.113\n",">epoch= 6 lrate= 0.1 error=142.346\n",">epoch= 7 lrate= 0.1 error=140.036\n",">epoch= 8 lrate= 0.1 error=138.078\n",">epoch= 9 lrate= 0.1 error=136.396\n",">epoch= 10 lrate= 0.1 error=134.935\n",">epoch= 11 lrate= 0.1 error=133.656\n",">epoch= 12 lrate= 0.1 error=132.526\n",">epoch= 13 lrate= 0.1 error=131.520\n",">epoch= 14 lrate= 0.1 error=130.620\n",">epoch= 15 lrate= 0.1 error=129.810\n",">epoch= 16 lrate= 0.1 error=129.077\n",">epoch= 17 lrate= 0.1 error=128.411\n",">epoch= 18 lrate= 0.1 error=127.803\n",">epoch= 19 lrate= 0.1 error=127.247\n",">epoch= 20 lrate= 0.1 error=126.735\n",">epoch= 21 lrate= 0.1 error=126.264\n",">epoch= 22 lrate= 0.1 error=125.829\n",">epoch= 23 lrate= 0.1 error=125.425\n",">epoch= 24 lrate= 0.1 error=125.050\n",">epoch= 25 lrate= 0.1 error=124.701\n",">epoch= 26 lrate= 0.1 error=124.376\n",">epoch= 27 lrate= 0.1 error=124.071\n",">epoch= 28 lrate= 0.1 error=123.786\n",">epoch= 29 lrate= 0.1 error=123.519\n",">epoch= 30 lrate= 0.1 error=123.268\n",">epoch= 31 lrate= 0.1 error=123.032\n",">epoch= 32 lrate= 0.1 error=122.810\n",">epoch= 33 lrate= 0.1 error=122.600\n",">epoch= 34 lrate= 0.1 error=122.402\n",">epoch= 35 lrate= 0.1 error=122.215\n",">epoch= 36 lrate= 0.1 error=122.037\n",">epoch= 37 lrate= 0.1 error=121.869\n",">epoch= 38 lrate= 0.1 error=121.710\n",">epoch= 39 lrate= 0.1 error=121.559\n",">epoch= 40 lrate= 0.1 error=121.415\n",">epoch= 41 lrate= 0.1 error=121.279\n",">epoch= 42 lrate= 0.1 error=121.149\n",">epoch= 43 lrate= 0.1 error=121.025\n",">epoch= 44 lrate= 0.1 error=120.907\n",">epoch= 45 lrate= 0.1 error=120.795\n",">epoch= 46 lrate= 0.1 error=120.687\n",">epoch= 47 lrate= 0.1 error=120.585\n",">epoch= 48 lrate= 0.1 error=120.487\n",">epoch= 49 lrate= 0.1 error=120.393\n",">epoch= 50 lrate= 0.1 error=120.304\n",">epoch= 51 lrate= 0.1 error=120.218\n",">epoch= 52 lrate= 0.1 error=120.136\n",">epoch= 53 lrate= 0.1 error=120.057\n",">epoch= 54 lrate= 0.1 error=119.982\n",">epoch= 55 lrate= 0.1 error=119.909\n",">epoch= 56 lrate= 0.1 error=119.840\n",">epoch= 57 lrate= 0.1 error=119.773\n",">epoch= 58 lrate= 0.1 error=119.709\n",">epoch= 59 lrate= 0.1 error=119.647\n",">epoch= 60 lrate= 0.1 error=119.588\n",">epoch= 61 lrate= 0.1 error=119.531\n",">epoch= 62 lrate= 0.1 error=119.476\n",">epoch= 63 lrate= 0.1 error=119.424\n",">epoch= 64 lrate= 0.1 error=119.373\n",">epoch= 65 lrate= 0.1 error=119.324\n",">epoch= 66 lrate= 0.1 error=119.277\n",">epoch= 67 lrate= 0.1 error=119.231\n",">epoch= 68 lrate= 0.1 error=119.188\n",">epoch= 69 lrate= 0.1 error=119.145\n",">epoch= 70 lrate= 0.1 error=119.105\n",">epoch= 71 lrate= 0.1 error=119.065\n",">epoch= 72 lrate= 0.1 error=119.027\n",">epoch= 73 lrate= 0.1 error=118.990\n",">epoch= 74 lrate= 0.1 error=118.955\n",">epoch= 75 lrate= 0.1 error=118.921\n",">epoch= 76 lrate= 0.1 error=118.888\n",">epoch= 77 lrate= 0.1 error=118.856\n",">epoch= 78 lrate= 0.1 error=118.825\n",">epoch= 79 lrate= 0.1 error=118.795\n",">epoch= 80 lrate= 0.1 error=118.766\n",">epoch= 81 lrate= 0.1 error=118.738\n",">epoch= 82 lrate= 0.1 error=118.710\n",">epoch= 83 lrate= 0.1 error=118.684\n",">epoch= 84 lrate= 0.1 error=118.659\n",">epoch= 85 lrate= 0.1 error=118.634\n",">epoch= 86 lrate= 0.1 error=118.610\n",">epoch= 87 lrate= 0.1 error=118.587\n",">epoch= 88 lrate= 0.1 error=118.564\n",">epoch= 89 lrate= 0.1 error=118.542\n",">epoch= 90 lrate= 0.1 error=118.521\n",">epoch= 91 lrate= 0.1 error=118.501\n",">epoch= 92 lrate= 0.1 error=118.481\n",">epoch= 93 lrate= 0.1 error=118.461\n",">epoch= 94 lrate= 0.1 error=118.443\n",">epoch= 95 lrate= 0.1 error=118.425\n",">epoch= 96 lrate= 0.1 error=118.407\n",">epoch= 97 lrate= 0.1 error=118.390\n",">epoch= 98 lrate= 0.1 error=118.373\n",">epoch= 99 lrate= 0.1 error=118.357\n","     INTERCEPT       :     -6.959\n","    Pregnancies      :      1.884\n","      Glucose        :      6.567\n","   BloodPressure     :     -1.366\n","   SkinThickness     :     -0.172\n","      Insulin        :     -0.471\n","        BMI          :      4.587\n","  DiabetesPedigree   :      2.367\n","        Age          :      0.584\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"--Tacyz8JCjM"},"source":["Analysis: \n","\n","The accuracy of Logistic Regression is 77.344%, which is higher than accuracy of ZeroRC(baseline), which is 65.104%. This means that the Logistic Regression is doing good becuase it behaves better than the baseline. \n","\n","According to the coefficient, we can conclude that the most effective features are: Glucose, BMI, and Diabetes Pedigree. Their coefficient values are the three highest scores among 8 features. Hence, they have the greatest influence towards the predicted ouput. We can use these three features to classify and to make the prediction instead of collecting data of all 8 features.\n","\n","Based on the logistic regression model, we could say that 77.34% that the predicted result of whether one has diabetes is true. "]},{"cell_type":"markdown","metadata":{"id":"qYMdgnmW30rn"},"source":["Ok. But I've said that using the same data for training and testing is a terrible idea. Below, I'm going to cut the data up into FOUR sets, X_train and y_train for training the data, and X_test and y_test for testing the data. If you've defined X_values and y_values above for this data as you should have, then the following will work. It creates training data that comprises 66% of the total data, and test data that is 33% of the data.\n","\n","The random state is simply a seed value for random selection of isntances. We use a known random seed to give us reproducability of our results.\n","\n","(t) Run the code below, adding in calls to your logistic regression and zeroR functions, and calcualte accuracy. This time, when you use X_train, y_train, X_test and y_test data sets, you'll see that they are DISTINCT. Add in comments comparing the accuracy of this experiment to (s), above.\n"]},{"cell_type":"code","metadata":{"id":"rm8Epsob5-Qe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622146124741,"user_tz":240,"elapsed":1045,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"d295b6ce-7f89-4b61-f696-cb924a79f6c7"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_row, X_col = X_values.shape\n","y_row = y_values.shape\n","\n","print('***PRE SELECTION***')\n","print()\n","print('Our original input data (X) is comprised of')\n","print(X_row,'instances and',X_col,'features')\n","print()\n","print('Our original output data (y) is comprised of')\n","print(y_row,'instances')\n","\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_values, y_values, test_size=0.33, random_state=42)\n","\n","X_train_row, X_train_col = X_train.shape\n","y_train_row = y_train.shape\n","\n","X_test_row, X_test_col = X_test.shape\n","y_test_row = y_test.shape\n","\n","print()\n","print('***SLICING the data into TRAIN and TEST data***')\n","print()\n","print(\"TRAINING DATA:\")\n","print('\\tOur training data input (X_train) is comprised of')\n","print('\\t',X_train_row,'instances and',X_train_col,'features')\n","print('\\tOur trainging output (y_train) is comprised of')\n","print('\\t',y_train_row,'instances')\n","\n","print(\"TESTING DATA:\")\n","print('\\tOur testing data input (X_test) is comprised of')\n","print('\\t',X_test_row,'instances and',X_test_col,'features')\n","print('\\tOur testing output (y_test) is comprised of')\n","print('\\t',y_test_row,'instances')\n","print()\n","\n","######\n","#\n","# Call your functions for logistic regression, zeroR, and calculate accuracy\n","# PRINT NICELY\n","#\n","######\n","\n","\n","log_predictY = log_reg(X_train, y_train, X_test, Learning_rate, Epochs)\n","zeroRC_predictY = zeroRC(y_train.tolist(), X_test)\n","\n","accuracy_logReg2 = accuracy(y_test, log_predictY) * 100\n","accuracy_zeroRC2 = accuracy(y_test, zeroRC_predictY) * 100\n","print(\"Logistic Regression prediction accuracy = %.3f\" %accuracy_logReg2,\"%\")\n","print(\"ZeroR prediction accuracy = %.3f\" % accuracy_zeroRC2,\"%\")\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["***PRE SELECTION***\n","\n","Our original input data (X) is comprised of\n","768 instances and 8 features\n","\n","Our original output data (y) is comprised of\n","(768,) instances\n","\n","***SLICING the data into TRAIN and TEST data***\n","\n","TRAINING DATA:\n","\tOur training data input (X_train) is comprised of\n","\t 514 instances and 8 features\n","\tOur trainging output (y_train) is comprised of\n","\t (514,) instances\n","TESTING DATA:\n","\tOur testing data input (X_test) is comprised of\n","\t 254 instances and 8 features\n","\tOur testing output (y_test) is comprised of\n","\t (254,) instances\n","\n",">epoch= 0 lrate= 0.1 error=119.896\n",">epoch= 1 lrate= 0.1 error=114.148\n",">epoch= 2 lrate= 0.1 error=110.057\n",">epoch= 3 lrate= 0.1 error=106.675\n",">epoch= 4 lrate= 0.1 error=103.863\n",">epoch= 5 lrate= 0.1 error=101.500\n",">epoch= 6 lrate= 0.1 error=99.493\n",">epoch= 7 lrate= 0.1 error=97.766\n",">epoch= 8 lrate= 0.1 error=96.264\n",">epoch= 9 lrate= 0.1 error=94.946\n",">epoch= 10 lrate= 0.1 error=93.778\n",">epoch= 11 lrate= 0.1 error=92.736\n",">epoch= 12 lrate= 0.1 error=91.799\n",">epoch= 13 lrate= 0.1 error=90.953\n",">epoch= 14 lrate= 0.1 error=90.184\n",">epoch= 15 lrate= 0.1 error=89.483\n",">epoch= 16 lrate= 0.1 error=88.841\n",">epoch= 17 lrate= 0.1 error=88.250\n",">epoch= 18 lrate= 0.1 error=87.705\n",">epoch= 19 lrate= 0.1 error=87.201\n",">epoch= 20 lrate= 0.1 error=86.734\n",">epoch= 21 lrate= 0.1 error=86.298\n",">epoch= 22 lrate= 0.1 error=85.893\n",">epoch= 23 lrate= 0.1 error=85.513\n",">epoch= 24 lrate= 0.1 error=85.158\n",">epoch= 25 lrate= 0.1 error=84.825\n",">epoch= 26 lrate= 0.1 error=84.512\n",">epoch= 27 lrate= 0.1 error=84.217\n",">epoch= 28 lrate= 0.1 error=83.939\n",">epoch= 29 lrate= 0.1 error=83.677\n",">epoch= 30 lrate= 0.1 error=83.428\n",">epoch= 31 lrate= 0.1 error=83.193\n",">epoch= 32 lrate= 0.1 error=82.970\n",">epoch= 33 lrate= 0.1 error=82.758\n",">epoch= 34 lrate= 0.1 error=82.557\n",">epoch= 35 lrate= 0.1 error=82.366\n",">epoch= 36 lrate= 0.1 error=82.183\n",">epoch= 37 lrate= 0.1 error=82.010\n",">epoch= 38 lrate= 0.1 error=81.844\n",">epoch= 39 lrate= 0.1 error=81.685\n",">epoch= 40 lrate= 0.1 error=81.534\n",">epoch= 41 lrate= 0.1 error=81.389\n",">epoch= 42 lrate= 0.1 error=81.250\n",">epoch= 43 lrate= 0.1 error=81.118\n",">epoch= 44 lrate= 0.1 error=80.990\n",">epoch= 45 lrate= 0.1 error=80.868\n",">epoch= 46 lrate= 0.1 error=80.751\n",">epoch= 47 lrate= 0.1 error=80.638\n",">epoch= 48 lrate= 0.1 error=80.530\n",">epoch= 49 lrate= 0.1 error=80.426\n",">epoch= 50 lrate= 0.1 error=80.325\n",">epoch= 51 lrate= 0.1 error=80.229\n",">epoch= 52 lrate= 0.1 error=80.136\n",">epoch= 53 lrate= 0.1 error=80.046\n",">epoch= 54 lrate= 0.1 error=79.960\n",">epoch= 55 lrate= 0.1 error=79.877\n",">epoch= 56 lrate= 0.1 error=79.796\n",">epoch= 57 lrate= 0.1 error=79.719\n",">epoch= 58 lrate= 0.1 error=79.644\n",">epoch= 59 lrate= 0.1 error=79.571\n",">epoch= 60 lrate= 0.1 error=79.501\n",">epoch= 61 lrate= 0.1 error=79.433\n",">epoch= 62 lrate= 0.1 error=79.368\n",">epoch= 63 lrate= 0.1 error=79.304\n",">epoch= 64 lrate= 0.1 error=79.243\n",">epoch= 65 lrate= 0.1 error=79.183\n",">epoch= 66 lrate= 0.1 error=79.125\n",">epoch= 67 lrate= 0.1 error=79.070\n",">epoch= 68 lrate= 0.1 error=79.015\n",">epoch= 69 lrate= 0.1 error=78.963\n",">epoch= 70 lrate= 0.1 error=78.912\n",">epoch= 71 lrate= 0.1 error=78.862\n",">epoch= 72 lrate= 0.1 error=78.814\n",">epoch= 73 lrate= 0.1 error=78.768\n",">epoch= 74 lrate= 0.1 error=78.723\n",">epoch= 75 lrate= 0.1 error=78.679\n",">epoch= 76 lrate= 0.1 error=78.636\n",">epoch= 77 lrate= 0.1 error=78.594\n",">epoch= 78 lrate= 0.1 error=78.554\n",">epoch= 79 lrate= 0.1 error=78.515\n",">epoch= 80 lrate= 0.1 error=78.476\n",">epoch= 81 lrate= 0.1 error=78.439\n",">epoch= 82 lrate= 0.1 error=78.403\n",">epoch= 83 lrate= 0.1 error=78.368\n",">epoch= 84 lrate= 0.1 error=78.334\n",">epoch= 85 lrate= 0.1 error=78.300\n",">epoch= 86 lrate= 0.1 error=78.268\n",">epoch= 87 lrate= 0.1 error=78.236\n",">epoch= 88 lrate= 0.1 error=78.206\n",">epoch= 89 lrate= 0.1 error=78.175\n",">epoch= 90 lrate= 0.1 error=78.146\n",">epoch= 91 lrate= 0.1 error=78.118\n",">epoch= 92 lrate= 0.1 error=78.090\n",">epoch= 93 lrate= 0.1 error=78.063\n",">epoch= 94 lrate= 0.1 error=78.036\n",">epoch= 95 lrate= 0.1 error=78.010\n",">epoch= 96 lrate= 0.1 error=77.985\n",">epoch= 97 lrate= 0.1 error=77.961\n",">epoch= 98 lrate= 0.1 error=77.937\n",">epoch= 99 lrate= 0.1 error=77.913\n","Coefficient is:  [-6.708105293136109, 0.6884396182317725, 6.211688545551201, -1.0942292789493706, -0.2059639089090158, -0.14145005977312214, 4.615910583831365, 1.2756994157285912, 1.6969505948486565]\n","Logistic Regression prediction accuracy = 75.197 %\n","ZeroR prediction accuracy = 66.142 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"REijQ4q8SdTQ"},"source":["The accuracy of logistic regression (75.197%) is lower than before(77.344%) by spliting data. At the same time, the accuracy of ZeroR(66.142%) is higher than before (65.104%) by spliting data.\n","\n","But overall, the accuracy of Logistic regression is still better than the baseline."]},{"cell_type":"markdown","metadata":{"id":"l4BxZqYp_P4Y"},"source":["Above is the single, holdout estimation approach. It's a useful first step. But I've told you that we typically prefer k-fold cross-validation. The most frequently used is 10-fold cross-validation. For now, we'll use a value of k=5 (i.e. a five-fold cross validation). We're going to do that TWICE - once will be STRATIFIED, and once without stratification. Make sure you know what that means.  \n","\n","For each of the cells below, I'm creating an instance of the kfold mechanism. We'll start with the version that is NOT STRATIFIED.\n","\n","I'm creating a list to hold my scores from each of my two algorithms. You're going to be running logistic regression and zeroR.\n","\n","I'm splitting my data in X_train, y_train, X_test and y_test data. \n","\n","(u) Then inside the loop, you need to call:\n","- your logistic regression function (using X_train,y_train and y_test), to get a list of predicted y values\n","- your zeroR function (with y_train and X_test), to get a list of predicted y values\n","- your accuracy measure, given y_test and your predicted values for each algorithm\n","- append the accuracy score for logistic regression to lg_scores\n","- append the accuracy score for zeroR to the zr_scores\n","\n","At the completion of the loop, print the following ON ONE LINE per algorithm:\n","- Average accuracy over the 5 runs\n","- MINIMIUM accuracy of the 5 runs\n","- MAXIMUM accuracy of the 5 runs\n","\n","Compare these scores to the single holdout estimation in (t) above, and to the substitution error in (s) above, for both logistic regression and zeroR, and comment on the differences.\n"]},{"cell_type":"code","metadata":{"id":"vP__qDzP7RYf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622146131037,"user_tz":240,"elapsed":6304,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"c20ef265-00c7-4cd7-b221-08a0d07ba645"},"source":["from sklearn.model_selection import KFold\n","kf = KFold(n_splits=5,shuffle=True)\n","lg_scores = []\n","zr_scores = []\n","\n","# The loop below will iterate for n_splits times - 5 in this case\n","\n","for train_index, test_index in kf.split(X_values,y_values):\n","    X_train, X_test = X_values[train_index], X_values[test_index]\n","    y_train, y_test = y_values[train_index], y_values[test_index]\n","\n","    # Add in calls to your logistic regression and zeroR functions here\n","    # Then calculate accuracy, and append each score to the appropriate list\n","    logReg_Pred = log_reg(X_train,y_train,X_test,Learning_rate, Epochs)\n","    zeroR_pred = zeroRC(y_train.tolist(),X_test)\n","    print(\"zeroR predict: \", zeroR_pred)\n","    accuracy_logReg3 = accuracy(y_test, logReg_Pred) * 100\n","    lg_scores.append(accuracy_logReg3)\n","\n","    accuracy_zeroRC3 = accuracy(y_test, zeroR_pred) * 100\n","    zr_scores.append(accuracy_zeroRC3)\n","    \n","# Once the loop is over, calculate and print average score, min and max for each algorithm.\n","\n","lg_average_accuracy = sum(lg_scores)/len(lg_scores)\n","zr_average_accuracy = sum(zr_scores)/len(zr_scores)\n","print(\"Average accuracy of logistic regression = \", lg_average_accuracy, \n","      \" Average accuracy of ZeroR = \", zr_average_accuracy)\n","\n","lg_min_accuracy = min(lg_scores)\n","zr_min_accuracy = min(zr_scores)\n","print(\"The minimun accuracy of Logistic Regression = \", lg_min_accuracy, \n","      \"The minimun accuracy of ZeroR = \", zr_min_accuracy)\n","\n","lg_max_accuracy = max(lg_scores)\n","zr_max_accuracy = max(zr_scores)\n","print(\"The maximun accuracy of Logistic Regression = \", lg_max_accuracy, \n","      \"The maximun accuracy of ZeroR = \", zr_max_accuracy)\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":[">epoch= 0 lrate= 0.1 error=141.596\n",">epoch= 1 lrate= 0.1 error=134.512\n",">epoch= 2 lrate= 0.1 error=129.026\n",">epoch= 3 lrate= 0.1 error=124.692\n",">epoch= 4 lrate= 0.1 error=121.218\n",">epoch= 5 lrate= 0.1 error=118.385\n",">epoch= 6 lrate= 0.1 error=116.032\n",">epoch= 7 lrate= 0.1 error=114.045\n",">epoch= 8 lrate= 0.1 error=112.344\n",">epoch= 9 lrate= 0.1 error=110.870\n",">epoch= 10 lrate= 0.1 error=109.578\n",">epoch= 11 lrate= 0.1 error=108.437\n",">epoch= 12 lrate= 0.1 error=107.420\n",">epoch= 13 lrate= 0.1 error=106.509\n",">epoch= 14 lrate= 0.1 error=105.687\n",">epoch= 15 lrate= 0.1 error=104.942\n",">epoch= 16 lrate= 0.1 error=104.263\n",">epoch= 17 lrate= 0.1 error=103.642\n",">epoch= 18 lrate= 0.1 error=103.072\n",">epoch= 19 lrate= 0.1 error=102.546\n",">epoch= 20 lrate= 0.1 error=102.061\n",">epoch= 21 lrate= 0.1 error=101.611\n",">epoch= 22 lrate= 0.1 error=101.192\n",">epoch= 23 lrate= 0.1 error=100.802\n",">epoch= 24 lrate= 0.1 error=100.438\n",">epoch= 25 lrate= 0.1 error=100.097\n",">epoch= 26 lrate= 0.1 error=99.778\n",">epoch= 27 lrate= 0.1 error=99.478\n",">epoch= 28 lrate= 0.1 error=99.195\n",">epoch= 29 lrate= 0.1 error=98.929\n",">epoch= 30 lrate= 0.1 error=98.677\n",">epoch= 31 lrate= 0.1 error=98.440\n",">epoch= 32 lrate= 0.1 error=98.215\n",">epoch= 33 lrate= 0.1 error=98.001\n",">epoch= 34 lrate= 0.1 error=97.799\n",">epoch= 35 lrate= 0.1 error=97.607\n",">epoch= 36 lrate= 0.1 error=97.424\n",">epoch= 37 lrate= 0.1 error=97.250\n",">epoch= 38 lrate= 0.1 error=97.084\n",">epoch= 39 lrate= 0.1 error=96.926\n",">epoch= 40 lrate= 0.1 error=96.775\n",">epoch= 41 lrate= 0.1 error=96.630\n",">epoch= 42 lrate= 0.1 error=96.492\n",">epoch= 43 lrate= 0.1 error=96.361\n",">epoch= 44 lrate= 0.1 error=96.234\n",">epoch= 45 lrate= 0.1 error=96.113\n",">epoch= 46 lrate= 0.1 error=95.997\n",">epoch= 47 lrate= 0.1 error=95.886\n",">epoch= 48 lrate= 0.1 error=95.779\n",">epoch= 49 lrate= 0.1 error=95.676\n",">epoch= 50 lrate= 0.1 error=95.578\n",">epoch= 51 lrate= 0.1 error=95.483\n",">epoch= 52 lrate= 0.1 error=95.392\n",">epoch= 53 lrate= 0.1 error=95.304\n",">epoch= 54 lrate= 0.1 error=95.220\n",">epoch= 55 lrate= 0.1 error=95.138\n",">epoch= 56 lrate= 0.1 error=95.060\n",">epoch= 57 lrate= 0.1 error=94.984\n",">epoch= 58 lrate= 0.1 error=94.911\n",">epoch= 59 lrate= 0.1 error=94.841\n",">epoch= 60 lrate= 0.1 error=94.773\n",">epoch= 61 lrate= 0.1 error=94.707\n",">epoch= 62 lrate= 0.1 error=94.644\n",">epoch= 63 lrate= 0.1 error=94.582\n",">epoch= 64 lrate= 0.1 error=94.523\n",">epoch= 65 lrate= 0.1 error=94.466\n",">epoch= 66 lrate= 0.1 error=94.410\n",">epoch= 67 lrate= 0.1 error=94.357\n",">epoch= 68 lrate= 0.1 error=94.305\n",">epoch= 69 lrate= 0.1 error=94.255\n",">epoch= 70 lrate= 0.1 error=94.206\n",">epoch= 71 lrate= 0.1 error=94.159\n",">epoch= 72 lrate= 0.1 error=94.113\n",">epoch= 73 lrate= 0.1 error=94.069\n",">epoch= 74 lrate= 0.1 error=94.026\n",">epoch= 75 lrate= 0.1 error=93.984\n",">epoch= 76 lrate= 0.1 error=93.943\n",">epoch= 77 lrate= 0.1 error=93.904\n",">epoch= 78 lrate= 0.1 error=93.866\n",">epoch= 79 lrate= 0.1 error=93.829\n",">epoch= 80 lrate= 0.1 error=93.793\n",">epoch= 81 lrate= 0.1 error=93.758\n",">epoch= 82 lrate= 0.1 error=93.724\n",">epoch= 83 lrate= 0.1 error=93.691\n",">epoch= 84 lrate= 0.1 error=93.659\n",">epoch= 85 lrate= 0.1 error=93.628\n",">epoch= 86 lrate= 0.1 error=93.598\n",">epoch= 87 lrate= 0.1 error=93.569\n",">epoch= 88 lrate= 0.1 error=93.540\n",">epoch= 89 lrate= 0.1 error=93.512\n",">epoch= 90 lrate= 0.1 error=93.485\n",">epoch= 91 lrate= 0.1 error=93.459\n",">epoch= 92 lrate= 0.1 error=93.433\n",">epoch= 93 lrate= 0.1 error=93.408\n",">epoch= 94 lrate= 0.1 error=93.383\n",">epoch= 95 lrate= 0.1 error=93.360\n",">epoch= 96 lrate= 0.1 error=93.337\n",">epoch= 97 lrate= 0.1 error=93.314\n",">epoch= 98 lrate= 0.1 error=93.292\n",">epoch= 99 lrate= 0.1 error=93.271\n","Coefficient is:  [-6.790354849555508, 2.140043803685448, 6.6210902443035256, -1.2533597583698806, -0.16094738585533366, -0.27111656201866574, 4.206449782342862, 1.9886070515355976, 0.5532215624671357]\n","zeroR predict:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",">epoch= 0 lrate= 0.1 error=140.267\n",">epoch= 1 lrate= 0.1 error=134.219\n",">epoch= 2 lrate= 0.1 error=129.338\n",">epoch= 3 lrate= 0.1 error=125.357\n",">epoch= 4 lrate= 0.1 error=122.091\n",">epoch= 5 lrate= 0.1 error=119.384\n",">epoch= 6 lrate= 0.1 error=117.115\n",">epoch= 7 lrate= 0.1 error=115.192\n",">epoch= 8 lrate= 0.1 error=113.545\n",">epoch= 9 lrate= 0.1 error=112.120\n",">epoch= 10 lrate= 0.1 error=110.877\n",">epoch= 11 lrate= 0.1 error=109.784\n",">epoch= 12 lrate= 0.1 error=108.816\n",">epoch= 13 lrate= 0.1 error=107.953\n",">epoch= 14 lrate= 0.1 error=107.179\n",">epoch= 15 lrate= 0.1 error=106.481\n",">epoch= 16 lrate= 0.1 error=105.848\n",">epoch= 17 lrate= 0.1 error=105.273\n",">epoch= 18 lrate= 0.1 error=104.747\n",">epoch= 19 lrate= 0.1 error=104.264\n",">epoch= 20 lrate= 0.1 error=103.820\n",">epoch= 21 lrate= 0.1 error=103.410\n",">epoch= 22 lrate= 0.1 error=103.031\n",">epoch= 23 lrate= 0.1 error=102.678\n",">epoch= 24 lrate= 0.1 error=102.350\n",">epoch= 25 lrate= 0.1 error=102.044\n",">epoch= 26 lrate= 0.1 error=101.757\n",">epoch= 27 lrate= 0.1 error=101.489\n",">epoch= 28 lrate= 0.1 error=101.237\n",">epoch= 29 lrate= 0.1 error=101.000\n",">epoch= 30 lrate= 0.1 error=100.777\n",">epoch= 31 lrate= 0.1 error=100.567\n",">epoch= 32 lrate= 0.1 error=100.368\n",">epoch= 33 lrate= 0.1 error=100.180\n",">epoch= 34 lrate= 0.1 error=100.002\n",">epoch= 35 lrate= 0.1 error=99.833\n",">epoch= 36 lrate= 0.1 error=99.672\n",">epoch= 37 lrate= 0.1 error=99.520\n",">epoch= 38 lrate= 0.1 error=99.375\n",">epoch= 39 lrate= 0.1 error=99.237\n",">epoch= 40 lrate= 0.1 error=99.105\n",">epoch= 41 lrate= 0.1 error=98.979\n",">epoch= 42 lrate= 0.1 error=98.859\n",">epoch= 43 lrate= 0.1 error=98.745\n",">epoch= 44 lrate= 0.1 error=98.635\n",">epoch= 45 lrate= 0.1 error=98.530\n",">epoch= 46 lrate= 0.1 error=98.430\n",">epoch= 47 lrate= 0.1 error=98.334\n",">epoch= 48 lrate= 0.1 error=98.242\n",">epoch= 49 lrate= 0.1 error=98.153\n",">epoch= 50 lrate= 0.1 error=98.068\n",">epoch= 51 lrate= 0.1 error=97.987\n",">epoch= 52 lrate= 0.1 error=97.908\n",">epoch= 53 lrate= 0.1 error=97.833\n",">epoch= 54 lrate= 0.1 error=97.760\n",">epoch= 55 lrate= 0.1 error=97.691\n",">epoch= 56 lrate= 0.1 error=97.623\n",">epoch= 57 lrate= 0.1 error=97.559\n",">epoch= 58 lrate= 0.1 error=97.496\n",">epoch= 59 lrate= 0.1 error=97.436\n",">epoch= 60 lrate= 0.1 error=97.378\n",">epoch= 61 lrate= 0.1 error=97.322\n",">epoch= 62 lrate= 0.1 error=97.268\n",">epoch= 63 lrate= 0.1 error=97.216\n",">epoch= 64 lrate= 0.1 error=97.166\n",">epoch= 65 lrate= 0.1 error=97.117\n",">epoch= 66 lrate= 0.1 error=97.070\n",">epoch= 67 lrate= 0.1 error=97.025\n",">epoch= 68 lrate= 0.1 error=96.981\n",">epoch= 69 lrate= 0.1 error=96.938\n",">epoch= 70 lrate= 0.1 error=96.897\n",">epoch= 71 lrate= 0.1 error=96.857\n",">epoch= 72 lrate= 0.1 error=96.818\n",">epoch= 73 lrate= 0.1 error=96.781\n",">epoch= 74 lrate= 0.1 error=96.745\n",">epoch= 75 lrate= 0.1 error=96.709\n",">epoch= 76 lrate= 0.1 error=96.675\n",">epoch= 77 lrate= 0.1 error=96.642\n",">epoch= 78 lrate= 0.1 error=96.610\n",">epoch= 79 lrate= 0.1 error=96.579\n",">epoch= 80 lrate= 0.1 error=96.549\n",">epoch= 81 lrate= 0.1 error=96.520\n",">epoch= 82 lrate= 0.1 error=96.491\n",">epoch= 83 lrate= 0.1 error=96.464\n",">epoch= 84 lrate= 0.1 error=96.437\n",">epoch= 85 lrate= 0.1 error=96.411\n",">epoch= 86 lrate= 0.1 error=96.385\n",">epoch= 87 lrate= 0.1 error=96.361\n",">epoch= 88 lrate= 0.1 error=96.337\n",">epoch= 89 lrate= 0.1 error=96.314\n",">epoch= 90 lrate= 0.1 error=96.291\n",">epoch= 91 lrate= 0.1 error=96.269\n",">epoch= 92 lrate= 0.1 error=96.247\n",">epoch= 93 lrate= 0.1 error=96.227\n",">epoch= 94 lrate= 0.1 error=96.206\n",">epoch= 95 lrate= 0.1 error=96.187\n",">epoch= 96 lrate= 0.1 error=96.167\n",">epoch= 97 lrate= 0.1 error=96.149\n",">epoch= 98 lrate= 0.1 error=96.130\n",">epoch= 99 lrate= 0.1 error=96.112\n","Coefficient is:  [-6.5001955399653, 1.725071170212954, 5.995642757716791, -1.3600244596184274, -0.026225087954700833, 0.19892859273147892, 4.098513854605037, 2.5590876724450857, 0.5789971013295381]\n","zeroR predict:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",">epoch= 0 lrate= 0.1 error=137.625\n",">epoch= 1 lrate= 0.1 error=131.039\n",">epoch= 2 lrate= 0.1 error=125.960\n",">epoch= 3 lrate= 0.1 error=121.826\n",">epoch= 4 lrate= 0.1 error=118.434\n",">epoch= 5 lrate= 0.1 error=115.612\n",">epoch= 6 lrate= 0.1 error=113.231\n",">epoch= 7 lrate= 0.1 error=111.193\n",">epoch= 8 lrate= 0.1 error=109.428\n",">epoch= 9 lrate= 0.1 error=107.884\n",">epoch= 10 lrate= 0.1 error=106.520\n",">epoch= 11 lrate= 0.1 error=105.307\n",">epoch= 12 lrate= 0.1 error=104.220\n",">epoch= 13 lrate= 0.1 error=103.240\n",">epoch= 14 lrate= 0.1 error=102.353\n",">epoch= 15 lrate= 0.1 error=101.545\n",">epoch= 16 lrate= 0.1 error=100.807\n",">epoch= 17 lrate= 0.1 error=100.130\n",">epoch= 18 lrate= 0.1 error=99.506\n",">epoch= 19 lrate= 0.1 error=98.930\n",">epoch= 20 lrate= 0.1 error=98.397\n",">epoch= 21 lrate= 0.1 error=97.902\n",">epoch= 22 lrate= 0.1 error=97.441\n",">epoch= 23 lrate= 0.1 error=97.011\n",">epoch= 24 lrate= 0.1 error=96.608\n",">epoch= 25 lrate= 0.1 error=96.231\n",">epoch= 26 lrate= 0.1 error=95.878\n",">epoch= 27 lrate= 0.1 error=95.545\n",">epoch= 28 lrate= 0.1 error=95.232\n",">epoch= 29 lrate= 0.1 error=94.936\n",">epoch= 30 lrate= 0.1 error=94.657\n",">epoch= 31 lrate= 0.1 error=94.393\n",">epoch= 32 lrate= 0.1 error=94.142\n",">epoch= 33 lrate= 0.1 error=93.905\n",">epoch= 34 lrate= 0.1 error=93.680\n",">epoch= 35 lrate= 0.1 error=93.466\n",">epoch= 36 lrate= 0.1 error=93.262\n",">epoch= 37 lrate= 0.1 error=93.068\n",">epoch= 38 lrate= 0.1 error=92.883\n",">epoch= 39 lrate= 0.1 error=92.707\n",">epoch= 40 lrate= 0.1 error=92.538\n",">epoch= 41 lrate= 0.1 error=92.377\n",">epoch= 42 lrate= 0.1 error=92.223\n",">epoch= 43 lrate= 0.1 error=92.076\n",">epoch= 44 lrate= 0.1 error=91.935\n",">epoch= 45 lrate= 0.1 error=91.799\n",">epoch= 46 lrate= 0.1 error=91.669\n",">epoch= 47 lrate= 0.1 error=91.545\n",">epoch= 48 lrate= 0.1 error=91.425\n",">epoch= 49 lrate= 0.1 error=91.310\n",">epoch= 50 lrate= 0.1 error=91.199\n",">epoch= 51 lrate= 0.1 error=91.093\n",">epoch= 52 lrate= 0.1 error=90.990\n",">epoch= 53 lrate= 0.1 error=90.892\n",">epoch= 54 lrate= 0.1 error=90.797\n",">epoch= 55 lrate= 0.1 error=90.705\n",">epoch= 56 lrate= 0.1 error=90.616\n",">epoch= 57 lrate= 0.1 error=90.531\n",">epoch= 58 lrate= 0.1 error=90.449\n",">epoch= 59 lrate= 0.1 error=90.369\n",">epoch= 60 lrate= 0.1 error=90.292\n",">epoch= 61 lrate= 0.1 error=90.218\n",">epoch= 62 lrate= 0.1 error=90.146\n",">epoch= 63 lrate= 0.1 error=90.077\n",">epoch= 64 lrate= 0.1 error=90.009\n",">epoch= 65 lrate= 0.1 error=89.944\n",">epoch= 66 lrate= 0.1 error=89.881\n",">epoch= 67 lrate= 0.1 error=89.820\n",">epoch= 68 lrate= 0.1 error=89.761\n",">epoch= 69 lrate= 0.1 error=89.704\n",">epoch= 70 lrate= 0.1 error=89.648\n",">epoch= 71 lrate= 0.1 error=89.594\n",">epoch= 72 lrate= 0.1 error=89.542\n",">epoch= 73 lrate= 0.1 error=89.491\n",">epoch= 74 lrate= 0.1 error=89.441\n",">epoch= 75 lrate= 0.1 error=89.393\n",">epoch= 76 lrate= 0.1 error=89.347\n",">epoch= 77 lrate= 0.1 error=89.302\n",">epoch= 78 lrate= 0.1 error=89.258\n",">epoch= 79 lrate= 0.1 error=89.215\n",">epoch= 80 lrate= 0.1 error=89.173\n",">epoch= 81 lrate= 0.1 error=89.133\n",">epoch= 82 lrate= 0.1 error=89.093\n",">epoch= 83 lrate= 0.1 error=89.055\n",">epoch= 84 lrate= 0.1 error=89.018\n",">epoch= 85 lrate= 0.1 error=88.981\n",">epoch= 86 lrate= 0.1 error=88.946\n",">epoch= 87 lrate= 0.1 error=88.912\n",">epoch= 88 lrate= 0.1 error=88.878\n",">epoch= 89 lrate= 0.1 error=88.845\n",">epoch= 90 lrate= 0.1 error=88.814\n",">epoch= 91 lrate= 0.1 error=88.782\n",">epoch= 92 lrate= 0.1 error=88.752\n",">epoch= 93 lrate= 0.1 error=88.723\n",">epoch= 94 lrate= 0.1 error=88.694\n",">epoch= 95 lrate= 0.1 error=88.666\n",">epoch= 96 lrate= 0.1 error=88.638\n",">epoch= 97 lrate= 0.1 error=88.611\n",">epoch= 98 lrate= 0.1 error=88.585\n",">epoch= 99 lrate= 0.1 error=88.560\n","Coefficient is:  [-6.953278251926474, 1.7729610063311434, 6.35607963792215, -1.7357607695465498, -0.5483093560280323, -0.4279775199029173, 4.888239158235704, 2.7127084545311786, 0.7797161691725936]\n","zeroR predict:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",">epoch= 0 lrate= 0.1 error=143.102\n",">epoch= 1 lrate= 0.1 error=136.810\n",">epoch= 2 lrate= 0.1 error=131.751\n",">epoch= 3 lrate= 0.1 error=127.734\n",">epoch= 4 lrate= 0.1 error=124.504\n",">epoch= 5 lrate= 0.1 error=121.865\n",">epoch= 6 lrate= 0.1 error=119.674\n",">epoch= 7 lrate= 0.1 error=117.827\n",">epoch= 8 lrate= 0.1 error=116.251\n",">epoch= 9 lrate= 0.1 error=114.890\n",">epoch= 10 lrate= 0.1 error=113.703\n",">epoch= 11 lrate= 0.1 error=112.659\n",">epoch= 12 lrate= 0.1 error=111.734\n",">epoch= 13 lrate= 0.1 error=110.908\n",">epoch= 14 lrate= 0.1 error=110.168\n",">epoch= 15 lrate= 0.1 error=109.500\n",">epoch= 16 lrate= 0.1 error=108.895\n",">epoch= 17 lrate= 0.1 error=108.345\n",">epoch= 18 lrate= 0.1 error=107.842\n",">epoch= 19 lrate= 0.1 error=107.381\n",">epoch= 20 lrate= 0.1 error=106.957\n",">epoch= 21 lrate= 0.1 error=106.566\n",">epoch= 22 lrate= 0.1 error=106.204\n",">epoch= 23 lrate= 0.1 error=105.868\n",">epoch= 24 lrate= 0.1 error=105.556\n",">epoch= 25 lrate= 0.1 error=105.266\n",">epoch= 26 lrate= 0.1 error=104.995\n",">epoch= 27 lrate= 0.1 error=104.741\n",">epoch= 28 lrate= 0.1 error=104.503\n",">epoch= 29 lrate= 0.1 error=104.280\n",">epoch= 30 lrate= 0.1 error=104.071\n",">epoch= 31 lrate= 0.1 error=103.874\n",">epoch= 32 lrate= 0.1 error=103.688\n",">epoch= 33 lrate= 0.1 error=103.512\n",">epoch= 34 lrate= 0.1 error=103.347\n",">epoch= 35 lrate= 0.1 error=103.190\n",">epoch= 36 lrate= 0.1 error=103.041\n",">epoch= 37 lrate= 0.1 error=102.901\n",">epoch= 38 lrate= 0.1 error=102.767\n",">epoch= 39 lrate= 0.1 error=102.641\n",">epoch= 40 lrate= 0.1 error=102.520\n",">epoch= 41 lrate= 0.1 error=102.406\n",">epoch= 42 lrate= 0.1 error=102.297\n",">epoch= 43 lrate= 0.1 error=102.193\n",">epoch= 44 lrate= 0.1 error=102.094\n",">epoch= 45 lrate= 0.1 error=101.999\n",">epoch= 46 lrate= 0.1 error=101.909\n",">epoch= 47 lrate= 0.1 error=101.823\n",">epoch= 48 lrate= 0.1 error=101.740\n",">epoch= 49 lrate= 0.1 error=101.662\n",">epoch= 50 lrate= 0.1 error=101.586\n",">epoch= 51 lrate= 0.1 error=101.514\n",">epoch= 52 lrate= 0.1 error=101.445\n",">epoch= 53 lrate= 0.1 error=101.379\n",">epoch= 54 lrate= 0.1 error=101.316\n",">epoch= 55 lrate= 0.1 error=101.255\n",">epoch= 56 lrate= 0.1 error=101.196\n",">epoch= 57 lrate= 0.1 error=101.140\n",">epoch= 58 lrate= 0.1 error=101.086\n",">epoch= 59 lrate= 0.1 error=101.035\n",">epoch= 60 lrate= 0.1 error=100.985\n",">epoch= 61 lrate= 0.1 error=100.937\n",">epoch= 62 lrate= 0.1 error=100.891\n",">epoch= 63 lrate= 0.1 error=100.847\n",">epoch= 64 lrate= 0.1 error=100.804\n",">epoch= 65 lrate= 0.1 error=100.763\n",">epoch= 66 lrate= 0.1 error=100.724\n",">epoch= 67 lrate= 0.1 error=100.686\n",">epoch= 68 lrate= 0.1 error=100.649\n",">epoch= 69 lrate= 0.1 error=100.614\n",">epoch= 70 lrate= 0.1 error=100.580\n",">epoch= 71 lrate= 0.1 error=100.547\n",">epoch= 72 lrate= 0.1 error=100.515\n",">epoch= 73 lrate= 0.1 error=100.485\n",">epoch= 74 lrate= 0.1 error=100.455\n",">epoch= 75 lrate= 0.1 error=100.426\n",">epoch= 76 lrate= 0.1 error=100.399\n",">epoch= 77 lrate= 0.1 error=100.372\n",">epoch= 78 lrate= 0.1 error=100.347\n",">epoch= 79 lrate= 0.1 error=100.322\n",">epoch= 80 lrate= 0.1 error=100.298\n",">epoch= 81 lrate= 0.1 error=100.275\n",">epoch= 82 lrate= 0.1 error=100.252\n",">epoch= 83 lrate= 0.1 error=100.230\n",">epoch= 84 lrate= 0.1 error=100.209\n",">epoch= 85 lrate= 0.1 error=100.189\n",">epoch= 86 lrate= 0.1 error=100.169\n",">epoch= 87 lrate= 0.1 error=100.150\n",">epoch= 88 lrate= 0.1 error=100.132\n",">epoch= 89 lrate= 0.1 error=100.114\n",">epoch= 90 lrate= 0.1 error=100.097\n",">epoch= 91 lrate= 0.1 error=100.080\n",">epoch= 92 lrate= 0.1 error=100.064\n",">epoch= 93 lrate= 0.1 error=100.048\n",">epoch= 94 lrate= 0.1 error=100.033\n",">epoch= 95 lrate= 0.1 error=100.018\n",">epoch= 96 lrate= 0.1 error=100.003\n",">epoch= 97 lrate= 0.1 error=99.989\n",">epoch= 98 lrate= 0.1 error=99.976\n",">epoch= 99 lrate= 0.1 error=99.963\n","Coefficient is:  [-6.322207059506271, 1.7382825223428309, 6.2133929703581, -1.2945633661397473, 0.8540551578404236, -1.0055686246666315, 3.5462200069832077, 1.4981216080598942, 0.9209381468645665]\n","zeroR predict:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",">epoch= 0 lrate= 0.1 error=139.511\n",">epoch= 1 lrate= 0.1 error=133.459\n",">epoch= 2 lrate= 0.1 error=128.760\n",">epoch= 3 lrate= 0.1 error=124.978\n",">epoch= 4 lrate= 0.1 error=121.901\n",">epoch= 5 lrate= 0.1 error=119.357\n",">epoch= 6 lrate= 0.1 error=117.216\n",">epoch= 7 lrate= 0.1 error=115.387\n",">epoch= 8 lrate= 0.1 error=113.803\n",">epoch= 9 lrate= 0.1 error=112.414\n",">epoch= 10 lrate= 0.1 error=111.185\n",">epoch= 11 lrate= 0.1 error=110.089\n",">epoch= 12 lrate= 0.1 error=109.104\n",">epoch= 13 lrate= 0.1 error=108.214\n",">epoch= 14 lrate= 0.1 error=107.405\n",">epoch= 15 lrate= 0.1 error=106.668\n",">epoch= 16 lrate= 0.1 error=105.992\n",">epoch= 17 lrate= 0.1 error=105.371\n",">epoch= 18 lrate= 0.1 error=104.799\n",">epoch= 19 lrate= 0.1 error=104.270\n",">epoch= 20 lrate= 0.1 error=103.779\n",">epoch= 21 lrate= 0.1 error=103.323\n",">epoch= 22 lrate= 0.1 error=102.899\n",">epoch= 23 lrate= 0.1 error=102.502\n",">epoch= 24 lrate= 0.1 error=102.131\n",">epoch= 25 lrate= 0.1 error=101.784\n",">epoch= 26 lrate= 0.1 error=101.458\n",">epoch= 27 lrate= 0.1 error=101.151\n",">epoch= 28 lrate= 0.1 error=100.863\n",">epoch= 29 lrate= 0.1 error=100.590\n",">epoch= 30 lrate= 0.1 error=100.333\n",">epoch= 31 lrate= 0.1 error=100.090\n",">epoch= 32 lrate= 0.1 error=99.860\n",">epoch= 33 lrate= 0.1 error=99.642\n",">epoch= 34 lrate= 0.1 error=99.434\n",">epoch= 35 lrate= 0.1 error=99.238\n",">epoch= 36 lrate= 0.1 error=99.051\n",">epoch= 37 lrate= 0.1 error=98.873\n",">epoch= 38 lrate= 0.1 error=98.703\n",">epoch= 39 lrate= 0.1 error=98.541\n",">epoch= 40 lrate= 0.1 error=98.387\n",">epoch= 41 lrate= 0.1 error=98.239\n",">epoch= 42 lrate= 0.1 error=98.098\n",">epoch= 43 lrate= 0.1 error=97.963\n",">epoch= 44 lrate= 0.1 error=97.834\n",">epoch= 45 lrate= 0.1 error=97.710\n",">epoch= 46 lrate= 0.1 error=97.592\n",">epoch= 47 lrate= 0.1 error=97.478\n",">epoch= 48 lrate= 0.1 error=97.369\n",">epoch= 49 lrate= 0.1 error=97.264\n",">epoch= 50 lrate= 0.1 error=97.163\n",">epoch= 51 lrate= 0.1 error=97.066\n",">epoch= 52 lrate= 0.1 error=96.973\n",">epoch= 53 lrate= 0.1 error=96.883\n",">epoch= 54 lrate= 0.1 error=96.797\n",">epoch= 55 lrate= 0.1 error=96.714\n",">epoch= 56 lrate= 0.1 error=96.634\n",">epoch= 57 lrate= 0.1 error=96.556\n",">epoch= 58 lrate= 0.1 error=96.482\n",">epoch= 59 lrate= 0.1 error=96.410\n",">epoch= 60 lrate= 0.1 error=96.340\n",">epoch= 61 lrate= 0.1 error=96.273\n",">epoch= 62 lrate= 0.1 error=96.208\n",">epoch= 63 lrate= 0.1 error=96.146\n",">epoch= 64 lrate= 0.1 error=96.085\n",">epoch= 65 lrate= 0.1 error=96.026\n",">epoch= 66 lrate= 0.1 error=95.970\n",">epoch= 67 lrate= 0.1 error=95.915\n",">epoch= 68 lrate= 0.1 error=95.862\n",">epoch= 69 lrate= 0.1 error=95.810\n",">epoch= 70 lrate= 0.1 error=95.761\n",">epoch= 71 lrate= 0.1 error=95.712\n",">epoch= 72 lrate= 0.1 error=95.665\n",">epoch= 73 lrate= 0.1 error=95.620\n",">epoch= 74 lrate= 0.1 error=95.576\n",">epoch= 75 lrate= 0.1 error=95.533\n",">epoch= 76 lrate= 0.1 error=95.492\n",">epoch= 77 lrate= 0.1 error=95.452\n",">epoch= 78 lrate= 0.1 error=95.413\n",">epoch= 79 lrate= 0.1 error=95.375\n",">epoch= 80 lrate= 0.1 error=95.338\n",">epoch= 81 lrate= 0.1 error=95.302\n",">epoch= 82 lrate= 0.1 error=95.268\n",">epoch= 83 lrate= 0.1 error=95.234\n",">epoch= 84 lrate= 0.1 error=95.201\n",">epoch= 85 lrate= 0.1 error=95.169\n",">epoch= 86 lrate= 0.1 error=95.138\n",">epoch= 87 lrate= 0.1 error=95.108\n",">epoch= 88 lrate= 0.1 error=95.079\n",">epoch= 89 lrate= 0.1 error=95.050\n",">epoch= 90 lrate= 0.1 error=95.022\n",">epoch= 91 lrate= 0.1 error=94.995\n",">epoch= 92 lrate= 0.1 error=94.969\n",">epoch= 93 lrate= 0.1 error=94.943\n",">epoch= 94 lrate= 0.1 error=94.918\n",">epoch= 95 lrate= 0.1 error=94.894\n",">epoch= 96 lrate= 0.1 error=94.870\n",">epoch= 97 lrate= 0.1 error=94.847\n",">epoch= 98 lrate= 0.1 error=94.825\n",">epoch= 99 lrate= 0.1 error=94.803\n","Coefficient is:  [-6.633978207666777, 1.6771210056578523, 6.494879234468145, -1.0990217130732656, -0.6565249275155197, -0.2804061079644275, 4.2693221603839735, 2.1896777750721577, 0.31728810946485153]\n","zeroR predict:  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","Average accuracy of logistic regression =  76.30931160342926  Average accuracy of ZeroR =  65.10822510822511\n","The minimun accuracy of Logistic Regression =  68.83116883116884 The minimun accuracy of ZeroR =  57.7922077922078\n","The maximun accuracy of Logistic Regression =  79.73856209150327 The maximun accuracy of ZeroR =  71.89542483660131\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5FFjYQb1fEYV"},"source":["The average score of logistic regression here is lower than the score of logistic regression in substitution error, but similar/higher than the socre of logistic regression in holdout estimation.\n","\n","The average score of ZeroR here is lower than the score of ZeroR in holdout estimation, but similar to the socre of ZeroR in substitution error.\n","\n","The minimun score of Logistic Regression and ZeroR are both lower than those in holdout estimation and substitution error.\n","\n","The maximun score of Logistic Regression and ZeroR are both higher than those in holdout estimation and substitution error."]},{"cell_type":"markdown","metadata":{"id":"yxLOoFdSU6RR"},"source":["Missing interpretation: This means "]},{"cell_type":"markdown","metadata":{"id":"GTWHnSQeBSWe"},"source":["(v) Finally we'll repeat the experiment above, using STRATIFIED K-fold cross-validation. I've created the model. You can fill in the rest, exactly as above. Compare the results of the stratified run to the un-stratified run, above. What's the difference?"]},{"cell_type":"code","metadata":{"id":"KapXvVlRB2ow","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622146136798,"user_tz":240,"elapsed":5780,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"a8f1dfc5-71cc-43e8-dcfc-e4512b22bf43"},"source":["from sklearn.model_selection import StratifiedKFold\n","\n","skf = StratifiedKFold(n_splits=5)\n","lg_scores = []\n","zr_scores = []\n","\n","# The loop below will iterate for n_splits times - 5 in this case\n","\n","for train_index, test_index in skf.split(X_values,y_values):\n","    X_train, X_test = X_values[train_index], X_values[test_index]\n","    y_train, y_test = y_values[train_index], y_values[test_index]\n","\n","    # Add in calls to your logistic regression and zeroR functions here\n","    # Then calculate accuracy, and append each score to the appropriate list\n","    logReg_Pred = log_reg(X_train,y_train,X_test,Learning_rate, Epochs)\n","    zeroR_pred = zeroRC(y_train.tolist(),X_test)\n","\n","    accuracy_logReg3 = accuracy(y_test, logReg_Pred) * 100\n","    lg_scores.append(accuracy_logReg3)\n","\n","    accuracy_zeroRC3 = accuracy(y_test, zeroR_pred) * 100\n","    zr_scores.append(accuracy_zeroRC3)\n","    \n","# Once the loop is over, calculate and print average score, min and max for each algorithm.\n","\n","lg_average_accuracy = sum(lg_scores)/len(lg_scores)\n","zr_average_accuracy = sum(zr_scores)/len(zr_scores)\n","print(\"Average accuracy of logistic regression = \", lg_average_accuracy, \n","      \" Average accuracy of ZeroR = \", zr_average_accuracy)\n","\n","lg_min_accuracy = min(lg_scores)\n","zr_min_accuracy = min(zr_scores)\n","print(\"The minimun accuracy of Logistic Regression = \", lg_min_accuracy, \n","      \"The minimun accuracy of ZeroR = \", zr_min_accuracy)\n","\n","lg_max_accuracy = max(lg_scores)\n","zr_max_accuracy = max(zr_scores)\n","print(\"The maximun accuracy of Logistic Regression = \", lg_max_accuracy, \n","      \"The maximun accuracy of ZeroR = \", zr_max_accuracy)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":[">epoch= 0 lrate= 0.1 error=139.882\n",">epoch= 1 lrate= 0.1 error=133.863\n",">epoch= 2 lrate= 0.1 error=128.995\n",">epoch= 3 lrate= 0.1 error=125.036\n",">epoch= 4 lrate= 0.1 error=121.780\n",">epoch= 5 lrate= 0.1 error=119.063\n",">epoch= 6 lrate= 0.1 error=116.762\n",">epoch= 7 lrate= 0.1 error=114.788\n",">epoch= 8 lrate= 0.1 error=113.074\n",">epoch= 9 lrate= 0.1 error=111.572\n",">epoch= 10 lrate= 0.1 error=110.245\n",">epoch= 11 lrate= 0.1 error=109.064\n",">epoch= 12 lrate= 0.1 error=108.005\n",">epoch= 13 lrate= 0.1 error=107.052\n",">epoch= 14 lrate= 0.1 error=106.189\n",">epoch= 15 lrate= 0.1 error=105.404\n",">epoch= 16 lrate= 0.1 error=104.688\n",">epoch= 17 lrate= 0.1 error=104.032\n",">epoch= 18 lrate= 0.1 error=103.428\n",">epoch= 19 lrate= 0.1 error=102.872\n",">epoch= 20 lrate= 0.1 error=102.357\n",">epoch= 21 lrate= 0.1 error=101.879\n",">epoch= 22 lrate= 0.1 error=101.436\n",">epoch= 23 lrate= 0.1 error=101.022\n",">epoch= 24 lrate= 0.1 error=100.636\n",">epoch= 25 lrate= 0.1 error=100.274\n",">epoch= 26 lrate= 0.1 error=99.935\n",">epoch= 27 lrate= 0.1 error=99.617\n",">epoch= 28 lrate= 0.1 error=99.317\n",">epoch= 29 lrate= 0.1 error=99.035\n",">epoch= 30 lrate= 0.1 error=98.769\n",">epoch= 31 lrate= 0.1 error=98.517\n",">epoch= 32 lrate= 0.1 error=98.279\n",">epoch= 33 lrate= 0.1 error=98.054\n",">epoch= 34 lrate= 0.1 error=97.840\n",">epoch= 35 lrate= 0.1 error=97.637\n",">epoch= 36 lrate= 0.1 error=97.444\n",">epoch= 37 lrate= 0.1 error=97.260\n",">epoch= 38 lrate= 0.1 error=97.085\n",">epoch= 39 lrate= 0.1 error=96.918\n",">epoch= 40 lrate= 0.1 error=96.759\n",">epoch= 41 lrate= 0.1 error=96.607\n",">epoch= 42 lrate= 0.1 error=96.462\n",">epoch= 43 lrate= 0.1 error=96.324\n",">epoch= 44 lrate= 0.1 error=96.191\n",">epoch= 45 lrate= 0.1 error=96.064\n",">epoch= 46 lrate= 0.1 error=95.942\n",">epoch= 47 lrate= 0.1 error=95.825\n",">epoch= 48 lrate= 0.1 error=95.713\n",">epoch= 49 lrate= 0.1 error=95.606\n",">epoch= 50 lrate= 0.1 error=95.503\n",">epoch= 51 lrate= 0.1 error=95.403\n",">epoch= 52 lrate= 0.1 error=95.308\n",">epoch= 53 lrate= 0.1 error=95.216\n",">epoch= 54 lrate= 0.1 error=95.128\n",">epoch= 55 lrate= 0.1 error=95.043\n",">epoch= 56 lrate= 0.1 error=94.961\n",">epoch= 57 lrate= 0.1 error=94.883\n",">epoch= 58 lrate= 0.1 error=94.807\n",">epoch= 59 lrate= 0.1 error=94.733\n",">epoch= 60 lrate= 0.1 error=94.663\n",">epoch= 61 lrate= 0.1 error=94.594\n",">epoch= 62 lrate= 0.1 error=94.529\n",">epoch= 63 lrate= 0.1 error=94.465\n",">epoch= 64 lrate= 0.1 error=94.404\n",">epoch= 65 lrate= 0.1 error=94.344\n",">epoch= 66 lrate= 0.1 error=94.287\n",">epoch= 67 lrate= 0.1 error=94.231\n",">epoch= 68 lrate= 0.1 error=94.178\n",">epoch= 69 lrate= 0.1 error=94.126\n",">epoch= 70 lrate= 0.1 error=94.075\n",">epoch= 71 lrate= 0.1 error=94.027\n",">epoch= 72 lrate= 0.1 error=93.979\n",">epoch= 73 lrate= 0.1 error=93.934\n",">epoch= 74 lrate= 0.1 error=93.890\n",">epoch= 75 lrate= 0.1 error=93.847\n",">epoch= 76 lrate= 0.1 error=93.805\n",">epoch= 77 lrate= 0.1 error=93.765\n",">epoch= 78 lrate= 0.1 error=93.726\n",">epoch= 79 lrate= 0.1 error=93.688\n",">epoch= 80 lrate= 0.1 error=93.651\n",">epoch= 81 lrate= 0.1 error=93.615\n",">epoch= 82 lrate= 0.1 error=93.580\n",">epoch= 83 lrate= 0.1 error=93.547\n",">epoch= 84 lrate= 0.1 error=93.514\n",">epoch= 85 lrate= 0.1 error=93.482\n",">epoch= 86 lrate= 0.1 error=93.451\n",">epoch= 87 lrate= 0.1 error=93.421\n",">epoch= 88 lrate= 0.1 error=93.392\n",">epoch= 89 lrate= 0.1 error=93.364\n",">epoch= 90 lrate= 0.1 error=93.336\n",">epoch= 91 lrate= 0.1 error=93.309\n",">epoch= 92 lrate= 0.1 error=93.283\n",">epoch= 93 lrate= 0.1 error=93.258\n",">epoch= 94 lrate= 0.1 error=93.233\n",">epoch= 95 lrate= 0.1 error=93.209\n",">epoch= 96 lrate= 0.1 error=93.186\n",">epoch= 97 lrate= 0.1 error=93.163\n",">epoch= 98 lrate= 0.1 error=93.141\n",">epoch= 99 lrate= 0.1 error=93.119\n","Coefficient is:  [-6.801889126434203, 2.1647013790318548, 6.746103194307659, -1.3248159412138283, -0.07482884220795069, -0.4574898495561948, 3.9428600772520785, 2.4825371543502466, 0.20775107107500124]\n",">epoch= 0 lrate= 0.1 error=139.675\n",">epoch= 1 lrate= 0.1 error=133.310\n",">epoch= 2 lrate= 0.1 error=128.104\n",">epoch= 3 lrate= 0.1 error=123.885\n",">epoch= 4 lrate= 0.1 error=120.439\n",">epoch= 5 lrate= 0.1 error=117.593\n",">epoch= 6 lrate= 0.1 error=115.213\n",">epoch= 7 lrate= 0.1 error=113.198\n",">epoch= 8 lrate= 0.1 error=111.473\n",">epoch= 9 lrate= 0.1 error=109.981\n",">epoch= 10 lrate= 0.1 error=108.679\n",">epoch= 11 lrate= 0.1 error=107.532\n",">epoch= 12 lrate= 0.1 error=106.515\n",">epoch= 13 lrate= 0.1 error=105.608\n",">epoch= 14 lrate= 0.1 error=104.794\n",">epoch= 15 lrate= 0.1 error=104.058\n",">epoch= 16 lrate= 0.1 error=103.392\n",">epoch= 17 lrate= 0.1 error=102.784\n",">epoch= 18 lrate= 0.1 error=102.229\n",">epoch= 19 lrate= 0.1 error=101.719\n",">epoch= 20 lrate= 0.1 error=101.250\n",">epoch= 21 lrate= 0.1 error=100.817\n",">epoch= 22 lrate= 0.1 error=100.415\n",">epoch= 23 lrate= 0.1 error=100.042\n",">epoch= 24 lrate= 0.1 error=99.695\n",">epoch= 25 lrate= 0.1 error=99.371\n",">epoch= 26 lrate= 0.1 error=99.068\n",">epoch= 27 lrate= 0.1 error=98.784\n",">epoch= 28 lrate= 0.1 error=98.518\n",">epoch= 29 lrate= 0.1 error=98.268\n",">epoch= 30 lrate= 0.1 error=98.032\n",">epoch= 31 lrate= 0.1 error=97.810\n",">epoch= 32 lrate= 0.1 error=97.600\n",">epoch= 33 lrate= 0.1 error=97.401\n",">epoch= 34 lrate= 0.1 error=97.213\n",">epoch= 35 lrate= 0.1 error=97.035\n",">epoch= 36 lrate= 0.1 error=96.865\n",">epoch= 37 lrate= 0.1 error=96.704\n",">epoch= 38 lrate= 0.1 error=96.552\n",">epoch= 39 lrate= 0.1 error=96.406\n",">epoch= 40 lrate= 0.1 error=96.267\n",">epoch= 41 lrate= 0.1 error=96.135\n",">epoch= 42 lrate= 0.1 error=96.009\n",">epoch= 43 lrate= 0.1 error=95.888\n",">epoch= 44 lrate= 0.1 error=95.773\n",">epoch= 45 lrate= 0.1 error=95.662\n",">epoch= 46 lrate= 0.1 error=95.556\n",">epoch= 47 lrate= 0.1 error=95.455\n",">epoch= 48 lrate= 0.1 error=95.358\n",">epoch= 49 lrate= 0.1 error=95.265\n",">epoch= 50 lrate= 0.1 error=95.176\n",">epoch= 51 lrate= 0.1 error=95.090\n",">epoch= 52 lrate= 0.1 error=95.007\n",">epoch= 53 lrate= 0.1 error=94.928\n",">epoch= 54 lrate= 0.1 error=94.852\n",">epoch= 55 lrate= 0.1 error=94.778\n",">epoch= 56 lrate= 0.1 error=94.708\n",">epoch= 57 lrate= 0.1 error=94.640\n",">epoch= 58 lrate= 0.1 error=94.574\n",">epoch= 59 lrate= 0.1 error=94.511\n",">epoch= 60 lrate= 0.1 error=94.450\n",">epoch= 61 lrate= 0.1 error=94.391\n",">epoch= 62 lrate= 0.1 error=94.334\n",">epoch= 63 lrate= 0.1 error=94.279\n",">epoch= 64 lrate= 0.1 error=94.226\n",">epoch= 65 lrate= 0.1 error=94.175\n",">epoch= 66 lrate= 0.1 error=94.125\n",">epoch= 67 lrate= 0.1 error=94.077\n",">epoch= 68 lrate= 0.1 error=94.031\n",">epoch= 69 lrate= 0.1 error=93.986\n",">epoch= 70 lrate= 0.1 error=93.942\n",">epoch= 71 lrate= 0.1 error=93.900\n",">epoch= 72 lrate= 0.1 error=93.860\n",">epoch= 73 lrate= 0.1 error=93.820\n",">epoch= 74 lrate= 0.1 error=93.782\n",">epoch= 75 lrate= 0.1 error=93.745\n",">epoch= 76 lrate= 0.1 error=93.709\n",">epoch= 77 lrate= 0.1 error=93.674\n",">epoch= 78 lrate= 0.1 error=93.640\n",">epoch= 79 lrate= 0.1 error=93.608\n",">epoch= 80 lrate= 0.1 error=93.576\n",">epoch= 81 lrate= 0.1 error=93.545\n",">epoch= 82 lrate= 0.1 error=93.515\n",">epoch= 83 lrate= 0.1 error=93.486\n",">epoch= 84 lrate= 0.1 error=93.457\n",">epoch= 85 lrate= 0.1 error=93.430\n",">epoch= 86 lrate= 0.1 error=93.403\n",">epoch= 87 lrate= 0.1 error=93.377\n",">epoch= 88 lrate= 0.1 error=93.352\n",">epoch= 89 lrate= 0.1 error=93.327\n",">epoch= 90 lrate= 0.1 error=93.303\n",">epoch= 91 lrate= 0.1 error=93.280\n",">epoch= 92 lrate= 0.1 error=93.257\n",">epoch= 93 lrate= 0.1 error=93.235\n",">epoch= 94 lrate= 0.1 error=93.214\n",">epoch= 95 lrate= 0.1 error=93.193\n",">epoch= 96 lrate= 0.1 error=93.173\n",">epoch= 97 lrate= 0.1 error=93.153\n",">epoch= 98 lrate= 0.1 error=93.134\n",">epoch= 99 lrate= 0.1 error=93.115\n","Coefficient is:  [-6.673922792014459, 1.5709843363902447, 6.552047466239609, -1.4698007123757197, 0.08866655295370109, -0.04062905476497924, 4.2047635298653745, 1.3714099086359934, 1.0551852228377494]\n",">epoch= 0 lrate= 0.1 error=140.361\n",">epoch= 1 lrate= 0.1 error=133.814\n",">epoch= 2 lrate= 0.1 error=128.662\n",">epoch= 3 lrate= 0.1 error=124.548\n",">epoch= 4 lrate= 0.1 error=121.228\n",">epoch= 5 lrate= 0.1 error=118.506\n",">epoch= 6 lrate= 0.1 error=116.240\n",">epoch= 7 lrate= 0.1 error=114.324\n",">epoch= 8 lrate= 0.1 error=112.684\n",">epoch= 9 lrate= 0.1 error=111.262\n",">epoch= 10 lrate= 0.1 error=110.019\n",">epoch= 11 lrate= 0.1 error=108.921\n",">epoch= 12 lrate= 0.1 error=107.945\n",">epoch= 13 lrate= 0.1 error=107.071\n",">epoch= 14 lrate= 0.1 error=106.284\n",">epoch= 15 lrate= 0.1 error=105.571\n",">epoch= 16 lrate= 0.1 error=104.923\n",">epoch= 17 lrate= 0.1 error=104.331\n",">epoch= 18 lrate= 0.1 error=103.788\n",">epoch= 19 lrate= 0.1 error=103.288\n",">epoch= 20 lrate= 0.1 error=102.826\n",">epoch= 21 lrate= 0.1 error=102.399\n",">epoch= 22 lrate= 0.1 error=102.001\n",">epoch= 23 lrate= 0.1 error=101.632\n",">epoch= 24 lrate= 0.1 error=101.286\n",">epoch= 25 lrate= 0.1 error=100.963\n",">epoch= 26 lrate= 0.1 error=100.660\n",">epoch= 27 lrate= 0.1 error=100.376\n",">epoch= 28 lrate= 0.1 error=100.108\n",">epoch= 29 lrate= 0.1 error=99.856\n",">epoch= 30 lrate= 0.1 error=99.618\n",">epoch= 31 lrate= 0.1 error=99.393\n",">epoch= 32 lrate= 0.1 error=99.180\n",">epoch= 33 lrate= 0.1 error=98.978\n",">epoch= 34 lrate= 0.1 error=98.787\n",">epoch= 35 lrate= 0.1 error=98.605\n",">epoch= 36 lrate= 0.1 error=98.432\n",">epoch= 37 lrate= 0.1 error=98.267\n",">epoch= 38 lrate= 0.1 error=98.110\n",">epoch= 39 lrate= 0.1 error=97.961\n",">epoch= 40 lrate= 0.1 error=97.818\n",">epoch= 41 lrate= 0.1 error=97.681\n",">epoch= 42 lrate= 0.1 error=97.551\n",">epoch= 43 lrate= 0.1 error=97.426\n",">epoch= 44 lrate= 0.1 error=97.306\n",">epoch= 45 lrate= 0.1 error=97.192\n",">epoch= 46 lrate= 0.1 error=97.082\n",">epoch= 47 lrate= 0.1 error=96.977\n",">epoch= 48 lrate= 0.1 error=96.876\n",">epoch= 49 lrate= 0.1 error=96.778\n",">epoch= 50 lrate= 0.1 error=96.685\n",">epoch= 51 lrate= 0.1 error=96.595\n",">epoch= 52 lrate= 0.1 error=96.509\n",">epoch= 53 lrate= 0.1 error=96.426\n",">epoch= 54 lrate= 0.1 error=96.346\n",">epoch= 55 lrate= 0.1 error=96.269\n",">epoch= 56 lrate= 0.1 error=96.194\n",">epoch= 57 lrate= 0.1 error=96.122\n",">epoch= 58 lrate= 0.1 error=96.053\n",">epoch= 59 lrate= 0.1 error=95.987\n",">epoch= 60 lrate= 0.1 error=95.922\n",">epoch= 61 lrate= 0.1 error=95.860\n",">epoch= 62 lrate= 0.1 error=95.800\n",">epoch= 63 lrate= 0.1 error=95.741\n",">epoch= 64 lrate= 0.1 error=95.685\n",">epoch= 65 lrate= 0.1 error=95.631\n",">epoch= 66 lrate= 0.1 error=95.578\n",">epoch= 67 lrate= 0.1 error=95.527\n",">epoch= 68 lrate= 0.1 error=95.478\n",">epoch= 69 lrate= 0.1 error=95.430\n",">epoch= 70 lrate= 0.1 error=95.384\n",">epoch= 71 lrate= 0.1 error=95.339\n",">epoch= 72 lrate= 0.1 error=95.295\n",">epoch= 73 lrate= 0.1 error=95.253\n",">epoch= 74 lrate= 0.1 error=95.212\n",">epoch= 75 lrate= 0.1 error=95.172\n",">epoch= 76 lrate= 0.1 error=95.134\n",">epoch= 77 lrate= 0.1 error=95.097\n",">epoch= 78 lrate= 0.1 error=95.060\n",">epoch= 79 lrate= 0.1 error=95.025\n",">epoch= 80 lrate= 0.1 error=94.991\n",">epoch= 81 lrate= 0.1 error=94.957\n",">epoch= 82 lrate= 0.1 error=94.925\n",">epoch= 83 lrate= 0.1 error=94.893\n",">epoch= 84 lrate= 0.1 error=94.863\n",">epoch= 85 lrate= 0.1 error=94.833\n",">epoch= 86 lrate= 0.1 error=94.804\n",">epoch= 87 lrate= 0.1 error=94.776\n",">epoch= 88 lrate= 0.1 error=94.749\n",">epoch= 89 lrate= 0.1 error=94.722\n",">epoch= 90 lrate= 0.1 error=94.696\n",">epoch= 91 lrate= 0.1 error=94.671\n",">epoch= 92 lrate= 0.1 error=94.646\n",">epoch= 93 lrate= 0.1 error=94.622\n",">epoch= 94 lrate= 0.1 error=94.598\n",">epoch= 95 lrate= 0.1 error=94.576\n",">epoch= 96 lrate= 0.1 error=94.553\n",">epoch= 97 lrate= 0.1 error=94.532\n",">epoch= 98 lrate= 0.1 error=94.511\n",">epoch= 99 lrate= 0.1 error=94.490\n","Coefficient is:  [-6.395675028594732, 1.6805602810533564, 6.195412164349598, -1.7051319557157345, -0.22248405772536067, -0.6510539650429398, 4.201475973910789, 2.7431808776312776, 1.0544093887366686]\n",">epoch= 0 lrate= 0.1 error=140.244\n",">epoch= 1 lrate= 0.1 error=134.258\n",">epoch= 2 lrate= 0.1 error=129.500\n",">epoch= 3 lrate= 0.1 error=125.729\n",">epoch= 4 lrate= 0.1 error=122.696\n",">epoch= 5 lrate= 0.1 error=120.212\n",">epoch= 6 lrate= 0.1 error=118.137\n",">epoch= 7 lrate= 0.1 error=116.376\n",">epoch= 8 lrate= 0.1 error=114.860\n",">epoch= 9 lrate= 0.1 error=113.537\n",">epoch= 10 lrate= 0.1 error=112.373\n",">epoch= 11 lrate= 0.1 error=111.339\n",">epoch= 12 lrate= 0.1 error=110.415\n",">epoch= 13 lrate= 0.1 error=109.584\n",">epoch= 14 lrate= 0.1 error=108.833\n",">epoch= 15 lrate= 0.1 error=108.151\n",">epoch= 16 lrate= 0.1 error=107.529\n",">epoch= 17 lrate= 0.1 error=106.960\n",">epoch= 18 lrate= 0.1 error=106.439\n",">epoch= 19 lrate= 0.1 error=105.958\n",">epoch= 20 lrate= 0.1 error=105.515\n",">epoch= 21 lrate= 0.1 error=105.105\n",">epoch= 22 lrate= 0.1 error=104.725\n",">epoch= 23 lrate= 0.1 error=104.372\n",">epoch= 24 lrate= 0.1 error=104.043\n",">epoch= 25 lrate= 0.1 error=103.736\n",">epoch= 26 lrate= 0.1 error=103.449\n",">epoch= 27 lrate= 0.1 error=103.181\n",">epoch= 28 lrate= 0.1 error=102.929\n",">epoch= 29 lrate= 0.1 error=102.693\n",">epoch= 30 lrate= 0.1 error=102.471\n",">epoch= 31 lrate= 0.1 error=102.262\n",">epoch= 32 lrate= 0.1 error=102.065\n",">epoch= 33 lrate= 0.1 error=101.879\n",">epoch= 34 lrate= 0.1 error=101.703\n",">epoch= 35 lrate= 0.1 error=101.537\n",">epoch= 36 lrate= 0.1 error=101.379\n",">epoch= 37 lrate= 0.1 error=101.230\n",">epoch= 38 lrate= 0.1 error=101.088\n",">epoch= 39 lrate= 0.1 error=100.954\n",">epoch= 40 lrate= 0.1 error=100.826\n",">epoch= 41 lrate= 0.1 error=100.705\n",">epoch= 42 lrate= 0.1 error=100.589\n",">epoch= 43 lrate= 0.1 error=100.479\n",">epoch= 44 lrate= 0.1 error=100.374\n",">epoch= 45 lrate= 0.1 error=100.274\n",">epoch= 46 lrate= 0.1 error=100.179\n",">epoch= 47 lrate= 0.1 error=100.087\n",">epoch= 48 lrate= 0.1 error=100.000\n",">epoch= 49 lrate= 0.1 error=99.917\n",">epoch= 50 lrate= 0.1 error=99.837\n",">epoch= 51 lrate= 0.1 error=99.761\n",">epoch= 52 lrate= 0.1 error=99.688\n",">epoch= 53 lrate= 0.1 error=99.618\n",">epoch= 54 lrate= 0.1 error=99.550\n",">epoch= 55 lrate= 0.1 error=99.486\n",">epoch= 56 lrate= 0.1 error=99.424\n",">epoch= 57 lrate= 0.1 error=99.365\n",">epoch= 58 lrate= 0.1 error=99.308\n",">epoch= 59 lrate= 0.1 error=99.253\n",">epoch= 60 lrate= 0.1 error=99.200\n",">epoch= 61 lrate= 0.1 error=99.150\n",">epoch= 62 lrate= 0.1 error=99.101\n",">epoch= 63 lrate= 0.1 error=99.054\n",">epoch= 64 lrate= 0.1 error=99.009\n",">epoch= 65 lrate= 0.1 error=98.966\n",">epoch= 66 lrate= 0.1 error=98.924\n",">epoch= 67 lrate= 0.1 error=98.883\n",">epoch= 68 lrate= 0.1 error=98.844\n",">epoch= 69 lrate= 0.1 error=98.807\n",">epoch= 70 lrate= 0.1 error=98.771\n",">epoch= 71 lrate= 0.1 error=98.736\n",">epoch= 72 lrate= 0.1 error=98.702\n",">epoch= 73 lrate= 0.1 error=98.670\n",">epoch= 74 lrate= 0.1 error=98.638\n",">epoch= 75 lrate= 0.1 error=98.608\n",">epoch= 76 lrate= 0.1 error=98.578\n",">epoch= 77 lrate= 0.1 error=98.550\n",">epoch= 78 lrate= 0.1 error=98.523\n",">epoch= 79 lrate= 0.1 error=98.496\n",">epoch= 80 lrate= 0.1 error=98.471\n",">epoch= 81 lrate= 0.1 error=98.446\n",">epoch= 82 lrate= 0.1 error=98.422\n",">epoch= 83 lrate= 0.1 error=98.399\n",">epoch= 84 lrate= 0.1 error=98.376\n",">epoch= 85 lrate= 0.1 error=98.354\n",">epoch= 86 lrate= 0.1 error=98.333\n",">epoch= 87 lrate= 0.1 error=98.313\n",">epoch= 88 lrate= 0.1 error=98.293\n",">epoch= 89 lrate= 0.1 error=98.274\n",">epoch= 90 lrate= 0.1 error=98.255\n",">epoch= 91 lrate= 0.1 error=98.237\n",">epoch= 92 lrate= 0.1 error=98.220\n",">epoch= 93 lrate= 0.1 error=98.203\n",">epoch= 94 lrate= 0.1 error=98.186\n",">epoch= 95 lrate= 0.1 error=98.170\n",">epoch= 96 lrate= 0.1 error=98.155\n",">epoch= 97 lrate= 0.1 error=98.140\n",">epoch= 98 lrate= 0.1 error=98.125\n",">epoch= 99 lrate= 0.1 error=98.111\n","Coefficient is:  [-6.422434823861371, 1.6825119653350986, 6.160564394163588, -1.1096192037566472, 0.04282061536569483, -0.5740016376031059, 3.7827769854778546, 1.8109191595187322, 0.6832228641363485]\n",">epoch= 0 lrate= 0.1 error=140.437\n",">epoch= 1 lrate= 0.1 error=134.368\n",">epoch= 2 lrate= 0.1 error=129.240\n",">epoch= 3 lrate= 0.1 error=125.134\n",">epoch= 4 lrate= 0.1 error=121.817\n",">epoch= 5 lrate= 0.1 error=119.096\n",">epoch= 6 lrate= 0.1 error=116.828\n",">epoch= 7 lrate= 0.1 error=114.910\n",">epoch= 8 lrate= 0.1 error=113.266\n",">epoch= 9 lrate= 0.1 error=111.839\n",">epoch= 10 lrate= 0.1 error=110.589\n",">epoch= 11 lrate= 0.1 error=109.483\n",">epoch= 12 lrate= 0.1 error=108.497\n",">epoch= 13 lrate= 0.1 error=107.612\n",">epoch= 14 lrate= 0.1 error=106.813\n",">epoch= 15 lrate= 0.1 error=106.087\n",">epoch= 16 lrate= 0.1 error=105.425\n",">epoch= 17 lrate= 0.1 error=104.818\n",">epoch= 18 lrate= 0.1 error=104.260\n",">epoch= 19 lrate= 0.1 error=103.745\n",">epoch= 20 lrate= 0.1 error=103.268\n",">epoch= 21 lrate= 0.1 error=102.825\n",">epoch= 22 lrate= 0.1 error=102.412\n",">epoch= 23 lrate= 0.1 error=102.027\n",">epoch= 24 lrate= 0.1 error=101.667\n",">epoch= 25 lrate= 0.1 error=101.329\n",">epoch= 26 lrate= 0.1 error=101.011\n",">epoch= 27 lrate= 0.1 error=100.713\n",">epoch= 28 lrate= 0.1 error=100.432\n",">epoch= 29 lrate= 0.1 error=100.166\n",">epoch= 30 lrate= 0.1 error=99.915\n",">epoch= 31 lrate= 0.1 error=99.677\n",">epoch= 32 lrate= 0.1 error=99.452\n",">epoch= 33 lrate= 0.1 error=99.239\n",">epoch= 34 lrate= 0.1 error=99.036\n",">epoch= 35 lrate= 0.1 error=98.843\n",">epoch= 36 lrate= 0.1 error=98.660\n",">epoch= 37 lrate= 0.1 error=98.485\n",">epoch= 38 lrate= 0.1 error=98.319\n",">epoch= 39 lrate= 0.1 error=98.160\n",">epoch= 40 lrate= 0.1 error=98.008\n",">epoch= 41 lrate= 0.1 error=97.863\n",">epoch= 42 lrate= 0.1 error=97.724\n",">epoch= 43 lrate= 0.1 error=97.592\n",">epoch= 44 lrate= 0.1 error=97.465\n",">epoch= 45 lrate= 0.1 error=97.343\n",">epoch= 46 lrate= 0.1 error=97.226\n",">epoch= 47 lrate= 0.1 error=97.114\n",">epoch= 48 lrate= 0.1 error=97.007\n",">epoch= 49 lrate= 0.1 error=96.904\n",">epoch= 50 lrate= 0.1 error=96.805\n",">epoch= 51 lrate= 0.1 error=96.710\n",">epoch= 52 lrate= 0.1 error=96.618\n",">epoch= 53 lrate= 0.1 error=96.530\n",">epoch= 54 lrate= 0.1 error=96.445\n",">epoch= 55 lrate= 0.1 error=96.364\n",">epoch= 56 lrate= 0.1 error=96.285\n",">epoch= 57 lrate= 0.1 error=96.209\n",">epoch= 58 lrate= 0.1 error=96.136\n",">epoch= 59 lrate= 0.1 error=96.066\n",">epoch= 60 lrate= 0.1 error=95.998\n",">epoch= 61 lrate= 0.1 error=95.932\n",">epoch= 62 lrate= 0.1 error=95.869\n",">epoch= 63 lrate= 0.1 error=95.807\n",">epoch= 64 lrate= 0.1 error=95.748\n",">epoch= 65 lrate= 0.1 error=95.691\n",">epoch= 66 lrate= 0.1 error=95.636\n",">epoch= 67 lrate= 0.1 error=95.582\n",">epoch= 68 lrate= 0.1 error=95.531\n",">epoch= 69 lrate= 0.1 error=95.481\n",">epoch= 70 lrate= 0.1 error=95.432\n",">epoch= 71 lrate= 0.1 error=95.385\n",">epoch= 72 lrate= 0.1 error=95.340\n",">epoch= 73 lrate= 0.1 error=95.296\n",">epoch= 74 lrate= 0.1 error=95.253\n",">epoch= 75 lrate= 0.1 error=95.212\n",">epoch= 76 lrate= 0.1 error=95.172\n",">epoch= 77 lrate= 0.1 error=95.133\n",">epoch= 78 lrate= 0.1 error=95.095\n",">epoch= 79 lrate= 0.1 error=95.058\n",">epoch= 80 lrate= 0.1 error=95.023\n",">epoch= 81 lrate= 0.1 error=94.988\n",">epoch= 82 lrate= 0.1 error=94.955\n",">epoch= 83 lrate= 0.1 error=94.922\n",">epoch= 84 lrate= 0.1 error=94.891\n",">epoch= 85 lrate= 0.1 error=94.860\n",">epoch= 86 lrate= 0.1 error=94.830\n",">epoch= 87 lrate= 0.1 error=94.801\n",">epoch= 88 lrate= 0.1 error=94.773\n",">epoch= 89 lrate= 0.1 error=94.746\n",">epoch= 90 lrate= 0.1 error=94.719\n",">epoch= 91 lrate= 0.1 error=94.693\n",">epoch= 92 lrate= 0.1 error=94.668\n",">epoch= 93 lrate= 0.1 error=94.643\n",">epoch= 94 lrate= 0.1 error=94.619\n",">epoch= 95 lrate= 0.1 error=94.596\n",">epoch= 96 lrate= 0.1 error=94.573\n",">epoch= 97 lrate= 0.1 error=94.551\n",">epoch= 98 lrate= 0.1 error=94.530\n",">epoch= 99 lrate= 0.1 error=94.509\n","Coefficient is:  [-6.81181410819999, 1.8048086975898594, 5.902860431807605, -1.2554234861379419, -0.35396334002563873, 0.009027265540986894, 4.808752509002328, 2.5152850789671186, 0.2462577225727147]\n","Average accuracy of logistic regression =  76.5690518631695  Average accuracy of ZeroR =  65.10482981071216\n","The minimun accuracy of Logistic Regression =  72.72727272727273 The minimun accuracy of ZeroR =  64.93506493506493\n","The maximun accuracy of Logistic Regression =  82.35294117647058 The maximun accuracy of ZeroR =  65.359477124183\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pIsAhJzeiq5k"},"source":["Compare with the un-stratified run, we have that the minimun scores of both logistic regression and zeroR are bigger than those in un-stratified run.\n","\n","The maximun score of zeroR in stratified run is lower than that in un-stratified run. And the maximun score of logistic regression in stratified run is similar to that in un-stratified run.\n","\n","The average scores of logistic regression and zeroR in un-stratified run and stratified run are similar.\n","\n","Overall, the difference between maximun and minimun in stratified run is lower than that in un-stratified run."]},{"cell_type":"markdown","metadata":{"id":"Ro_lmHCDVdGK"},"source":[""]}]}
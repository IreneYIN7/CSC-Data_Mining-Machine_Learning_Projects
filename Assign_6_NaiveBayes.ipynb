{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Irene CSC-321 Assignment 6_NaiveBayes.ipynb","provenance":[{"file_id":"1m4yhSEYv1K8J7vE6MQPHGTW1izTRFfpP","timestamp":1620850460674},{"file_id":"1PAsVQWvWfcol21nUIt6gbqA0TliOdqVV","timestamp":1588784443848}],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"42lZmjtvkK4U"},"source":["# CSC-321: Data Mining and Machine Learning\n","# Irene Yin\n","## Assignment 6 Classification with probability\n","\n","### Part 1: Naive Bayes\n","\n","Everything so far has been a linear classifier. Now we'll change gears, and implement some non-linear classifiers. The first, as we saw in class, is Naive Bayes, which makes use of probability to make predictions.\n","\n","We use Bayes' Theorem which allows us to calculate the probability of a piece of data belonging to a given class, given our prior knowledge. Bayes' Theorem is stated as:\n","\n","$$P(class|data) =\\frac{(P(data|class) * P(class))}{P(data)}$$\n","\n","\n","Where P(class|data) is the probability of class given the provided data\n","\n","We're going to break this down into several steps. Again, I've given you an initial contrived data set for you to test your functions. MAKE SURE you verify that you're getting the correct output. Use the slides and the example in class to overstand the overview of how naive bayes works."]},{"cell_type":"markdown","metadata":{"id":"8lk5biXykK4V"},"source":["Conceptually straightforward, there's still a lot to do to get this to work, and I'm going to take the problem bottom up. In order to understand the whole process, I recommend reading EVERYTHING before you start, to make sure you get why we're doing things this way.\n","\n","Ultimately, we're going to do naive bayes prediction on an instance with numeric feature values. That means we need to calculate the gaussian probability density, and as you remember from class, that means we need to gather several statistics on a feature by feature basis. As you also remember, we need to calcualte the probabilities for all the classes. \n","\n","In the early steps, we'll be dealing with TWO input features and TWO class values (0 and 1) BUT DO NOT make any assumptions about the number of input features OR the number of class values.\n","\n","#### (a) Separate by class\n","\n","Just as in the slides, we need to calculate the probability of data by the class they belong to. We're going to do this in stages. Read all of the steps before coding. First, we need to separate our data by the class values. \n","\n","Our dataset contains TWO input features, and one class value. We'll assume (as we often do) that the LAST value of each instance is the class value.\n","\n","Taking our dataset, create two variables, called X_values (containing a list of lists of input values) and y_values (a list of the class values). Pass these to a function that will create and return a dictionary, where the key is a class value, and the corresponding dictionary value is a list of all input instances with that class value. "]},{"cell_type":"code","metadata":{"id":"mN7qTGH3kK4W","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620993484807,"user_tz":240,"elapsed":870,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"804014ee-4948-4b15-b049-d5aa0db22855"},"source":["# Contrived data set\n","\n","dataset = [[3.393533211,2.331273381,0],\n","    [3.110073483,1.781539638,0],\n","    [1.343808831,3.368360954,0],\n","    [3.582294042,4.67917911,0],\n","    [2.280362439,2.866990263,0],\n","    [7.423436942,4.696522875,1],\n","    [5.745051997,3.533989803,1],\n","    [9.172168622,2.511101045,1],\n","    [7.792783481,3.424088941,1],\n","    [7.939820817,0.791637231,1]]\n","\n","X_values = [i[:-1] for i in dataset]\n","y_values = [i[-1] for i in dataset]\n","\n","def SeperateData(X_value, y_value):\n","  # return: a dictionary, where the key is a class value, and the corresponding dictionary value \n","  #         is a list of all input instances with that class value.\n","  Dictionary = {}\n","  for index in range(len(y_value)):\n","    key = y_value[index]\n","    if key not in Dictionary:\n","      Dictionary[key] = [X_value[index]]\n","    else:\n","      Dictionary[key].append(X_value[index])\n","  return Dictionary\n","\n","Dict = SeperateData(X_values, y_values)\n","print(Dict)\n","print(\"0 class: \", Dict[0])\n","print(\"1 class: \", Dict[1])\n","\n","\n","\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["{0: [[3.393533211, 2.331273381], [3.110073483, 1.781539638], [1.343808831, 3.368360954], [3.582294042, 4.67917911], [2.280362439, 2.866990263]], 1: [[7.423436942, 4.696522875], [5.745051997, 3.533989803], [9.172168622, 2.511101045], [7.792783481, 3.424088941], [7.939820817, 0.791637231]]}\n","0 class:  [[3.393533211, 2.331273381], [3.110073483, 1.781539638], [1.343808831, 3.368360954], [3.582294042, 4.67917911], [2.280362439, 2.866990263]]\n","1 class:  [[7.423436942, 4.696522875], [5.745051997, 3.533989803], [9.172168622, 2.511101045], [7.792783481, 3.424088941], [7.939820817, 0.791637231]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"zElUAIq3kK4a"},"source":["#### (b) Summarize the data\n","\n","We are going to need two statistics from the data, the mean and the standard deviation. You should have the base of these functions in a previous assignment, remembering that the standard deviation is simply the square root of the variance. \n","\n","For this assignment, we need to compute the **average variance**, not the total variance as we did with SLR. Also, we're using a **SAMPLE** and not a **POPULATION**. You should know what that means with respect to what we divide by. Refer to the slides on SLR if you don't remember. \n","\n","In the contrived data, we have 3 features - 2 input features and an output feature, y.\n","\n","We need the mean and standard deviation for each of our input features. Create a function that summarizes a given set of X_values instances, by calculating the mean and standard deviation on that data. We'll collect this information into a tuple, one per column, comprising the mean, the standard deviation and the number of elements in each column). Return a list of these tuples.\n","\n","REMEMBER: Do NOT do anything that relies on the fact that are only 2 input features in this data. \n","\n","More generally, the output should be:\n","\n","[(feature1_mean,feature1_std,feature1_count), (feature2_mean,feature2_std,feature2_count),....,(featureN_mean,featureN_std,featureN_count)]\n"]},{"cell_type":"code","metadata":{"id":"wgpHs0sGkK4a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620993485226,"user_tz":240,"elapsed":1280,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"b79977d0-22b0-4fea-d51a-7ac2c2e7c74b"},"source":["# implement your function here, and copy across any functions you need to help you\n","\n","def getMean(inputList):\n","    total = 0\n","    for num in inputList:\n","        total += num\n","    return total/len(inputList)\n","\n","def getVariance(inputList):\n","  # return: the total variance of a list of values.\n","  meanValue = getMean(inputList)\n","  variance = 0\n","  for num in inputList:\n","    variance += (num-meanValue)**2\n","    aver_variance = variance/(len(inputList)-1)\n","  return aver_variance\n","\n","def summarizeData(X_values):\n","  dataSummary = []\n","  for col in range(len(X_values[0])):\n","    currentcol = []\n","    for instance in X_values:\n","      currentcol.append(instance[col])\n","    mean = getMean(currentcol)\n","    Samplevariance = getVariance(currentcol)\n","    stadDevi = Samplevariance**0.5\n","    dataSummary.append((mean, stadDevi, len(currentcol)))\n","  return dataSummary\n","\n","print(summarizeData(X_values))\n","\n","\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[(5.178333386499999, 2.7665845055177263, 10), (2.9984683241, 1.218556343617447, 10)]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sFmFpy8ukK4d"},"source":["#### (c) Summarize data by class\n","\n","We now need to combine the functions from (a) and (b) above. Create a  function, that takes X_values and y_values. Your function should split the data by class using your function from (a). \n","\n","It then calculates statistics for all instances of the data for each class using (b), getting a summary of the feature values for each feature, for each class.\n","\n","The results - the list of tuples of statistics, one per column - should then be stored in a dictionary by their class value. summarizeByClass should return such a dictionary. I include my output on the included dataset for verification. "]},{"cell_type":"code","metadata":{"id":"hZgK1NxYkK4e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620993485227,"user_tz":240,"elapsed":1275,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"37368eb3-a4d4-4d43-e71b-1aa9b46d63f8"},"source":["\n","# implement your function here\n","def summarizeByClass(X_value, y_value):\n","  #return: a dictionary which stored the class value and the summary of features values for each feature.\n","  Dict = SeperateData(X_value, y_value)\n","  SummaryDict = {}\n","  for Class in Dict:\n","    Classdata = Dict[Class]\n","    dataStatistic = summarizeData(Classdata)\n","    SummaryDict[Class] = dataStatistic\n","  return SummaryDict\n","\n","# The output dictionary for the contrived data should look like:\n","# {0: [(2.7420144012, 0.9265683289298018, 5), (3.0054686692, 1.1073295894898725, 5)], 1: [(7.6146523718, 1.2344321550313704, 5), (2.9914679790000003, 1.4541931384601618, 5)]}\n","print(summarizeByClass(X_values, y_values))\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["{0: [(2.7420144012, 0.9265683289298018, 5), (3.0054686692, 1.1073295894898725, 5)], 1: [(7.6146523718, 1.2344321550313704, 5), (2.9914679790000003, 1.4541931384601618, 5)]}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tcAyj5fgkK4i"},"source":["#### (d) Guassiaun Probability Density\n","\n","We're working with features that contain numerical rather than nominal data here, so we need to implement the gaussian probability density function (PDF) we talked about in class, so we can attach probabilities to real values. A gaussian distribution can be summarized from two values - mean and standard deviation. The gaussian PDF is calculated as follows:\n","\n","\n","$$probability(x) = \\frac{1}{\\sqrt{ 2 * \\pi } * \\sigma}*e^-(\\frac{(x-mean(x))^2}{2 * \\sigma^2})$$\n","\n","Where:\n","- sigma is the standard deviation\n","- e is Euler's number (math.exp(x)) \n","\n","Hopefully, you can see why we're going to need the mean and the std_dev from function (c). I find it incredibly helpful to calculate the above equation in stages - i.e. calculate the first part of the equation and the exponent separately, then combine them at the end.\n","\n","Create a function that:\n","- takes a value (x)\n","- takes a mean\n","- takes a standard deviation\n","\n","and returns the probability of seeing that value, using the formula above."]},{"cell_type":"code","metadata":{"id":"fto11aKikK4j","executionInfo":{"status":"ok","timestamp":1620993485227,"user_tz":240,"elapsed":1274,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}}},"source":["import math\n","# Implement your function here\n","\n","def GaussianPDF(value, mean, stadDevi):\n","  # return: the probability of seeing that value, using the fomula above.\n","  firstPart = 1/(math.sqrt(2*math.pi)*stadDevi)\n","  exponent = -(((value - mean)**2)/(2*stadDevi **2))\n","  probability = firstPart*math.exp(exponent)\n","  return probability\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LcFcLnQ0kK4m"},"source":["#### (e) Class Probabilities\n","\n","We can now use probabilites from our training data to calculate the class probabilities for any instance of new data, by creating a function called calculateProbabilities. \n","\n","Probabilites have to be calculated separately for each possible class value; for each class value we have to calculate the likelihood that the new instance belongs to that class value. This is exactly what I work through on the slides, so refer to those if it's helpful. The probability that a piece of data belongs to a class value is calculated by:\n","\n","$$P(class|data) =(P(data|class) * P(class))$$\n","\n","\n","The divison from Bayes' Theorem has been removed, because we're just trying to *maximize* the result of the formula above. The largest value we get for a class value determines which class value we assign. In the case where we have TWO input features in our data (X1 and X2), the probablility that an instance belongs to class value 0 is calculated by:\n","\n","$$P(class=0|X_{1},X_{2}) = P(X_{1}|class=0) * P(X_{2}|class=0) * P(class=0)$$\n","\n","We have to repeat this for each class value, and then choose the class value with the highest score. We should not assume a fixed number of input features, X, the above was just an illustration. More generally it is:\n","\n","$$P(class=0|X_{1},X_{2}) = P(X_{1}|class=0) * P(X_{2}|class=0) *  ... * P(X_{N}|class=0) * P(class=0)$$\n","\n","We'll start by creating a function that will return the probabilities of predicting each class value for a given instance. \n","\n","This function will take a dictionary of summaries (as returned by (c), above) and an instance, and will generate a dictionary of probabilites, with one entry per class. The steps are as follows:\n","\n","- We need to calculate the total number of training instances, by counting the counts stored in the summary statistics. So if there are 9 instances with one label, and 5 with another (as in the weather data) then we need to know there are 14 instances total. Thankfully, in the data from function (c), we know where to find those counts for each class. \n","\n","- This will help us calculate the prior probability, P(class), as the ratio of rows with a given class divided by all rows in the training data\n","\n","- Next probabilities are calculated for each feature value in the instance to be classified. Using the function from (d), we can generate a probability of associating a feature value with a class value. Those probabilites are multiplied together as they are accumulated with the formula given above. \n","\n","- The process is repeated for each class in the data\n","\n","- Return the dictionary of probabilities for each class for the new instance\n","\n","Some things that might help with implementation. \n","\n","- Dictionaries are your friend here\n","- List comprehensions are also your friend\n","- The data returned by (c) above is already divided by class. You can:\n","    - discover the prior probability from this data (how many instances for this class, divided by the total instances)\n","    - iterate over the tuples, which give you the information (mean, std_dev, count) on a per column basis\n","    - calculate probability given the attribute value corresponding to that column using your function from (d)\n","\n","Try this out on the contrived data. You should be able to calculate all the relevant scores by hand to determine if your code is accurate.\n","\n","NOTE: If you want to output ACTUAL probabilities by class, we divide each score in the dictionary for an instance, by the sum of the values. You don't need to do this, as I've included code to do it for you.\n"]},{"cell_type":"code","metadata":{"id":"bhH_yTcTkK4n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620993485648,"user_tz":240,"elapsed":1689,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"8fe381d5-83bb-467e-e0f8-3156304beb76"},"source":["\n","# Implement your function here\n","\n","def calculateProbabilities(summaries, instance):\n","    #return: the probabilities of predicting each class value for a given instance.\n","    probDict = {}\n","    totalinstance = 0\n","    for classValue in summaries:\n","        totalinstance += summaries[classValue][0][-1]\n","    for classValue in summaries:\n","        priorProb = summaries[classValue][0][-1]/totalinstance\n","        classProb = priorProb\n","        for i in range(len(summaries[classValue])):\n","            mean = summaries[classValue][i][0]\n","            stadDevi = summaries[classValue][i][1]\n","            instanceProb = GaussianPDF(instance[i], mean, stadDevi)\n","            classProb *= instanceProb\n","        probDict[classValue] = classProb\n","    return probDict\n","\n","# Test it out here\n","# Get a bunch of summaries from your function from (c)\n","# Pass those summaries (a dictionary) to your function from (e)\n","# in order to predict the class of the test instance, below.\n","# PRINT OUT the dictionary returned by calculateProbabilities\n","# And compare to my score, below\n","\n","testInstance = X_values[0]\n","summaries = summarizeByClass(X_values,y_values)\n","probabilities = calculateProbabilities(summaries, testInstance)\n","print('Probabilities are: ',probabilities)\n","\n","# I think if everything works, the returned value from (e) should be:\n","# {0: 0.05032427673372075, 1: 0.00011557718379945765}\n","# which according to the percentage calculation given should be:\n","# 99.77% in favour of class 0 \n","# Code to compute this is given below, assuming you store the dictionary returned\n","# from calculateProbabilities in a variable called probabilities\n","\n","print()\n","sumProbs = sum([value for _,value in probabilities.items()])\n","for key,value in probabilities.items():\n","    normProb = value / sumProbs * 100\n","    print('The probability of the instance belonging to class {} is {:.2f}'.format(key,normProb))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Probabilities are:  {0: 0.05032427673372076, 1: 0.00011557718379945765}\n","\n","The probability of the instance belonging to class 0 is 99.77\n","The probability of the instance belonging to class 1 is 0.23\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ets2xIjTkK4q"},"source":["#### (f) Tying it all together\n","\n","You need to create a predict function. This function works very much as the example above, in that it takes a dictionary of summaries and a single row, and uses calculateProbabilites to get the resulting dictionary of probabilities. From this dictionary, find the largest value and corresponding class. Return this class. \n","\n","You also need a naiveBayes function, that takes a training set (X,train, y_train) and a test set (X_test). It needs to generate summary statistics from the training set (using (c), above), then make predictions for each instance in the test set, by calling your predict function for each instance, using the summaries generated. Append these predictions to a list you return.\n","\n","Print out the list of actual values and predicted values, for now using the same data for training and testing."]},{"cell_type":"code","metadata":{"id":"-T9_e7XokK4q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620993485649,"user_tz":240,"elapsed":1683,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"5009d89c-970d-4ce9-8a6b-45eb0bbaa077"},"source":["import operator\n","# Implement predict(summaries,instance) here\n","\n","def predict(summaries,instance):\n","  # return: find the largest value and return corresponding class\n","  ProbDict = calculateProbabilities(summaries,instance)\n","  return max(ProbDict.items(), key=operator.itemgetter(1))[0]\n","\n","# Implement naive_bayes(X_train,y_train,X_test) here\t\n","def naiveBayes(X_train,y_train, X_test):\n","    classSummary = summarizeByClass(X_train, y_train)\n","    return [predict(classSummary, instance) for instance in X_test]\n","\n","X_train = X_values\n","X_test = X_train\n","y_train = y_values\n","y_test = y_train\n","\n","NBpredict = naiveBayes(X_train, y_train, X_test)\n","print(\"The actural values: \", y_test)\n","print(\"The predicted values: \", NBpredict)\n","\n","\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":["The actural values:  [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n","The predicted values:  [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZXjC4ZLYkK4u"},"source":["### Part 2: Applying to real data\n","\n","You've seen bits of the iris dataset in class. It's one of the most well known data sets in machine learning and data mining. So you might as well have a go at it! You can find out more about it here: https://en.wikipedia.org/wiki/Iris_flower_data_set\n","\n","I don't need to use pandas to load the data, as the iris data set is included with scikit learn. Below, I'm going to load the data for you. You have to:\n","\n","- call your naive bayes algorithm, using a Stratified 5-fold cross-validation\n","- compare this to some reasonable baseline, using your code\n","- give me a short write up of the results\n","\n","This will require checking back at how I did things last week, with respect to doing a Stratified 5-fold cross-validation."]},{"cell_type":"code","metadata":{"id":"4mkDk8XdvCFI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620993485649,"user_tz":240,"elapsed":1678,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"e650351d-474c-4d47-c211-6fc1ddbc1cd9"},"source":["# Do part 2 here\n","\n","from sklearn import datasets\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import plot_confusion_matrix\n","import matplotlib.pyplot as plt\n","\n","iris = datasets.load_iris()\n","X_values = iris.data\n","y_values = iris.target\n","\n","rows,cols = X_values.shape\n","print(\"This is the IRIS training data set. It has\", rows, \"instances, and it has\", cols, \"features.\\n\")\n","\n","# The below is only to show you what the class values are\n","# You do NOT need to use class_values anywhere\n","\n","X_train = X_values\n","X_test = X_train\n","y_train = y_values\n","y_test = y_train\n","\n","class_values = set(y_values)\n","print(\"The IRIS test data class has the folowing values:\", class_values, \"\\n\")\n","\n","# baseline,ZeroRC\n","def zeroRC(train, test):\n","  # return baseline\n","  commonElement = max(set(train), key = train.count)\n","  return [commonElement for i in range(len(test))]\n","\n","ZeroR_predict = zeroRC(y_train.tolist(), X_test)\n","\n","# naive bayes, using a Stratified 5-fold cross-validation\n","def accuracy(actual_value, predicted_value):\n","  # return how many times the function predicts correctly in ratio.\n","  counter = 0 \n","  for i in range(len(actual_value)):\n","    if actual_value[i] == predicted_value[i]:\n","      counter += 1\n","  return counter/len(actual_value)\n","\n","\n","skf = StratifiedKFold(n_splits=5)\n","NB_scores = []\n","zr_scores = []\n","for train_index, test_index in skf.split(X_values,y_values):\n","    X_train, X_test = X_values[train_index], X_values[test_index]\n","    y_train, y_test = y_values[train_index], y_values[test_index]\n","\n","    NB_pred = naiveBayes(X_train, y_train, X_test)\n","    zeroR_pred = zeroRC(y_train.tolist(),X_test)\n","\n","    accuracy_NB = accuracy(y_test, NB_pred) * 100\n","    NB_scores.append(accuracy_NB)\n","\n","    accuracy_zeroRC = accuracy(y_test, zeroR_pred) * 100\n","    zr_scores.append(accuracy_zeroRC)\n","print('Naive Bayes score:', NB_scores)\n","print('zeroR score: ', zr_scores)\n","NB_average_accuracy = sum(NB_scores)/len(NB_scores)\n","zr_average_accuracy = sum(zr_scores)/len(zr_scores)\n","\n","print()\n","print(\"Stratified 5-Fold Cross-Validation\")\n","print()\n","\n","\n","print('Naive Bayes accuracy: {:.2f}% ({:.2f}% / {:.2f}%)'.format(NB_average_accuracy, min(NB_scores), max(NB_scores)))\n","\n","print('ZeroR accuracy: {:.2f}% ({:.2f}% / {:.2f}%)'.format(zr_average_accuracy, min(zr_scores), max(zr_scores)))\n","\n","\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["This is the IRIS training data set. It has 150 instances, and it has 4 features.\n","\n","The IRIS test data class has the folowing values: {0, 1, 2} \n","\n","Naive Bayes score: [93.33333333333333, 96.66666666666667, 93.33333333333333, 93.33333333333333, 100.0]\n","zeroR score:  [33.33333333333333, 33.33333333333333, 33.33333333333333, 33.33333333333333, 33.33333333333333]\n","\n","Stratified 5-Fold Cross-Validation\n","\n","Naive Bayes accuracy: 95.33% (93.33% / 100.00%)\n","ZeroR accuracy: 33.33% (33.33% / 33.33%)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UHEenm4FFUwC"},"source":["According to the accuracy, we have that the baseline is 33.33%, which does make sense because we are using stratified 5-fold cross-validation where we are faily distributing 150 instances into 5 groups where each group has same number of instance in each class. The baseline is 33.33% means that there is 33.33% chance that the prediction of which class an instance belongs is correct. \n","\n","Then we have the accuracy of Naive Bayes is 95.33%, which is much higher than the baseline. The naive Bayes performs very well. The prediction of which type of iris flower by using Naive bayes has 95.33% chance that is correct. This also make sense because Naive Bayes doesn't require accurate probability estimates as long as maximun probability is assigned to correct class. And also, each class in this iris flower dataset has a very decentralized distributions where they don't overlap a lot.  \n","\n","We cannot determine which features are the more important since NB is considering the probability from all the features. We cannot seperatly know which features is more important just based on NB."]},{"cell_type":"markdown","metadata":{"id":"Whpk2fJ7D54w"},"source":["## Part 3: Introduction to scikit-learn\n","\n","One of the most popular open-source python machine learning libraries is scikit-learn.\n","\n","As we go through this class I'll introduce you to some of the functionality. Below I want you to use Gaussian Naive Bayes.\n","\n","You can find out more in general at: https://scikit-learn.org/stable/index.html\n","\n","This time, I'm only doing the bare minimum. I'll load the relevant models from scikit-learn, but it's up to you to train and test them, and report the scores appropriately. Your scores should be the same as your code, above.\n","\n","You need to:\n","- Use [gaussian naive bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n","- Use a reasonable baseline\n","- Use a reasonable measure of performance\n","- Perform a stratified 5 fold cross validation\n","- Collect the results\n","- Print the mean, the min and the max of each cross-validation experiment\n"]},{"cell_type":"code","metadata":{"id":"8HZZ-JpqkK4u","colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"status":"ok","timestamp":1620993485650,"user_tz":240,"elapsed":1673,"user":{"displayName":"Zhebin Yin","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiTfiuQnAmcQ204at7eIRExK27--n1fVuv6wi6C=s64","userId":"00932536702020326795"}},"outputId":"8efb9904-e409-4794-876f-e89999ed6b0d"},"source":["from sklearn.dummy import DummyClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","import matplotlib.pyplot as plt\n","import numpy as np\n","#GNB\n","GNB_clf = GaussianNB()\n","\n","# ZeroR\n","zr_clf = DummyClassifier(strategy=\"most_frequent\")\n","\n","skf = StratifiedKFold(n_splits=5)\n","NB_Scores = []\n","zr_Scores = []\n","for train_index, test_index in skf.split(X_values,y_values):\n","    X_train, X_test = X_values[train_index], X_values[test_index]\n","    y_train, y_test = y_values[train_index], y_values[test_index]\n","\n","    GNB = GNB_clf.fit(X_train, y_train)\n","    GNB_pred = GNB.predict(X_test)\n","    gnb_score = accuracy_score(y_test, GNB_pred) \n","    NB_Scores.append(gnb_score)\n","\n","    zeroR = zr_clf.fit(X_train, y_train)\n","    zeroR_pred = zeroR.predict(X_test)\n","    zeroR_score = accuracy_score(y_test, zeroR_pred) \n","    zr_Scores.append(zeroR_score)\n","\n","print('Naive Bayes score :', NB_Scores)\n","\n","print('zeroR score : ', zr_Scores)\n","print()\n","print(\"Stratified 5-Fold Cross-Validation\")\n","print()\n","\n","NB_average_accuracy1 = (sum(NB_Scores)/len(NB_Scores))*100\n","zr_average_accuracy1 = (sum(zr_Scores)/len(zr_Scores)) *100\n","NB_min = min(NB_Scores) * 100\n","NB_max = max(NB_Scores) *100\n","zr_min = min(zr_Scores) *100\n","zr_max = max(zr_Scores) * 100\n","print('Naive Bayes accuracy: {:.2f}% ({:.2f}% / {:.2f}%)'.format(NB_average_accuracy1, NB_min, NB_max))\n","\n","print('ZeroR accuracy: {:.2f}% ({:.2f}% / {:.2f}%)'.format(zr_average_accuracy1, zr_min, zr_max))\n","\n","plot_confusion_matrix(GNB, X_test, y_test) \n","plt.show()\n","\n"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Naive Bayes score : [0.9333333333333333, 0.9666666666666667, 0.9333333333333333, 0.9333333333333333, 1.0]\n","zeroR score :  [0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333, 0.3333333333333333]\n","\n","Stratified 5-Fold Cross-Validation\n","\n","Naive Bayes accuracy: 95.33% (93.33% / 100.00%)\n","ZeroR accuracy: 33.33% (33.33% / 33.33%)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]}]}
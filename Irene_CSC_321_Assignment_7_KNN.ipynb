{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Irene_CSC-321 Assignment 7_KNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IreneYIN7/CSC-Data_Mining-Machine_Learning_Projects/blob/master/Irene_CSC_321_Assignment_7_KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPy8O0DVLIl9"
      },
      "source": [
        "# CSC-321: Data Mining and Machine Learning\n",
        "# Irene Yin\n",
        "## Assignment 7: K-Nearest Neighbor\n",
        "\n",
        "### Part 1: Implementation\n",
        "\n",
        "For this assignment, I'm going to let you break down the implementation as you see fit. You're going to implement KNN, an example of lazy learning. In brief, this means:\n",
        "\n",
        "- calculating euclidean distance between the feature values of a single test instance, and the feature values of a single training instance\n",
        "- making a prediction requires iterating through *all* the training instances, calculating the distances and storing each distance in a list, along with the corresponding class value (probably as some sort of tuple)\n",
        "- sorting the list, smallest distances first\n",
        "- selecting the *k* nearest neighbors (where k should be a parameter)\n",
        "- making a prediction by choosing the class that appears the most in the k nearest neighbors\n",
        "\n",
        "To calculate euclidean distance between an instance x1 and an instance x2, we need to iterate through the input features of the two instances (for i features) and for each take the difference of (x1[i]) - (x2[i]), squaring that difference, and summing over all features. At the end, take the square root of the total. In other words:\n",
        "\n",
        "$$distance=\\sqrt{\\sum_{i=1}^n (x1_{i} - x2_{i})^2}$$\n",
        "\n",
        "\n",
        "I would strongly suggest you follow the implementation outline of previous algorithms in terms of the functions you use, but I'm leaving it up to you.\n",
        "\n",
        "Below is the same contrived dataset you've used before. If your code works, you should be able to take an instance of this data, and compare it to all the others (including itself, where the distance SHOULD be 0). You should be able to select the k-nearest neighbors, and make a prediction based on the most frequently occuring class in those k neighbors. Try it for different values of k, from 1, 3 and 5.\n",
        "\n",
        "Make sure you create a knn function that takes a training set (X_train, y_train), a test set (X_test) and a value for k, that returns a list of predictions - one prediction for each instance in the test set.\n",
        "\n",
        "Run the algorithm over the sample dataset, using k=3. Print the predicted and the actual side by side."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd25OACkLIl-",
        "outputId": "16571a15-9012-4230-a28a-b77cddce3db5"
      },
      "source": [
        "# Contrived data set\n",
        "import operator\n",
        "\n",
        "dataset = [[3.393533211,2.331273381,0],\n",
        "    [3.110073483,1.781539638,0],\n",
        "    [1.343808831,3.368360954,0],\n",
        "    [3.582294042,4.67917911,0],\n",
        "    [2.280362439,2.866990263,0],\n",
        "    [7.423436942,4.696522875,1],\n",
        "    [5.745051997,3.533989803,1],\n",
        "    [9.172168622,2.511101045,1],\n",
        "    [7.792783481,3.424088941,1],\n",
        "    [7.939820817,0.791637231,1]]\n",
        "\n",
        "X_train = [i[:-1] for i in dataset]\n",
        "X_test = X_train\n",
        "y_train = [i[-1] for i in dataset]\n",
        "y_test = y_train\n",
        "\n",
        "def eucliDistance(instanceX1,instanceX2):\n",
        "  # return: return the euclidean distance between the feature values of a single \n",
        "  #         test instance, and of a single trainning instance.\n",
        "    distance = 0\n",
        "    for i in range(len(instanceX1)-1):\n",
        "        distance += (instanceX1[i] - instanceX2[i])**2\n",
        "    totalDistance = distance**(0.5)\n",
        "    return totalDistance\n",
        "\n",
        "\n",
        "def disWithValue(X_train, instance):\n",
        "  # return : store distance in a list, along with the corresponding class value \n",
        "      DistancewithValue = []\n",
        "      Distances = []\n",
        "      for j in range(len(X_train)):\n",
        "            Distances.append(eucliDistance(instance, X_train[j]))\n",
        "      Classvalue = [y_train[Distances.index(distance)] for distance in Distances]\n",
        "      for i in range(len(Classvalue)):\n",
        "        DistancewithValue.append((Distances[i],Classvalue[i]))\n",
        "      return DistancewithValue\n",
        "  \n",
        "def selectKNearest(distanceList, k):\n",
        "  # return: the k number of the nearest neighboours\n",
        "    sortedList = sorted(distanceList)\n",
        "    return sortedList[:k]\n",
        "\n",
        "\n",
        "def predict(KnearestList, k):\n",
        "  # return : The most frequent class\n",
        "    classvalue = [i[-1] for i in KnearestList]\n",
        "    return max(set(classvalue), key=classvalue.count)\n",
        "\n",
        "\n",
        "\n",
        "def knn(X_train, y_train, X_test, k):\n",
        "    predictions = []\n",
        "    for i in range(len(X_test)):\n",
        "        distance = disWithValue(X_train,X_test[i])\n",
        "        kNearestList = selectKNearest(distance,k)\n",
        "        predictions.append(predict(kNearestList,k))\n",
        "    return predictions\n",
        "\n",
        "k = 3\n",
        "KnnPred = knn(X_train, y_train, X_test,k)\n",
        "print('KNN prediction when k = 3:')\n",
        "print('\\nPREDICTED : ACTUAL')\n",
        "print('------------------')\n",
        "for i in range(len(KnnPred)):\n",
        "  print('    {:.2f}  :  {:.2f}  '.format(KnnPred[i],y_test[i]))\n",
        "print()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "KNN prediction when k = 3:\n",
            "\n",
            "PREDICTED : ACTUAL\n",
            "------------------\n",
            "    0.00  :  0.00  \n",
            "    0.00  :  0.00  \n",
            "    0.00  :  0.00  \n",
            "    0.00  :  0.00  \n",
            "    0.00  :  0.00  \n",
            "    1.00  :  1.00  \n",
            "    1.00  :  1.00  \n",
            "    1.00  :  1.00  \n",
            "    1.00  :  1.00  \n",
            "    1.00  :  1.00  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwpFG6iQLImD"
      },
      "source": [
        "### Part 2: Working with real data\n",
        "\n",
        "Apply the KNN algorithm above to the abalone data set. You can find more about it here: http://archive.ics.uci.edu/ml/datasets/Abalone\n",
        "\n",
        "I've started the process, because I want to show you another part of scikit learn. I've loaded in the data, and shown the head of the data. Pay attention to the sex column. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "bQYMk3UeLImE",
        "outputId": "0d6110fb-a23f-4129-a4e5-bd27090af269"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "labels = ['sex','length','diameter','height','whole_weight','shucked_weight',\n",
        "          'viscera_weight','shell_weight','rings']\n",
        "\n",
        "abalone_data = pd.read_csv('https://raw.githubusercontent.com/nixwebb/CSV_Data/master/abalone.csv',names=labels)\n",
        "abalone_data.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex</th>\n",
              "      <th>length</th>\n",
              "      <th>diameter</th>\n",
              "      <th>height</th>\n",
              "      <th>whole_weight</th>\n",
              "      <th>shucked_weight</th>\n",
              "      <th>viscera_weight</th>\n",
              "      <th>shell_weight</th>\n",
              "      <th>rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>M</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.5140</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.150</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>M</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>F</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>M</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sex  length  diameter  ...  viscera_weight  shell_weight  rings\n",
              "0   M   0.455     0.365  ...          0.1010         0.150     15\n",
              "1   M   0.350     0.265  ...          0.0485         0.070      7\n",
              "2   F   0.530     0.420  ...          0.1415         0.210      9\n",
              "3   M   0.440     0.365  ...          0.1140         0.155     10\n",
              "4   I   0.330     0.255  ...          0.0395         0.055      7\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x63WpqWFytr5"
      },
      "source": [
        "That first column is nominal data, not numeric. We first have to change it into numbers - either replacing each value with an integer (label encoding) or creating new columns to represent each possible value (one-hot encoding).\n",
        "\n",
        "For label encoding, I use the method in scikit learn. For one-hot encoding, if my data is in a pandas dataframe, I prefer the pandas method. Ultimately it doesn't matter, and you should feel free to check out both.\n",
        "\n",
        "Ultimately, I'll do one-hot encoding here. The method in pandas is called get_dummies. I'm going to do that in stages, to show you what happens, but you don't need to print after ever step like this, once you're convinced it works.\n",
        "\n",
        "The get_dummies method in pandas creates my new columns based on feature values. There are three possible feature values for the sex feature (M,F,I - and you should know what these mean), so I create three columns.\n",
        "\n",
        "Notice that in pandas I can refer to a column using a built in attribute. The abalone_data dataframe has a column labeled 'sex', so I can use abalone_data.sex to access that column. I'm creating column headings from the feature values, and adding the prefix 'sex' to each value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "021lQBjryuDr",
        "outputId": "35ff2c43-672d-48a3-b009-f514cc976cdf"
      },
      "source": [
        "abalone_sex = pd.get_dummies(abalone_data.sex, prefix='sex')\n",
        "abalone_sex.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex_F</th>\n",
              "      <th>sex_I</th>\n",
              "      <th>sex_M</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sex_F  sex_I  sex_M\n",
              "0      0      0      1\n",
              "1      0      0      1\n",
              "2      1      0      0\n",
              "3      0      0      1\n",
              "4      0      1      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8VcOB8Rz53N"
      },
      "source": [
        "Then I need to add these columns back into my overall dataframe. I'm using the pandas method concat to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "M_SFobpgz4-Y",
        "outputId": "c59e1cdb-a6c3-4eea-f953-bf4c2d152dd5"
      },
      "source": [
        "abalone_ohe = pd.concat([abalone_sex,abalone_data],axis=1)\n",
        "abalone_ohe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex_F</th>\n",
              "      <th>sex_I</th>\n",
              "      <th>sex_M</th>\n",
              "      <th>sex</th>\n",
              "      <th>length</th>\n",
              "      <th>diameter</th>\n",
              "      <th>height</th>\n",
              "      <th>whole_weight</th>\n",
              "      <th>shucked_weight</th>\n",
              "      <th>viscera_weight</th>\n",
              "      <th>shell_weight</th>\n",
              "      <th>rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.5140</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.150</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>F</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>M</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>I</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sex_F  sex_I  sex_M sex  ...  shucked_weight  viscera_weight  shell_weight  rings\n",
              "0      0      0      1   M  ...          0.2245          0.1010         0.150     15\n",
              "1      0      0      1   M  ...          0.0995          0.0485         0.070      7\n",
              "2      1      0      0   F  ...          0.2565          0.1415         0.210      9\n",
              "3      0      0      1   M  ...          0.2155          0.1140         0.155     10\n",
              "4      0      1      0   I  ...          0.0895          0.0395         0.055      7\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5o2eidcz4wh"
      },
      "source": [
        "And finally, now I've encoded the sex using one-hot encoding, I'm going to drop the sex column from the dataframe. However, just to show you how the scikit learn label encoder works, execute the code in the first code cell below, and check out what happens to the sex column values. Then run the second cell to drop the sex column from the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "MA3vKbHS6P4w",
        "outputId": "98a44abc-6f28-4534-e3f1-10ffb21c211a"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "abalone_ohe['sex'] = le.fit_transform(abalone_ohe.sex.values)\n",
        "abalone_ohe.head()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex_F</th>\n",
              "      <th>sex_I</th>\n",
              "      <th>sex_M</th>\n",
              "      <th>sex</th>\n",
              "      <th>length</th>\n",
              "      <th>diameter</th>\n",
              "      <th>height</th>\n",
              "      <th>whole_weight</th>\n",
              "      <th>shucked_weight</th>\n",
              "      <th>viscera_weight</th>\n",
              "      <th>shell_weight</th>\n",
              "      <th>rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.5140</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.150</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sex_F  sex_I  sex_M  ...  viscera_weight  shell_weight  rings\n",
              "0      0      0      1  ...          0.1010         0.150     15\n",
              "1      0      0      1  ...          0.0485         0.070      7\n",
              "2      1      0      0  ...          0.1415         0.210      9\n",
              "3      0      0      1  ...          0.1140         0.155     10\n",
              "4      0      1      0  ...          0.0395         0.055      7\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "SXf6FhF2z4XG",
        "outputId": "ec27d7a9-79f5-4d01-e528-48578bde5ec9"
      },
      "source": [
        "abalone_ohe.drop('sex',axis=1,inplace=True)\n",
        "abalone_ohe.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sex_F</th>\n",
              "      <th>sex_I</th>\n",
              "      <th>sex_M</th>\n",
              "      <th>length</th>\n",
              "      <th>diameter</th>\n",
              "      <th>height</th>\n",
              "      <th>whole_weight</th>\n",
              "      <th>shucked_weight</th>\n",
              "      <th>viscera_weight</th>\n",
              "      <th>shell_weight</th>\n",
              "      <th>rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.5140</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.150</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sex_F  sex_I  sex_M  ...  viscera_weight  shell_weight  rings\n",
              "0      0      0      1  ...          0.1010         0.150     15\n",
              "1      0      0      1  ...          0.0485         0.070      7\n",
              "2      1      0      0  ...          0.1415         0.210      9\n",
              "3      0      0      1  ...          0.1140         0.155     10\n",
              "4      0      1      0  ...          0.0395         0.055      7\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Hc14fC3y5uz"
      },
      "source": [
        "The y value for this data, the thing we're predicting, is the category represented by the number of rings.\n",
        "\n",
        "Using this dataset, extract X and y data, normalize the X values, and run a 5-fold cross-validation, with k set as 5. Also run a classification baseline. Report on classification accuracy, and write up some results.\n",
        "\n",
        "NOTE: This will be SLOW. If you're not sure your code is working, I recommend starting with a 3 fold cross-validation, and use k=1. You will probably get some UserWarnings from python also. I will explain these in class later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc3HDWovy6o8",
        "outputId": "5c672299-919a-4586-c4d2-aed196454cad"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# extract X and y data. \n",
        "\n",
        "abalone_value = abalone_ohe.values\n",
        "X_values = abalone_value[:,:-1]\n",
        "y_values = abalone_value[:,len(abalone_value[0])-1]\n",
        "\n",
        "rows,cols = X_values.shape\n",
        "print(\"This is the wine data set. It has\", rows, \"instances, and it has\", cols, \"input features.\\n\")\n",
        "print(\"The first FIVE instances look like:\")\n",
        "\n",
        "# normalize X values:\n",
        "\n",
        "# Show the first five instances\n",
        "print(X_values[:5])\n",
        "print()\n",
        "# Load and fit the scaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(X_values)\n",
        "# Use some attributes of the scaler to show min and max values per feature\n",
        "# Note these should align with the information from the pandas .describe\n",
        "# method, used above\n",
        "\n",
        "print(\"MAX values:\",scaler.data_max_)\n",
        "print(\"MIN values:\",scaler.data_min_)\n",
        "print()\n",
        "\n",
        "# Transform our X_values, so that data is now scaled\n",
        "# Note we can apply this transform to any data, including new data\n",
        "# and it will preserve the min and max values given above\n",
        "\n",
        "X_values = scaler.transform(X_values)\n",
        "\n",
        "# Take another look at those first five instances that should now be \n",
        "# normalized\n",
        "\n",
        "print(\"After normalization, the first FIVE instances look like:\")\n",
        "print(X_values[:5])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the wine data set. It has 4177 instances, and it has 10 input features.\n",
            "\n",
            "The first FIVE instances look like:\n",
            "[[0.     0.     1.     0.455  0.365  0.095  0.514  0.2245 0.101  0.15  ]\n",
            " [0.     0.     1.     0.35   0.265  0.09   0.2255 0.0995 0.0485 0.07  ]\n",
            " [1.     0.     0.     0.53   0.42   0.135  0.677  0.2565 0.1415 0.21  ]\n",
            " [0.     0.     1.     0.44   0.365  0.125  0.516  0.2155 0.114  0.155 ]\n",
            " [0.     1.     0.     0.33   0.255  0.08   0.205  0.0895 0.0395 0.055 ]]\n",
            "\n",
            "MAX values: [1.     1.     1.     0.815  0.65   1.13   2.8255 1.488  0.76   1.005 ]\n",
            "MIN values: [0.     0.     0.     0.075  0.055  0.     0.002  0.001  0.0005 0.0015]\n",
            "\n",
            "After normalization, the first FIVE instances look like:\n",
            "[[0.         0.         1.         0.51351351 0.5210084  0.0840708\n",
            "  0.18133522 0.15030262 0.1323239  0.14798206]\n",
            " [0.         0.         1.         0.37162162 0.35294118 0.07964602\n",
            "  0.07915707 0.06624075 0.06319947 0.06826109]\n",
            " [1.         0.         0.         0.61486486 0.61344538 0.11946903\n",
            "  0.23906499 0.17182246 0.18564845 0.2077728 ]\n",
            " [0.         0.         1.         0.49324324 0.5210084  0.11061947\n",
            "  0.18204356 0.14425017 0.14944042 0.15296462]\n",
            " [0.         1.         0.         0.34459459 0.33613445 0.07079646\n",
            "  0.07189658 0.0595158  0.05134957 0.0533134 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2zbxqoCWNvy",
        "outputId": "3657dc71-9d43-433f-b695-26873f026579"
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "X_train = X_values\n",
        "X_test = X_train\n",
        "y_train = y_values\n",
        "y_test = y_train\n",
        "\n",
        "# ZeroR\n",
        "def accuracy(actual_value, predicted_value):\n",
        "  # return how many times the function predicts correctly in ratio.\n",
        "  counter = 0 \n",
        "  for i in range(len(actual_value)):\n",
        "    if actual_value[i] == predicted_value[i]:\n",
        "      counter += 1\n",
        "  return counter/len(actual_value)\n",
        "\n",
        "def zeroRC(train, test):\n",
        "  # return baseline\n",
        "  commonElement = max(set(train), key = train.count)\n",
        "  return [commonElement for i in range(len(test))]\n",
        "\n",
        "# 5-fold cross validation\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "zeroR_scores = []\n",
        "knn_scores = []\n",
        "k = 3\n",
        "\n",
        "for train_index, test_index in skf.split(X_values,y_values):\n",
        "    X_train, X_test = X_values[train_index], X_values[test_index]\n",
        "    y_train, y_test = y_values[train_index], y_values[test_index]\n",
        "  \n",
        "    # Add in calls to your KNN and zeroR functions here\n",
        "    # Then calculate accuracy, and append each score to the appropriate list\n",
        "    KNN_Pred = knn(X_train,y_train,X_test,k)\n",
        "    zeroR_pred = zeroRC(y_train.tolist(),X_test)\n",
        "\n",
        "    accuracy_zeroR = accuracy(y_test, zeroR_pred) * 100\n",
        "    zeroR_scores.append(accuracy_zeroR)\n",
        "\n",
        "    accuracy_knn = accuracy(y_test, KNN_Pred) * 100\n",
        "    knn_scores.append(accuracy_knn)\n",
        "\n",
        "print(\"knn score:\", knn_scores)\n",
        "print(\"ZeroR score:\", zeroR_scores)\n",
        "knn_average_accuracy1 = (sum(knn_scores)/len(knn_scores))\n",
        "zr_average_accuracy1 = (sum(zeroR_scores)/len(zeroR_scores))\n",
        "KNN_min = min(knn_scores)\n",
        "KNN_max = max(knn_scores) \n",
        "zr_min = min(zeroR_scores) \n",
        "zr_max = max(zeroR_scores) \n",
        "print('KNN accuracy: {:.2f}% ({:.2f}% / {:.2f}%)'.format(knn_average_accuracy1, KNN_min, KNN_max))\n",
        "\n",
        "print('ZeroR accuracy: {:.2f}% ({:.2f}% / {:.2f}%)'.format(zr_average_accuracy1, zr_min, zr_max))\n",
        "  \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "knn score: [17.942583732057415, 24.16267942583732, 22.275449101796408, 23.592814371257482, 23.233532934131738]\n",
            "ZeroR score: [16.507177033492823, 16.507177033492823, 16.526946107784433, 16.407185628742514, 16.526946107784433]\n",
            "KNN accuracy: 22.24% (17.94% / 24.16%)\n",
            "ZeroR accuracy: 16.50% (16.41% / 16.53%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUQ6PGtZLImI"
      },
      "source": [
        "#### Analysis\n",
        "The baseline of the abalone dataset is 16.5%. KNN perfomrs better than the baseline where the average accuracy of this baseline is  22.24%, which is larger than the baseline. \n",
        "\n",
        "The baseline shows that there are 16.5% chance that the ages of the abalone can be predicted in the right class. And by using KNN model, there are 22.24% chance that the ages of the abalone is predicted in the right class. \n",
        "\n",
        "We cannot say which features is more important based on the KNN model since KNN is just calculating the distances between instances. \n",
        "\n",
        "Apart from that, it might be a good idea to use other classification model because the running time of KNN is very long. The more the data, the less efficient the KNN model is. It's not a good idea to use KNN model to train and predict with the large amonut of data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP5-6EO-LImJ"
      },
      "source": [
        "### Part 3: KNN regression\n",
        "\n",
        "We can also run KNN as a regression algorithm. In this case, instead of predicting the most common class in the k nearest neighbors, we can assign a predicted value that is the mean of the values of the k neighbors. \n",
        "\n",
        "Make this change to your algorithm (presumably by simply implementing a new predict function below, and then calling this new predict fucntion from your knn algorithm, because you divided your code up sensibly in Part 1), and run the abalone data as a regression problem. To do this, use the same number of folds and the same k value as before. Also run a regression baseline and report RMSE values for both. Give me some explanation of the results, both standalone and in comparison to the classification results above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4JkAnrFLImJ",
        "outputId": "c85553e0-27a9-4715-8f19-a0fc4d9f3c29"
      },
      "source": [
        "import numpy as np\n",
        "def getMean(inputList):\n",
        "  # return: the average (the mean) for any list of values(numbers).\n",
        "  \n",
        "  return np.mean(inputList)\n",
        "\n",
        "\n",
        "def predictReg(KnearestList, k):\n",
        "  # return : The mean \n",
        "    classValue = [i[-1] for i in KnearestList]\n",
        "    return getMean(classValue)\n",
        "\n",
        "def knnReg(X_train, y_train, X_test, k):\n",
        "    predictions = []\n",
        "    for i in range(len(X_test)):\n",
        "        distance = disWithValue(X_train,X_test[i])\n",
        "        kNearestList = selectKNearest(distance,k)\n",
        "        predictions.append(predictReg(kNearestList,k))\n",
        "    return predictions\n",
        "\n",
        "def zeroRR(ytrain, Xtest):\n",
        "  # return: compute the mean from the ytrain values and return the list of predictions.\n",
        "  meanOfytrain = getMean(ytrain)\n",
        "  prediction = []\n",
        "  for num in Xtest:\n",
        "    prediction.append(meanOfytrain)\n",
        "  return prediction\n",
        "\n",
        "def rmse(actual, predicted):\n",
        "  # return: prediction error.\n",
        "  predictionError = 0.0\n",
        "  for i in range(len(actual)):\n",
        "    predictionError += (actual[i] - predicted[i])**2\n",
        "  predictionError_avg = predictionError/len(actual)\n",
        "  predictionError_avg = predictionError_avg**0.5\n",
        "  return predictionError_avg\n",
        "\n",
        "prediction = zeroRR(y_train, X_train)\n",
        "print(\"ZeroR RMSE with same train and test:\", rmse(y_train, prediction))\n",
        "# 5-fold cross validation\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "zeroRR_scores = []\n",
        "knnReg_scores = []\n",
        "k = 3\n",
        "\n",
        "for train_index, test_index in skf.split(X_values,y_values):\n",
        "    X_train, X_test = X_values[train_index], X_values[test_index]\n",
        "    y_train, y_test = y_values[train_index], y_values[test_index]\n",
        "\n",
        "    # Add in calls to your KNN and zeroR functions here\n",
        "    # Then calculate accuracy, and append each score to the appropriate list\n",
        "    KnnReg_Pred = knnReg(X_train,y_train,X_test,k)\n",
        "    rmse_knn = rmse(y_test, KnnReg_Pred)\n",
        "    knnReg_scores.append(rmse_knn)\n",
        "\n",
        "    zeroRR_pred = zeroRR(y_train,X_test)\n",
        "    rmse_zeroRR = rmse(y_test, zeroRR_pred)\n",
        "    zeroRR_scores.append(rmse_zeroRR)\n",
        "    \n",
        "\n",
        "\n",
        "print(\"KNN Regression score:\", knnReg_scores)\n",
        "print(\"ZeroR score:\", zeroRR_scores)\n",
        "print('KNN Regression RMSE: {:.2f} ({:.2f} / {:.2f})'.format(getMean(knnReg_scores), min(knnReg_scores), max(knnReg_scores)))\n",
        "\n",
        "print('ZeroR RMSE: {:.2f} ({:.2f} / {:.2f})'.format(getMean(zeroRR_scores), min(zeroRR_scores), max(zeroRR_scores)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ZeroR RMSE with same train and test: 0.5\n",
            "KNN Regression score: [2.5749013493849255, 2.430014559675294, 2.398685601484791, 2.4536149278642325, 2.425359477714809]\n",
            "ZeroR score: [3.276135506905344, 3.2095351101549667, 3.2037611342745986, 3.227275777276688, 3.2016461361389403]\n",
            "KNN Regression RMSE: 2.46 (2.40 / 2.57)\n",
            "ZeroR RMSE: 3.22 (3.20 / 3.28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOzJ42AALImM"
      },
      "source": [
        "#### Analysis\n",
        "According to the data, we have the baseline as 3.22. This means that the prediction towards Abalone's age would have the error of 3.22.  Whereas the error rate of the KNN regression is 2.46, which is smaller than the baseline. That is, the prediction result using KNN would have error of 2.46. \n",
        "\n",
        "This shows that KNN regression did a slightly better than the baseline. However, considering the speed/time cost of the KNN regression, I would rather choose other model. \n",
        "\n",
        "Apart from that, it's also hard for us the know which features is more important since KNN is just calculating the distance between two instances.\n",
        "\n",
        "According to the dataset and the y_value, we can tell that the Class values are numeric values. Comparing with using the classification which is good at predicting the categorical values, Regrssion would fit the numerical data better. Thus, the Regression fits this task better. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6O79qUJQMzG"
      },
      "source": [
        "## Part 4: Introduction to scikit-learn\n",
        "\n",
        "One of the most popular open-source python machine learning libraries is scikit-learn. You can find out more in general at: https://scikit-learn.org/stable/index.html\n",
        "\n",
        "\n",
        "As we go through this class I'll introduce you to some of the functionality. Below I want you to use BOTH a KNN Classifier and the KNN Regressor.\n",
        "\n",
        "I also I want you to explore the cross_val_score function. Previously you used the StratifiedKFold function. You then had to fit the model, then use the predict method to apply the model, then collect the scores, find the mean and the min and the max. cross_val_score does most of that for you. \n",
        "\n",
        "cross_val_score takes a model (the classifier, or regressor) you want to use, X and y data, a value for cv (the default is 5 for a 5-fold cross validation), and a scoring metric. It does all the fitting, prediction and collection of scores for you. By default, the scoring measure is accuracy, which is great for classification.\n",
        "\n",
        "What is returned is a list of the cross-validation scores. You can apply the .mean(), .min() and .max() methods to this list to generate scores as you have before.\n",
        "\n",
        "If we want to cross-validate a regression algorithm, then we need to change the scoring. Use the parameter scoring='neg_mean_squared_error'.\n",
        "\n",
        "To turn this neg_mean_squared_error into a meaningful score for us, we'll need to take the absolute value (to reverse the sign) and then apply math.sqrt to transform the results into RMSE.\n",
        "\n",
        "The links to the relevant documentation pages are:\n",
        "- [KNN Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
        "- [KNN Regressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor)\n",
        "- [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score)\n",
        "\n",
        "I'll load the relevant models from scikit-learn, but it's up to you to train and test them, and report the scores appropriately, including comparison to baselines (use scikits dummy classifier) and write up. Your scores should be the broadly the same as your code, above.\n",
        "\n",
        "NOTE: I've added some code below to suppress the user warnings from earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2DWMB2KSqHM",
        "outputId": "49a584da-672b-4060-fbb1-9941f9f014f3"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.dummy import DummyRegressor\n",
        "import math\n",
        "\n",
        "# There are some user warnings that this time I'm going to suppress\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
        "\n",
        "\n",
        "# KNN classifier\n",
        "knnClass = KNeighborsClassifier(n_neighbors=3)\n",
        "KNNClass = knnClass.fit(X_train, y_train)\n",
        "knnClass_pred = KNNClass.predict(X_test)\n",
        "\n",
        "knnClassScore = cross_val_score(KNNClass, X_test, y_test, cv=5)\n",
        "knnClassEval = []\n",
        "for i in knnClassScore:\n",
        "  knnClassEval.append(i*100)\n",
        "\n",
        "# KNN Regressor\n",
        "knnReg = KNeighborsRegressor(n_neighbors=3)\n",
        "KNNReg = knnReg.fit(X_train, y_train)\n",
        "knnReg_pred = KNNReg.predict(X_test)\n",
        "\n",
        "knnRegScore = cross_val_score(KNNReg, X_test, y_test, scoring='neg_mean_squared_error')\n",
        "knnRegEval = [abs(i)**(0.5) for i in knnRegScore]\n",
        "\n",
        "# ZeroRC\n",
        "zr_clf = DummyClassifier(strategy=\"most_frequent\")\n",
        "zeroR = zr_clf.fit(X_train, y_train)\n",
        "zeroR_pred = zeroR.predict(X_test)\n",
        "zrScore = cross_val_score(zeroR, X_test, y_test, cv=5)\n",
        "zrEval = []\n",
        "for j in zrScore:\n",
        "  zrEval.append(j*100)\n",
        "\n",
        "# ZeroRR\n",
        "zr = DummyRegressor()\n",
        "zeroRR = zr.fit(X_train, y_train)\n",
        "zr_predY = zeroRR.predict(X_test)\n",
        "\n",
        "zeRScore = cross_val_score(zeroRR, X_test, y_test, scoring='neg_mean_squared_error')\n",
        "zeREval = [abs(i)**(0.5) for i in zeRScore]\n",
        "\n",
        "print()\n",
        "print('KNN accuracy sklearn: {:.2f}% ({:.2f}% / {:.2f}%)'.format(getMean(knnClassEval), min(knnClassEval), max(knnClassEval)))\n",
        "print('KNN accuracy: {:.2f}% ({:.2f}% / {:.2f}%)'.format(knn_average_accuracy1, KNN_min, KNN_max))\n",
        "print()\n",
        "print('ZeroR accuracy sklearn: {:.2f}% ({:.2f}% / {:.2f}%)'.format(getMean(zrEval), min(zrEval), max(zrEval)))\n",
        "print('ZeroR accuracy: {:.2f}% ({:.2f}% / {:.2f}%)'.format(zr_average_accuracy1, zr_min, zr_max))\n",
        "print()\n",
        "print('KNN RMSE sklearn: {:.2f} ({:.2f} / {:.2f})'.format(getMean(knnRegEval), min(knnRegEval), max(knnRegEval)))\n",
        "print('KNN RMSE: {:.2f} ({:.2f} / {:.2f})'.format(getMean(knnReg_scores), min(knnReg_scores), max(knnReg_scores)))\n",
        "print()\n",
        "print('ZeroR RMSE sklearn: {:.2f} ({:.2f} / {:.2f})'.format(getMean(zeREval), min(zeREval), max(zeREval)))\n",
        "print('ZeroR RMSE: {:.2f} ({:.2f} / {:.2f})'.format(getMean(zeroRR_scores), min(zeroRR_scores), max(zeroRR_scores)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "KNN accuracy sklearn: 21.08% (17.37% / 23.35%)\n",
            "KNN accuracy: 22.24% (17.94% / 24.16%)\n",
            "\n",
            "ZeroR accuracy sklearn: 16.53% (16.17% / 16.77%)\n",
            "ZeroR accuracy: 16.50% (16.41% / 16.53%)\n",
            "\n",
            "KNN RMSE sklearn: 2.36 (1.43 / 3.91)\n",
            "KNN RMSE: 2.46 (2.40 / 2.57)\n",
            "\n",
            "ZeroR RMSE sklearn: 3.04 (1.91 / 5.20)\n",
            "ZeroR RMSE: 3.22 (3.20 / 3.28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VTMG4PrFKMg"
      },
      "source": [
        "#### Analysis\n",
        "The data result by using the sklearn is similar to the results by writing the code ourselves. Since the 5-fold cross validation, the bins would be created randomly. Hence, there might be slightly different. The method, cross_val_score, in sklearn works really efficiently and easily. It automatically collect scores for us.\n",
        "\n",
        "For the KNN classification:\n",
        "\n",
        "The baseline of the abalone dataset is 16.53%. KNN perfomrs better than the baseline where the average accuracy of this baseline is  21.08%, which is larger than the baseline. \n",
        "\n",
        "The baseline shows that there are 16.53% chance that the ages of the abalone can be predicted in the right class. And by using KNN model, there are 21.08% chance that the ages of the abalone is predicted in the right class. \n",
        "\n",
        "For the KNN regression:\n",
        "\n",
        "According to the data, we have the baseline as 3.04. This means that the prediction towards Abalone's age would have the error of 3.04.  Whereas the error rate of the KNN regression is 2.36, which is smaller than the baseline. That is, the prediction result using KNN would have error of 2.36. This shows that KNN regression did a slightly better than the baseline. \n",
        "\n",
        "\n",
        "According to the dataset and the y_value, we can tell that the Class values are numeric values. Comparing with using the classification which is good at predicting the categorical values, Regrssion would fit the numerical data better. Thus, the Regression fits this task better.  \n",
        "\n",
        "We cannot say which features is more important based on the KNN model since KNN is just calculating the distances between instances. \n"
      ]
    }
  ]
}